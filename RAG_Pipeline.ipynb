{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa18cabe283949379613d0f1753ceb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30853b283b9041748dbc490cf611380d",
              "IPY_MODEL_8fcc1d7691244d3f8481cb9e99e21d9f",
              "IPY_MODEL_a650f990e0e64b2a9117aeb348af38b8"
            ],
            "layout": "IPY_MODEL_8bfca65edbf54dfeb3c7282441947835"
          }
        },
        "30853b283b9041748dbc490cf611380d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73d78321b21e459997f924fb5f1bfe8a",
            "placeholder": "​",
            "style": "IPY_MODEL_65bec95b20ac45179c91b6357e8a2c15",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8fcc1d7691244d3f8481cb9e99e21d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43047dada2b44bf7a5daa5d3d202007e",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d1bc211218541d190415d0d8850abcf",
            "value": 48
          }
        },
        "a650f990e0e64b2a9117aeb348af38b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_121acc13507e43299549b3aba197125c",
            "placeholder": "​",
            "style": "IPY_MODEL_3a7e3c8c00e74a999b4e2618eb5df3fa",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.76kB/s]"
          }
        },
        "8bfca65edbf54dfeb3c7282441947835": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d78321b21e459997f924fb5f1bfe8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65bec95b20ac45179c91b6357e8a2c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43047dada2b44bf7a5daa5d3d202007e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d1bc211218541d190415d0d8850abcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "121acc13507e43299549b3aba197125c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a7e3c8c00e74a999b4e2618eb5df3fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0fd08bf01e1499dad918edace7d6223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e7b623ba9994f9fb650912b1d775804",
              "IPY_MODEL_6006e60708634159b8677d76841fe6f4",
              "IPY_MODEL_7a1bae42de9f4ecc9ef99e18f546edb6"
            ],
            "layout": "IPY_MODEL_723a5979deab41b7b066c23feba1f388"
          }
        },
        "4e7b623ba9994f9fb650912b1d775804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00e0d4970ad2443cbfcf3e0a52e5bec4",
            "placeholder": "​",
            "style": "IPY_MODEL_46762faf3e934249a35991c00bf42ba8",
            "value": "config.json: 100%"
          }
        },
        "6006e60708634159b8677d76841fe6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48c4345060eb4ff7b03e0e1540b8be73",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c5e07301a3d425ab00e5ba07a5098e5",
            "value": 570
          }
        },
        "7a1bae42de9f4ecc9ef99e18f546edb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82e91205e9bb46f78a147d6b18b06770",
            "placeholder": "​",
            "style": "IPY_MODEL_c5f239273e17497f8cc54cdb9551096a",
            "value": " 570/570 [00:00&lt;00:00, 48.4kB/s]"
          }
        },
        "723a5979deab41b7b066c23feba1f388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00e0d4970ad2443cbfcf3e0a52e5bec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46762faf3e934249a35991c00bf42ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48c4345060eb4ff7b03e0e1540b8be73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5e07301a3d425ab00e5ba07a5098e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82e91205e9bb46f78a147d6b18b06770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f239273e17497f8cc54cdb9551096a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3143121c68994ad198bf59b50b3eb0a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f99eac364398434aa5ecfbf54d94613c",
              "IPY_MODEL_9e76b745e60749298c7ad7cdd89fc234",
              "IPY_MODEL_038bec5cbf874f158cf951c726f31fb7"
            ],
            "layout": "IPY_MODEL_5b136bf84c274ec98733224f8b7c41cc"
          }
        },
        "f99eac364398434aa5ecfbf54d94613c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaeb681ce16847fdbd6001c202999251",
            "placeholder": "​",
            "style": "IPY_MODEL_bf19688eeea04df6a53faf8153d8d25c",
            "value": "vocab.txt: 100%"
          }
        },
        "9e76b745e60749298c7ad7cdd89fc234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1241d394ca044ec9978fcb4627bffac9",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c979e4990e764d2589f49f3bdc5eeb88",
            "value": 231508
          }
        },
        "038bec5cbf874f158cf951c726f31fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9738c8e5743140aca2352a580309751d",
            "placeholder": "​",
            "style": "IPY_MODEL_9ab54032c4304485afd7fbd03f5af771",
            "value": " 232k/232k [00:00&lt;00:00, 3.55MB/s]"
          }
        },
        "5b136bf84c274ec98733224f8b7c41cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaeb681ce16847fdbd6001c202999251": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf19688eeea04df6a53faf8153d8d25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1241d394ca044ec9978fcb4627bffac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c979e4990e764d2589f49f3bdc5eeb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9738c8e5743140aca2352a580309751d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ab54032c4304485afd7fbd03f5af771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ff40f2c3e1345c789898e773c6e9010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3573d7a2e49142cd8fb4782ccd826f53",
              "IPY_MODEL_79f7287fefe4425b84474b65f97dbdf1",
              "IPY_MODEL_83f6f6eed9ce4cc49aac082b7ae0a16d"
            ],
            "layout": "IPY_MODEL_85c6ddbdf6e54e1690dd4d98a714bc2f"
          }
        },
        "3573d7a2e49142cd8fb4782ccd826f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fd0cfd908174446bd4aa207f4e32a88",
            "placeholder": "​",
            "style": "IPY_MODEL_f4017dfb43674bc5b8352e7cabb03d27",
            "value": "tokenizer.json: 100%"
          }
        },
        "79f7287fefe4425b84474b65f97dbdf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4322201a186a48188c1b68855df8ce07",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de9d42ea2b124a5a82a778d72ad7a2f9",
            "value": 466062
          }
        },
        "83f6f6eed9ce4cc49aac082b7ae0a16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61cf7e5d304a403381206e23d49ef9c1",
            "placeholder": "​",
            "style": "IPY_MODEL_d8f2742ff2b748d5ad5d9fe2a8675713",
            "value": " 466k/466k [00:00&lt;00:00, 599kB/s]"
          }
        },
        "85c6ddbdf6e54e1690dd4d98a714bc2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fd0cfd908174446bd4aa207f4e32a88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4017dfb43674bc5b8352e7cabb03d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4322201a186a48188c1b68855df8ce07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9d42ea2b124a5a82a778d72ad7a2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61cf7e5d304a403381206e23d49ef9c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8f2742ff2b748d5ad5d9fe2a8675713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5df6298ebe14cf38838b7a191593a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c98c4985cb547488dc1335cb7ef839f",
              "IPY_MODEL_b526ba58bc304fd3a246668dfccb22e9",
              "IPY_MODEL_0b2f6d8390fe48539ba3686784d43a9d"
            ],
            "layout": "IPY_MODEL_93d4a83c49864adea34ac90124d7315c"
          }
        },
        "2c98c4985cb547488dc1335cb7ef839f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed58bdc72cfe42819b6b529a9f8d8ae5",
            "placeholder": "​",
            "style": "IPY_MODEL_3b0bb904feca4b359280d83b706d66f3",
            "value": "model.safetensors: 100%"
          }
        },
        "b526ba58bc304fd3a246668dfccb22e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baacf5df2a754e80b7f90629beae635f",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6986db88f4d94dae8003b819ca283381",
            "value": 440449768
          }
        },
        "0b2f6d8390fe48539ba3686784d43a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7eb8a57fa0e44d7acdb0bc89c416925",
            "placeholder": "​",
            "style": "IPY_MODEL_28610adce4774fe28ff8bb5e199a52b9",
            "value": " 440M/440M [00:03&lt;00:00, 157MB/s]"
          }
        },
        "93d4a83c49864adea34ac90124d7315c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed58bdc72cfe42819b6b529a9f8d8ae5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b0bb904feca4b359280d83b706d66f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baacf5df2a754e80b7f90629beae635f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6986db88f4d94dae8003b819ca283381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7eb8a57fa0e44d7acdb0bc89c416925": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28610adce4774fe28ff8bb5e199a52b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21b9c1a660784c308e0ba1ecb840b1f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fa539ff355545948b7305d6c403566c",
              "IPY_MODEL_efe8d74d285b4c3bb35d878daf52c652",
              "IPY_MODEL_7842b8946aa34859b17b1b4db6688def"
            ],
            "layout": "IPY_MODEL_9bf3ae3e830c4e65a0879ff90424ee03"
          }
        },
        "9fa539ff355545948b7305d6c403566c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17c2c90e77064731911659f6353b5326",
            "placeholder": "​",
            "style": "IPY_MODEL_3d8bc1b0a9a547c1b6bd901e6a4d3c9b",
            "value": "Downloading .gitattributes: 100%"
          }
        },
        "efe8d74d285b4c3bb35d878daf52c652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_697dd1ebcf274f4aa0212e52e24991c6",
            "max": 1229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e01bc2bdbbb46cdbacd8eab6ff11fa2",
            "value": 1229
          }
        },
        "7842b8946aa34859b17b1b4db6688def": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a0b56f1d61449dcb17def2d60d3abb6",
            "placeholder": "​",
            "style": "IPY_MODEL_af09f0d19f5a4b67a3e0c0938eda3e46",
            "value": " 1.23k/1.23k [00:00&lt;00:00, 32.0kB/s]"
          }
        },
        "9bf3ae3e830c4e65a0879ff90424ee03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17c2c90e77064731911659f6353b5326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d8bc1b0a9a547c1b6bd901e6a4d3c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "697dd1ebcf274f4aa0212e52e24991c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e01bc2bdbbb46cdbacd8eab6ff11fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a0b56f1d61449dcb17def2d60d3abb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af09f0d19f5a4b67a3e0c0938eda3e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f9c80df55f549b8b4fd3325c97bcfaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_748eebcb522946ddb2f8986eac2cea67",
              "IPY_MODEL_21eec928981e4cf1b97e90cfd9a1e665",
              "IPY_MODEL_164bbbb863954230b090a353b011a312"
            ],
            "layout": "IPY_MODEL_0e012482313b4008a4711b6fc2b5fe32"
          }
        },
        "748eebcb522946ddb2f8986eac2cea67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75aab5ca85f94412b3b08f3e969d98f2",
            "placeholder": "​",
            "style": "IPY_MODEL_d938bf7c9fcd468a8ab8aa02ce02fb77",
            "value": "Downloading config.json: 100%"
          }
        },
        "21eec928981e4cf1b97e90cfd9a1e665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2411e001ad8443b4b71a94ba360f4227",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e11d78086dd4b8a8946eb950bb3cc93",
            "value": 190
          }
        },
        "164bbbb863954230b090a353b011a312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3751dba81894761b9ace1479765a0ab",
            "placeholder": "​",
            "style": "IPY_MODEL_c68fb56df1044679af67b43aa29c1e71",
            "value": " 190/190 [00:00&lt;00:00, 6.60kB/s]"
          }
        },
        "0e012482313b4008a4711b6fc2b5fe32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75aab5ca85f94412b3b08f3e969d98f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d938bf7c9fcd468a8ab8aa02ce02fb77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2411e001ad8443b4b71a94ba360f4227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e11d78086dd4b8a8946eb950bb3cc93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3751dba81894761b9ace1479765a0ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c68fb56df1044679af67b43aa29c1e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "391af866c46a4617b845bf631b3b03e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d8bdee8f63942b88cafc8a336e67161",
              "IPY_MODEL_279641a7019740a8a366c5526a338dc0",
              "IPY_MODEL_b0d23ffd9e8f4956a846d1386f1de8c9"
            ],
            "layout": "IPY_MODEL_39f0140e426d48ac8b274a9331996f92"
          }
        },
        "8d8bdee8f63942b88cafc8a336e67161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d54c12523fe4e7bbc515e5c2851543c",
            "placeholder": "​",
            "style": "IPY_MODEL_ca40401179914b4fbf8a1971168ed4e0",
            "value": "Downloading README.md: 100%"
          }
        },
        "279641a7019740a8a366c5526a338dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_385df39e51774a1794f9a5dfd79d46d3",
            "max": 10454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73cb7c4a64354811a32379535553d9f0",
            "value": 10454
          }
        },
        "b0d23ffd9e8f4956a846d1386f1de8c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a62c7955e654a3abb0557f88e8e20a3",
            "placeholder": "​",
            "style": "IPY_MODEL_4f4e33ae789246f991059e8343675f1d",
            "value": " 10.5k/10.5k [00:00&lt;00:00, 243kB/s]"
          }
        },
        "39f0140e426d48ac8b274a9331996f92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d54c12523fe4e7bbc515e5c2851543c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca40401179914b4fbf8a1971168ed4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "385df39e51774a1794f9a5dfd79d46d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73cb7c4a64354811a32379535553d9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a62c7955e654a3abb0557f88e8e20a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f4e33ae789246f991059e8343675f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "789e3f12b65148b484f4f7b42b5afcfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6267989b3940432c90c7353a077cc823",
              "IPY_MODEL_11fe7966b15a4ce89ef3e40fb6ea4d8f",
              "IPY_MODEL_96be8357107441f08f21105ed427a048"
            ],
            "layout": "IPY_MODEL_531bd0f2a1424df29c5156e5e9eef718"
          }
        },
        "6267989b3940432c90c7353a077cc823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d90793cf1af5437792a478dca88782ea",
            "placeholder": "​",
            "style": "IPY_MODEL_54c022ad418541c2a5ebd108d2e2af41",
            "value": "Downloading config.json: 100%"
          }
        },
        "11fe7966b15a4ce89ef3e40fb6ea4d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d00733804ddb4b3e97887efb64f8026c",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e84d470e760490aa72bd28f56a29a65",
            "value": 612
          }
        },
        "96be8357107441f08f21105ed427a048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c7e800e0d89488d9d887a232fe13c92",
            "placeholder": "​",
            "style": "IPY_MODEL_d5ad1408335349828238114c5dca07a4",
            "value": " 612/612 [00:00&lt;00:00, 16.5kB/s]"
          }
        },
        "531bd0f2a1424df29c5156e5e9eef718": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d90793cf1af5437792a478dca88782ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c022ad418541c2a5ebd108d2e2af41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d00733804ddb4b3e97887efb64f8026c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e84d470e760490aa72bd28f56a29a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c7e800e0d89488d9d887a232fe13c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5ad1408335349828238114c5dca07a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbb4eba603174610baf0bdc672182893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74dbf2c8860f403c8c64a7941fed0e11",
              "IPY_MODEL_3bd10ceff40c40bbad9ed11dc7124e37",
              "IPY_MODEL_c23786ac2eb44ee6adf82c748c89778a"
            ],
            "layout": "IPY_MODEL_5d0a1f16e1b64ae982767fbc9f8e578c"
          }
        },
        "74dbf2c8860f403c8c64a7941fed0e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75da60882c4a4e10849019ba532a9b90",
            "placeholder": "​",
            "style": "IPY_MODEL_0f4c698571ed4dfab1bebdd6c144fd0b",
            "value": "Downloading (…)ce_transformers.json: 100%"
          }
        },
        "3bd10ceff40c40bbad9ed11dc7124e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14fb8c3e2fd34a3cbb960eeddad4a56c",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_809394c834394a04b133d34791d69973",
            "value": 116
          }
        },
        "c23786ac2eb44ee6adf82c748c89778a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8623a0cd5e840b1a219cb43594ef094",
            "placeholder": "​",
            "style": "IPY_MODEL_cb41558222914ea58f2902973e54e380",
            "value": " 116/116 [00:00&lt;00:00, 4.00kB/s]"
          }
        },
        "5d0a1f16e1b64ae982767fbc9f8e578c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75da60882c4a4e10849019ba532a9b90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f4c698571ed4dfab1bebdd6c144fd0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14fb8c3e2fd34a3cbb960eeddad4a56c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "809394c834394a04b133d34791d69973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8623a0cd5e840b1a219cb43594ef094": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb41558222914ea58f2902973e54e380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b113c4fce3547779f6f78b91a24e814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3179230be963456486080f8b0032dc16",
              "IPY_MODEL_f7d79d8f5ee14b9ca65f3fcd2e154684",
              "IPY_MODEL_b6be46a4d4e6458e8e3bba725ea4a23a"
            ],
            "layout": "IPY_MODEL_0125e7f5ec244a0ba2205e6d13c50a02"
          }
        },
        "3179230be963456486080f8b0032dc16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f43c1a7a49bb443588080556db37845d",
            "placeholder": "​",
            "style": "IPY_MODEL_c4ddc5b7954947c4b50e926cb39545de",
            "value": "Downloading data_config.json: 100%"
          }
        },
        "f7d79d8f5ee14b9ca65f3fcd2e154684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54c5a2e7af284f66800922f7f763cf72",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff444354632e4346ad7ad9eb44c5cbac",
            "value": 39265
          }
        },
        "b6be46a4d4e6458e8e3bba725ea4a23a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b2ae483f1c24303a37d355f508f339f",
            "placeholder": "​",
            "style": "IPY_MODEL_32a012d48b684da4acdbc5896ca2b9fd",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 1.48MB/s]"
          }
        },
        "0125e7f5ec244a0ba2205e6d13c50a02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f43c1a7a49bb443588080556db37845d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4ddc5b7954947c4b50e926cb39545de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54c5a2e7af284f66800922f7f763cf72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff444354632e4346ad7ad9eb44c5cbac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b2ae483f1c24303a37d355f508f339f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32a012d48b684da4acdbc5896ca2b9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30ba7305db8447d28c1275413c59db63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0cf200c4bd364de9a7f8467d7c3ed4e6",
              "IPY_MODEL_7bf819a35c344fae9103a0e0578e4231",
              "IPY_MODEL_3dbff80ff9534867a7689208091fba64"
            ],
            "layout": "IPY_MODEL_57845b0cab574d99ab59ca6ba39d483c"
          }
        },
        "0cf200c4bd364de9a7f8467d7c3ed4e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be64b3ca458a44f2895a87a9019e4b27",
            "placeholder": "​",
            "style": "IPY_MODEL_b750a4e3cdf94c4ea76d3ad3ccb64dce",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "7bf819a35c344fae9103a0e0578e4231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7ed1f7c0cb9418d9c03e7289644a1b9",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9851dc3c4aec44f993082c4c04362adf",
            "value": 90868376
          }
        },
        "3dbff80ff9534867a7689208091fba64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c8aa8a7ac924d8ba319c8b6aaf8a69a",
            "placeholder": "​",
            "style": "IPY_MODEL_d30da5ee8a6f42fda8e8037a2c0c946a",
            "value": " 90.9M/90.9M [00:01&lt;00:00, 109MB/s]"
          }
        },
        "57845b0cab574d99ab59ca6ba39d483c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be64b3ca458a44f2895a87a9019e4b27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b750a4e3cdf94c4ea76d3ad3ccb64dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7ed1f7c0cb9418d9c03e7289644a1b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9851dc3c4aec44f993082c4c04362adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c8aa8a7ac924d8ba319c8b6aaf8a69a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d30da5ee8a6f42fda8e8037a2c0c946a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "190a9dc3578f43dfb1cb1cc4a9cbd3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f9bdeb2577e4f22a8b8795c9c07f1bf",
              "IPY_MODEL_6c23dd29a9794b9e9f074f4a16767ade",
              "IPY_MODEL_9365838a39854986b35ff5158c5047f5"
            ],
            "layout": "IPY_MODEL_417453bb846d49d2a3d961b89b1acac4"
          }
        },
        "8f9bdeb2577e4f22a8b8795c9c07f1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ead4562d31ef4f84a9731ac1c33a8a3f",
            "placeholder": "​",
            "style": "IPY_MODEL_deda8f8422344143bcfe84fe066f8b9b",
            "value": "Downloading model.onnx: 100%"
          }
        },
        "6c23dd29a9794b9e9f074f4a16767ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4f3b453909444eebb1d528f623235b1",
            "max": 90405214,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_553ab7d2d4044318b7219c0a0dccdc2b",
            "value": 90405214
          }
        },
        "9365838a39854986b35ff5158c5047f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae9c5a2f03b84b95a140589d5793f500",
            "placeholder": "​",
            "style": "IPY_MODEL_d7920aa190b3416ba7dc58e2f4c25e44",
            "value": " 90.4M/90.4M [00:01&lt;00:00, 70.9MB/s]"
          }
        },
        "417453bb846d49d2a3d961b89b1acac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead4562d31ef4f84a9731ac1c33a8a3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deda8f8422344143bcfe84fe066f8b9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4f3b453909444eebb1d528f623235b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "553ab7d2d4044318b7219c0a0dccdc2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae9c5a2f03b84b95a140589d5793f500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7920aa190b3416ba7dc58e2f4c25e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d3f414836354ef7adf0940feabc6f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a45fc2b47c304f1695793e22f1d78393",
              "IPY_MODEL_4b0e3ee7c5d740d9a5da377d77a2d890",
              "IPY_MODEL_c8fe51a260234942ac6054cc22da3f46"
            ],
            "layout": "IPY_MODEL_d370fcc3992b421b83fb198f44ed9e27"
          }
        },
        "a45fc2b47c304f1695793e22f1d78393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5876be384bae4c6f94f8f330b4a49aa9",
            "placeholder": "​",
            "style": "IPY_MODEL_a1e5ef9e86c74ddabd3645c47662e15b",
            "value": "Downloading model_O1.onnx: 100%"
          }
        },
        "4b0e3ee7c5d740d9a5da377d77a2d890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0717d4b85b5c4e5598e2048e3cb75d2e",
            "max": 90360328,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c36d76db4326455a87a2859b556dd446",
            "value": 90360328
          }
        },
        "c8fe51a260234942ac6054cc22da3f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c417111d075e4577b47ccc1f1d3d0d05",
            "placeholder": "​",
            "style": "IPY_MODEL_859206daf87d422e9add7f228720abd5",
            "value": " 90.4M/90.4M [00:04&lt;00:00, 9.10MB/s]"
          }
        },
        "d370fcc3992b421b83fb198f44ed9e27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5876be384bae4c6f94f8f330b4a49aa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1e5ef9e86c74ddabd3645c47662e15b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0717d4b85b5c4e5598e2048e3cb75d2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c36d76db4326455a87a2859b556dd446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c417111d075e4577b47ccc1f1d3d0d05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "859206daf87d422e9add7f228720abd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95b1be3f478d4944bdd1e8cfc073569f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b58914f3b2b431581d93bb42ab590c0",
              "IPY_MODEL_527a9d185a74479d99ae965f74698c0c",
              "IPY_MODEL_7876175021e04f3791c1913a2b114a3f"
            ],
            "layout": "IPY_MODEL_044dbcb880cb4a68b77a46a457d0d1ee"
          }
        },
        "7b58914f3b2b431581d93bb42ab590c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3f2c5b837f549b3947c662d94a10640",
            "placeholder": "​",
            "style": "IPY_MODEL_e35ae662710c4fb2a20bd25fc8828ba0",
            "value": "Downloading model_O2.onnx: 100%"
          }
        },
        "527a9d185a74479d99ae965f74698c0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e27af4ca530045c2a70528f68b4c9546",
            "max": 90326566,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ce5bad750cf4634bde7dd869bc18407",
            "value": 90326566
          }
        },
        "7876175021e04f3791c1913a2b114a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fd45822df8d455495cbee189e4e51c1",
            "placeholder": "​",
            "style": "IPY_MODEL_5cf8ec2d38e646ec829f8de492ea5c7f",
            "value": " 90.3M/90.3M [00:00&lt;00:00, 139MB/s]"
          }
        },
        "044dbcb880cb4a68b77a46a457d0d1ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f2c5b837f549b3947c662d94a10640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35ae662710c4fb2a20bd25fc8828ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e27af4ca530045c2a70528f68b4c9546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ce5bad750cf4634bde7dd869bc18407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fd45822df8d455495cbee189e4e51c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cf8ec2d38e646ec829f8de492ea5c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb30e11285a84e05807b9683d1a8b4f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7df0b4a9f33246e0be5424be9c4287a8",
              "IPY_MODEL_ba622fb44fc747f7804ff8c377eca070",
              "IPY_MODEL_6f3a6d9e149b4d88af3ad81badffbb83"
            ],
            "layout": "IPY_MODEL_1528f186f2354131b40812ff540102a5"
          }
        },
        "7df0b4a9f33246e0be5424be9c4287a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c8ebf3bd8e4254b31a2141bef2bca9",
            "placeholder": "​",
            "style": "IPY_MODEL_18993571215445b79d80c7089facfa77",
            "value": "Downloading model_O3.onnx: 100%"
          }
        },
        "ba622fb44fc747f7804ff8c377eca070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdfe4d0801c949ee9f176c95bca65cac",
            "max": 90326497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18f8b5e3b1db48f29eeb1ddb7c66bc7e",
            "value": 90326497
          }
        },
        "6f3a6d9e149b4d88af3ad81badffbb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43056684e452496ab2a9d0613f01fa69",
            "placeholder": "​",
            "style": "IPY_MODEL_02352cb11a424dc49a24bac9895caf33",
            "value": " 90.3M/90.3M [00:00&lt;00:00, 118MB/s]"
          }
        },
        "1528f186f2354131b40812ff540102a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74c8ebf3bd8e4254b31a2141bef2bca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18993571215445b79d80c7089facfa77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdfe4d0801c949ee9f176c95bca65cac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18f8b5e3b1db48f29eeb1ddb7c66bc7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43056684e452496ab2a9d0613f01fa69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02352cb11a424dc49a24bac9895caf33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "460e25a767e345caaf6c49edb16b70a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d6acea39af84861bb26983853ee12e8",
              "IPY_MODEL_44c5af1e46f34466b3f1ddd5e70dd177",
              "IPY_MODEL_d8b762198f9d48729cca6f949d3de030"
            ],
            "layout": "IPY_MODEL_27e32956646b48eba1617198149edf39"
          }
        },
        "2d6acea39af84861bb26983853ee12e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c774ebfe4be040d7ac4dd7bc95df50ce",
            "placeholder": "​",
            "style": "IPY_MODEL_667a24c673a2497ebea86e54676de918",
            "value": "Downloading model_O4.onnx: 100%"
          }
        },
        "44c5af1e46f34466b3f1ddd5e70dd177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5907fedc921e4eb1ab69420d44e95385",
            "max": 45212349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c0e652873ee41a08931647fbb17ea9d",
            "value": 45212349
          }
        },
        "d8b762198f9d48729cca6f949d3de030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5681615e58e44549bf1be21585ed43a",
            "placeholder": "​",
            "style": "IPY_MODEL_e72804f552154f928b213dd35f1ef0e1",
            "value": " 45.2M/45.2M [00:00&lt;00:00, 151MB/s]"
          }
        },
        "27e32956646b48eba1617198149edf39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c774ebfe4be040d7ac4dd7bc95df50ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "667a24c673a2497ebea86e54676de918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5907fedc921e4eb1ab69420d44e95385": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c0e652873ee41a08931647fbb17ea9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5681615e58e44549bf1be21585ed43a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e72804f552154f928b213dd35f1ef0e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70de958cd71b438590b368704a07b694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19faff1bb7b94b75987c45c9111c77aa",
              "IPY_MODEL_019b41e9c9244f1b8e0e82934877485f",
              "IPY_MODEL_0b00495ff2f44cb39dbb2905b4f8e02f"
            ],
            "layout": "IPY_MODEL_55bb29f0e62c44caa3dd5038e6c12b9e"
          }
        },
        "19faff1bb7b94b75987c45c9111c77aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36cf54ce7a814ea8b90a7aea89d3bc09",
            "placeholder": "​",
            "style": "IPY_MODEL_ced83df6957d4af6b0b6050508c076bf",
            "value": "Downloading (…)el_qint8_avx512.onnx: 100%"
          }
        },
        "019b41e9c9244f1b8e0e82934877485f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f8b0e795dbb4dd38937edd1016e961d",
            "max": 23026053,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd990f90070045dbafc114d3f33f872d",
            "value": 23026053
          }
        },
        "0b00495ff2f44cb39dbb2905b4f8e02f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19e0732fc1e244618464a2a6f85ea98b",
            "placeholder": "​",
            "style": "IPY_MODEL_ee594cf7fee249b4aa14cf519efd890c",
            "value": " 23.0M/23.0M [00:00&lt;00:00, 141MB/s]"
          }
        },
        "55bb29f0e62c44caa3dd5038e6c12b9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36cf54ce7a814ea8b90a7aea89d3bc09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced83df6957d4af6b0b6050508c076bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f8b0e795dbb4dd38937edd1016e961d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd990f90070045dbafc114d3f33f872d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19e0732fc1e244618464a2a6f85ea98b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee594cf7fee249b4aa14cf519efd890c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5b9d3a273d94dd18c23c4000fbae6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_176b87035b1a4948a810f621ddf1a4a3",
              "IPY_MODEL_0fe8e070578d4b73af7750556aa421f0",
              "IPY_MODEL_dd356865a8cb41e6a4587366366c9c54"
            ],
            "layout": "IPY_MODEL_f3d66828afa846328a13b5127e2b9c53"
          }
        },
        "176b87035b1a4948a810f621ddf1a4a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92fe14b127a64327bd6cf9183fe8e0aa",
            "placeholder": "​",
            "style": "IPY_MODEL_274b743e80994939bb8b56e51eccff79",
            "value": "Downloading (…)el_qint8_avx512.onnx: 100%"
          }
        },
        "0fe8e070578d4b73af7750556aa421f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f6cfb8c290d4786a5fcc0b64d3ac4e5",
            "max": 23026053,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55ddd8ff412f4a838b0c8b15774b0f88",
            "value": 23026053
          }
        },
        "dd356865a8cb41e6a4587366366c9c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02adbc1e1b5845e19d13062608a218e4",
            "placeholder": "​",
            "style": "IPY_MODEL_a7c2bebda6894b519f7f55ac52b161bc",
            "value": " 23.0M/23.0M [00:00&lt;00:00, 79.0MB/s]"
          }
        },
        "f3d66828afa846328a13b5127e2b9c53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92fe14b127a64327bd6cf9183fe8e0aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "274b743e80994939bb8b56e51eccff79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f6cfb8c290d4786a5fcc0b64d3ac4e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ddd8ff412f4a838b0c8b15774b0f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02adbc1e1b5845e19d13062608a218e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7c2bebda6894b519f7f55ac52b161bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cacea61ed4f74e0188012916d44ce5b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cc9888241b443769028a26ed3f3cc77",
              "IPY_MODEL_7d671105d3a945a1a6a91436ca01d32a",
              "IPY_MODEL_b507f58b762b4e89951e3122d840438a"
            ],
            "layout": "IPY_MODEL_8848baf74e2b405290235cfa1cea541d"
          }
        },
        "1cc9888241b443769028a26ed3f3cc77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9594a7bcb2d4668a2f77ab713678937",
            "placeholder": "​",
            "style": "IPY_MODEL_6697cc32240e4c3092974dd7fb881cf5",
            "value": "Downloading (…)el_qint8_avx512.onnx: 100%"
          }
        },
        "7d671105d3a945a1a6a91436ca01d32a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de9319c6f01347c9bf91e308d9811730",
            "max": 23026053,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71ce3c729d0c48708cd6006a2fc050aa",
            "value": 23026053
          }
        },
        "b507f58b762b4e89951e3122d840438a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aa6a9adc0504dec895775aaeacdbc66",
            "placeholder": "​",
            "style": "IPY_MODEL_c00e6001b4a94cdd9bd54c29bc15bed2",
            "value": " 23.0M/23.0M [00:00&lt;00:00, 121MB/s]"
          }
        },
        "8848baf74e2b405290235cfa1cea541d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9594a7bcb2d4668a2f77ab713678937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6697cc32240e4c3092974dd7fb881cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de9319c6f01347c9bf91e308d9811730": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71ce3c729d0c48708cd6006a2fc050aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1aa6a9adc0504dec895775aaeacdbc66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c00e6001b4a94cdd9bd54c29bc15bed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11f276e5b7fc47fbbf9461b7a7b8c563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14a04cc0ed6a4d4b907c2923b422a1ee",
              "IPY_MODEL_c68a6f39d2b04407b93f0a64c6ff93eb",
              "IPY_MODEL_ccafb1cf47bd475ea71b3f5d683e878b"
            ],
            "layout": "IPY_MODEL_922e300a9af3417098b037ed860ac137"
          }
        },
        "14a04cc0ed6a4d4b907c2923b422a1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0691f73f2daf474c86b7913a8a848fa0",
            "placeholder": "​",
            "style": "IPY_MODEL_02ac15342b1440b6bc329f9c96b9662d",
            "value": "Downloading model_quint8_avx2.onnx: 100%"
          }
        },
        "c68a6f39d2b04407b93f0a64c6ff93eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbc268df141b42e7865029c6449784ce",
            "max": 23046789,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d61101cfc2fb476b85008ef372b4c1b5",
            "value": 23046789
          }
        },
        "ccafb1cf47bd475ea71b3f5d683e878b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_866b249862a54902a3bdc615209a7b6c",
            "placeholder": "​",
            "style": "IPY_MODEL_0956271499034b1497664122a94e1b27",
            "value": " 23.0M/23.0M [00:00&lt;00:00, 83.5MB/s]"
          }
        },
        "922e300a9af3417098b037ed860ac137": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0691f73f2daf474c86b7913a8a848fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02ac15342b1440b6bc329f9c96b9662d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbc268df141b42e7865029c6449784ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d61101cfc2fb476b85008ef372b4c1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "866b249862a54902a3bdc615209a7b6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0956271499034b1497664122a94e1b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd4cc0ed7e5d41cfb512f5079f569605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00306243df2344b5b3dfbc5266214b18",
              "IPY_MODEL_c8e6490e0c0c480fa67cc3e31cebcd85",
              "IPY_MODEL_e97d2f366c4e4abaa0eebaf6d6459a9a"
            ],
            "layout": "IPY_MODEL_42f61fff1fba40cb9ae1576a69196991"
          }
        },
        "00306243df2344b5b3dfbc5266214b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a63932f61aa4f23bdba662593aa1463",
            "placeholder": "​",
            "style": "IPY_MODEL_fc1bd52bdfbb4897953250a9a4f8cce6",
            "value": "Downloading openvino_model.bin: 100%"
          }
        },
        "c8e6490e0c0c480fa67cc3e31cebcd85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_251bc9a6541140349f47b9e7d9c27da2",
            "max": 90265744,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46cade80ed4e439abdfab03f5dba1697",
            "value": 90265744
          }
        },
        "e97d2f366c4e4abaa0eebaf6d6459a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60e3ebed8f9b49c9839d592453cd1187",
            "placeholder": "​",
            "style": "IPY_MODEL_9c747311154c49d2ad6886bc511ac844",
            "value": " 90.3M/90.3M [00:00&lt;00:00, 103MB/s]"
          }
        },
        "42f61fff1fba40cb9ae1576a69196991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a63932f61aa4f23bdba662593aa1463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc1bd52bdfbb4897953250a9a4f8cce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "251bc9a6541140349f47b9e7d9c27da2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46cade80ed4e439abdfab03f5dba1697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60e3ebed8f9b49c9839d592453cd1187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c747311154c49d2ad6886bc511ac844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb4e2ec03f294456a24d83c879fb233e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d38d49ae7e9420d8b5b569897ad91a1",
              "IPY_MODEL_0b3b88d513bd418da7b00fe0a97053ca",
              "IPY_MODEL_d3455e96ce1c4ac485d12f3cf35db7e3"
            ],
            "layout": "IPY_MODEL_56f6639179a740fb8995178e8472fda4"
          }
        },
        "8d38d49ae7e9420d8b5b569897ad91a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_936b2ebd632d40cfa424f8670319cfc6",
            "placeholder": "​",
            "style": "IPY_MODEL_ce0f3f2927b94abcbbeef3135afb7b3d",
            "value": "Downloading openvino_model.xml: 100%"
          }
        },
        "0b3b88d513bd418da7b00fe0a97053ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c2f98bbd7bb488fa3b17d3dfb1e386a",
            "max": 211315,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2002b9259384420197006be847006007",
            "value": 211315
          }
        },
        "d3455e96ce1c4ac485d12f3cf35db7e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_028e098da36944828bb9c9ddaf391001",
            "placeholder": "​",
            "style": "IPY_MODEL_1133ed069729414bac9ca77bd3cdca7c",
            "value": " 211k/211k [00:00&lt;00:00, 8.68MB/s]"
          }
        },
        "56f6639179a740fb8995178e8472fda4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "936b2ebd632d40cfa424f8670319cfc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce0f3f2927b94abcbbeef3135afb7b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c2f98bbd7bb488fa3b17d3dfb1e386a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2002b9259384420197006be847006007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "028e098da36944828bb9c9ddaf391001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1133ed069729414bac9ca77bd3cdca7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6b5d1f826b94c8c84258142eb4b4ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d10d5b3c0e514684a34fa993419318da",
              "IPY_MODEL_6682168f66d643daae293a5bf49cd812",
              "IPY_MODEL_f01a4a5a551f40609f177957ab0cdfc7"
            ],
            "layout": "IPY_MODEL_f12de67d70bc4adbb28d8a294cce64d4"
          }
        },
        "d10d5b3c0e514684a34fa993419318da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b5cd0af559847d5a60d7c68f2494f0d",
            "placeholder": "​",
            "style": "IPY_MODEL_db93103c42b7481c8507359e78fb6104",
            "value": "Downloading (…)_qint8_quantized.bin: 100%"
          }
        },
        "6682168f66d643daae293a5bf49cd812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d03c6eab0add4d6baff1ce2f01642c75",
            "max": 22933664,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b917404e30ba4f5d9ef149841c7c5ac5",
            "value": 22933664
          }
        },
        "f01a4a5a551f40609f177957ab0cdfc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece85828402b4e77b24f16be970b4ef2",
            "placeholder": "​",
            "style": "IPY_MODEL_46ec75b0d2334ac2a874beba24826a80",
            "value": " 22.9M/22.9M [00:00&lt;00:00, 162MB/s]"
          }
        },
        "f12de67d70bc4adbb28d8a294cce64d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b5cd0af559847d5a60d7c68f2494f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db93103c42b7481c8507359e78fb6104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d03c6eab0add4d6baff1ce2f01642c75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b917404e30ba4f5d9ef149841c7c5ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ece85828402b4e77b24f16be970b4ef2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ec75b0d2334ac2a874beba24826a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7fc5ac85281422ab18bfe6270846e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2ded8ee631c4da28dbe2f6c8b8451dd",
              "IPY_MODEL_d01f19ae4685434786de64d97ee61fb1",
              "IPY_MODEL_9cc418dbeb5b4f5baf81dad6317d70af"
            ],
            "layout": "IPY_MODEL_744940a4f3e74cecb69a40a5a3dcbe9f"
          }
        },
        "d2ded8ee631c4da28dbe2f6c8b8451dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf668b5f78c74f10b40fb98e7a5b8366",
            "placeholder": "​",
            "style": "IPY_MODEL_97944e907ef24b14b0ea1cc8b174df38",
            "value": "Downloading (…)_qint8_quantized.xml: 100%"
          }
        },
        "d01f19ae4685434786de64d97ee61fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06381f9e5fd247cd9d6e822a18a73706",
            "max": 368006,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea21a6b861c34d4b918579f15dcc4d38",
            "value": 368006
          }
        },
        "9cc418dbeb5b4f5baf81dad6317d70af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c73a0a1832624952b51e57b6264434db",
            "placeholder": "​",
            "style": "IPY_MODEL_d1152ecd8b40425a9cd8b56a2b68f6eb",
            "value": " 368k/368k [00:00&lt;00:00, 10.3MB/s]"
          }
        },
        "744940a4f3e74cecb69a40a5a3dcbe9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf668b5f78c74f10b40fb98e7a5b8366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97944e907ef24b14b0ea1cc8b174df38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06381f9e5fd247cd9d6e822a18a73706": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea21a6b861c34d4b918579f15dcc4d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c73a0a1832624952b51e57b6264434db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1152ecd8b40425a9cd8b56a2b68f6eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e4d666f31554b9496a5a881af8513bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a67a1a28192947f1953de60940c85379",
              "IPY_MODEL_2226912146504e68bab6d9b9d6f2d627",
              "IPY_MODEL_98a0c47228964f3c9a5e6a0058f7ed1a"
            ],
            "layout": "IPY_MODEL_1e833b2db4e641f88e666cd614c18d8d"
          }
        },
        "a67a1a28192947f1953de60940c85379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d680afe295d47e2a524f145a2d5bbdd",
            "placeholder": "​",
            "style": "IPY_MODEL_2c23fe286dfd40ca8cd814619286a09e",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "2226912146504e68bab6d9b9d6f2d627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d60fa3e24874d5ea34a00f58b0d71e0",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39b8903ae41e4067a1b1a89b1fe3ba7f",
            "value": 90888945
          }
        },
        "98a0c47228964f3c9a5e6a0058f7ed1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4346003f9e194825938db3d5d0bcaf34",
            "placeholder": "​",
            "style": "IPY_MODEL_d098200f356a4ece8732250eb89684b9",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 122MB/s]"
          }
        },
        "1e833b2db4e641f88e666cd614c18d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d680afe295d47e2a524f145a2d5bbdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c23fe286dfd40ca8cd814619286a09e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d60fa3e24874d5ea34a00f58b0d71e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39b8903ae41e4067a1b1a89b1fe3ba7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4346003f9e194825938db3d5d0bcaf34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d098200f356a4ece8732250eb89684b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aeb35e37cc2b426ab35306fe643ddc61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5291d5258074b6ba02a938cc2fe2f84",
              "IPY_MODEL_35b1b77f4ea548edbca5716c832b2e11",
              "IPY_MODEL_b29728d79a074bb1bcdf9cec0caf005e"
            ],
            "layout": "IPY_MODEL_dc8d34ecc135436f8fde1aabc287f7cb"
          }
        },
        "a5291d5258074b6ba02a938cc2fe2f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1169ba14308340348c5cad3ecbccd5cf",
            "placeholder": "​",
            "style": "IPY_MODEL_25f3a662c597498cac31f1a44878ba76",
            "value": "Downloading (…)nce_bert_config.json: 100%"
          }
        },
        "35b1b77f4ea548edbca5716c832b2e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66730a6045924dc59a0b9a15b3d40319",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1b00674cba04318969f81e7d1e942ee",
            "value": 53
          }
        },
        "b29728d79a074bb1bcdf9cec0caf005e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99f9f8a8c07f4d058ea0972bf3f4efad",
            "placeholder": "​",
            "style": "IPY_MODEL_936aade77e3b47a4b4af01d50d1e875b",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.12kB/s]"
          }
        },
        "dc8d34ecc135436f8fde1aabc287f7cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1169ba14308340348c5cad3ecbccd5cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f3a662c597498cac31f1a44878ba76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66730a6045924dc59a0b9a15b3d40319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1b00674cba04318969f81e7d1e942ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99f9f8a8c07f4d058ea0972bf3f4efad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "936aade77e3b47a4b4af01d50d1e875b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf717310b00a420db4037257d1f44bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28fb3061611242adbdf09c6437c6e981",
              "IPY_MODEL_d90e8d0ce88c4ddaaceac548c27ecdb4",
              "IPY_MODEL_9acf91078d664327895a1dcdb245ed87"
            ],
            "layout": "IPY_MODEL_cf7c41209f6142f68e8ceb25f12ed3fc"
          }
        },
        "28fb3061611242adbdf09c6437c6e981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c636991f9494a849a95fa86bf09bea7",
            "placeholder": "​",
            "style": "IPY_MODEL_aaf082509c2147d38b58a2977bc3ed3d",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "d90e8d0ce88c4ddaaceac548c27ecdb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de4a3555ce4147538db40bd84d34f7ca",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e939436ee284bda88ff29aa9b1c58df",
            "value": 112
          }
        },
        "9acf91078d664327895a1dcdb245ed87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7603d22cd3744cbb95d6a9643e78897b",
            "placeholder": "​",
            "style": "IPY_MODEL_4d72d54f00a64e9a942fc96433e15b72",
            "value": " 112/112 [00:00&lt;00:00, 9.47kB/s]"
          }
        },
        "cf7c41209f6142f68e8ceb25f12ed3fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c636991f9494a849a95fa86bf09bea7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaf082509c2147d38b58a2977bc3ed3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de4a3555ce4147538db40bd84d34f7ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e939436ee284bda88ff29aa9b1c58df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7603d22cd3744cbb95d6a9643e78897b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d72d54f00a64e9a942fc96433e15b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aaa2f3755930484ca618e6cea36ddfdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf7a36763fb848c7a10f467489621e2c",
              "IPY_MODEL_8e8cd4c41f3944f2865d8c44df689474",
              "IPY_MODEL_1486bff72ec84047878e9a66f3dfc3b9"
            ],
            "layout": "IPY_MODEL_4c05ed439c044cdf8ee01037748ea4f8"
          }
        },
        "cf7a36763fb848c7a10f467489621e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa7be78b53404942ade8940e9d1d23a7",
            "placeholder": "​",
            "style": "IPY_MODEL_ba5a9793958445a388d5cd47846fb8ff",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "8e8cd4c41f3944f2865d8c44df689474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3eacfa36f0946db8961b7826777d799",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_414acde33a1e43ffac2b0631063df250",
            "value": 466247
          }
        },
        "1486bff72ec84047878e9a66f3dfc3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f953bbd4bb645a2b9529a77830c3d1f",
            "placeholder": "​",
            "style": "IPY_MODEL_a32db049f73c4316bd338750d3c621e2",
            "value": " 466k/466k [00:00&lt;00:00, 21.6MB/s]"
          }
        },
        "4c05ed439c044cdf8ee01037748ea4f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa7be78b53404942ade8940e9d1d23a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba5a9793958445a388d5cd47846fb8ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3eacfa36f0946db8961b7826777d799": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "414acde33a1e43ffac2b0631063df250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f953bbd4bb645a2b9529a77830c3d1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32db049f73c4316bd338750d3c621e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8490694e6cd4918adc1ae77c24d3993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89e7d1cef6df4a1b9c975bce577193dc",
              "IPY_MODEL_4d14a2adde014437b269b11767a84d91",
              "IPY_MODEL_f6023ef0d85346e4bf1734910273b08a"
            ],
            "layout": "IPY_MODEL_2fdc277be5b44ef0920e42cee956b1d3"
          }
        },
        "89e7d1cef6df4a1b9c975bce577193dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97089676cd6e4613a440d5b6c8ceb627",
            "placeholder": "​",
            "style": "IPY_MODEL_4d8932804e234222953e3b2b953ed69a",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "4d14a2adde014437b269b11767a84d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c9a0e88ed8a46a1b5553e51b31c7311",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa9cbacae2654b6986c55a6a5f597f34",
            "value": 350
          }
        },
        "f6023ef0d85346e4bf1734910273b08a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bbaa310eab74e52818b959289b45e43",
            "placeholder": "​",
            "style": "IPY_MODEL_3237c79daa054a2fa22d0c59a52352e9",
            "value": " 350/350 [00:00&lt;00:00, 24.4kB/s]"
          }
        },
        "2fdc277be5b44ef0920e42cee956b1d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97089676cd6e4613a440d5b6c8ceb627": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d8932804e234222953e3b2b953ed69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c9a0e88ed8a46a1b5553e51b31c7311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9cbacae2654b6986c55a6a5f597f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8bbaa310eab74e52818b959289b45e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3237c79daa054a2fa22d0c59a52352e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4b6e65bf4764f20a6d910ebf35aa15e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2255c84dc29449d499686039055b2e6d",
              "IPY_MODEL_6367cec122874b51ad3a02a4734f1c4f",
              "IPY_MODEL_49363cb5c6864c4f94c222da105a244d"
            ],
            "layout": "IPY_MODEL_f2c744d8284a4103b308c248d553aaa5"
          }
        },
        "2255c84dc29449d499686039055b2e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f7f745067c54f659b6c680d8b18cc8b",
            "placeholder": "​",
            "style": "IPY_MODEL_9a4b3aa91081424d8d03fd097dd9e7c1",
            "value": "Downloading train_script.py: 100%"
          }
        },
        "6367cec122874b51ad3a02a4734f1c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd341ecb7b8d4ee7bef2c62ba2fa3e58",
            "max": 13156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3647bce524834188bb15a77389c59075",
            "value": 13156
          }
        },
        "49363cb5c6864c4f94c222da105a244d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbd786e6e0504551bc89ea8854223f96",
            "placeholder": "​",
            "style": "IPY_MODEL_abcb7861931f4d93a4d7426cda1ca766",
            "value": " 13.2k/13.2k [00:00&lt;00:00, 1.01MB/s]"
          }
        },
        "f2c744d8284a4103b308c248d553aaa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f7f745067c54f659b6c680d8b18cc8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a4b3aa91081424d8d03fd097dd9e7c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd341ecb7b8d4ee7bef2c62ba2fa3e58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3647bce524834188bb15a77389c59075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbd786e6e0504551bc89ea8854223f96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abcb7861931f4d93a4d7426cda1ca766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f10eab93619a48f5984c490184198ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2576301315c848fab08f162407dc4c95",
              "IPY_MODEL_ee9bf9cba9b741bfbc239a0df03b66c3",
              "IPY_MODEL_62aaa77830754cc09b6f6b5e153be871"
            ],
            "layout": "IPY_MODEL_9f8fb2f27ca041e589d5cf74569c19d4"
          }
        },
        "2576301315c848fab08f162407dc4c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61696de2535b4eee97a33f70d4347632",
            "placeholder": "​",
            "style": "IPY_MODEL_2455ab2efde44672a0fa55966e01c295",
            "value": "Downloading vocab.txt: 100%"
          }
        },
        "ee9bf9cba9b741bfbc239a0df03b66c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d71cccef8164bd1aa2508b7b9c8a1b6",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f66592fe8c0f4ef39676a729a0413b37",
            "value": 231508
          }
        },
        "62aaa77830754cc09b6f6b5e153be871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_570adb3c0189483ba90dd82227fda24b",
            "placeholder": "​",
            "style": "IPY_MODEL_1f4e9bc3f7c14a5497ea2f8d95129a6e",
            "value": " 232k/232k [00:00&lt;00:00, 6.66MB/s]"
          }
        },
        "9f8fb2f27ca041e589d5cf74569c19d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61696de2535b4eee97a33f70d4347632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2455ab2efde44672a0fa55966e01c295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d71cccef8164bd1aa2508b7b9c8a1b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f66592fe8c0f4ef39676a729a0413b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "570adb3c0189483ba90dd82227fda24b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f4e9bc3f7c14a5497ea2f8d95129a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a07eb3a414a47f0b99a70d35e5f8eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc99c46d5ecd4cdb975f230a16ef8f21",
              "IPY_MODEL_58d7672c903c4d6b86b84ca4928baba5",
              "IPY_MODEL_f21680ee10794710b9d77f9f57746154"
            ],
            "layout": "IPY_MODEL_b913a2cb9f384de6ab5658a8c6dc95a5"
          }
        },
        "fc99c46d5ecd4cdb975f230a16ef8f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a97d50a78c954029a85fdc528d87cf6e",
            "placeholder": "​",
            "style": "IPY_MODEL_d7d73dbe12e6403ebb423fc682b0b645",
            "value": "Downloading modules.json: 100%"
          }
        },
        "58d7672c903c4d6b86b84ca4928baba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83af42a085c54334a244f225869202e2",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a271f324be38485d8f74668d24755b77",
            "value": 349
          }
        },
        "f21680ee10794710b9d77f9f57746154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8692b9addba04e4da85f5dcfc0e693fb",
            "placeholder": "​",
            "style": "IPY_MODEL_98154f790e82438ab4168bb065aa93d7",
            "value": " 349/349 [00:00&lt;00:00, 32.7kB/s]"
          }
        },
        "b913a2cb9f384de6ab5658a8c6dc95a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a97d50a78c954029a85fdc528d87cf6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7d73dbe12e6403ebb423fc682b0b645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83af42a085c54334a244f225869202e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a271f324be38485d8f74668d24755b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8692b9addba04e4da85f5dcfc0e693fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98154f790e82438ab4168bb065aa93d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name : Mansi M kanojiya"
      ],
      "metadata": {
        "id": "kXAZCEVPSu5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assesment 1"
      ],
      "metadata": {
        "id": "ks3qpc4cKOOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DugUCmL1DQxA"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text by converting to lowercase and removing punctuation.\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import inflect\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2==3.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwmyZ7qsFVYM",
        "outputId": "d2c61275-fbb6-4796-934c-c2ed51dcfc56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2==3.0.1 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2"
      ],
      "metadata": {
        "id": "KDAIKLjuFZON"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                text += reader.pages[page_num].extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF: {e} for file: {pdf_path}\")\n",
        "    return text\n",
        "\n",
        "# Function to convert text to lowercase (can keep this as is)\n",
        "def text_lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "\n",
        "# List of paths to your PDF files\n",
        "pdf_file_paths = [\"/content/Progress_Report_latest_26.pdf\",\n",
        "                  \"/content/reasearch paper 3.pdf\",\n",
        "                  \"/content/research paper 1.pdf\",\n",
        "                  \"/content/research paper2.pdf\"]\n",
        "\n",
        "# Process each PDF file in the list\n",
        "all_processed_text = \"\"\n",
        "for file_path in pdf_file_paths:\n",
        "    pdf_text = extract_text_from_pdf(file_path)\n",
        "    if pdf_text:  # Only process if text was successfully extracted\n",
        "        processed_text = text_lowercase(pdf_text)\n",
        "        all_processed_text += processed_text + \"\\n\" # Concatenate processed text\n",
        "\n",
        "# Print all processed text\n",
        "if all_processed_text:\n",
        "    print(all_processed_text)\n",
        "else:\n",
        "    print(\"No text extracted from any of the PDFs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2kfpXGBEwaO",
        "outputId": "9572d54c-a002-4d81-da16-fe048f16db27"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.000-7375328') invalid; use 0.0 instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "project progress report: digitizing & \n",
            "structuring physical documents for \n",
            "loan applications (ai chat for banking \n",
            "transactions)  \n",
            "name : mansi m kanojiya  \n",
            "prn: 23070243026  \n",
            "progress to date  \n",
            "workflow for pdf to postgresql ingestion  \n",
            "below is the workflow illustrating pdf ingestion and storage in postgresql:  \n",
            " \n",
            "1. executive summary  \n",
            "to date, i have developed a working prototype that automates the ingestion of scanned \n",
            "bank‐statement pdfs, structures their content in postgresql, and enables interactive q&a \n",
            "via large language models (llms). this pipelin e demonstrates the core objectives of the \n",
            "project synopsis —preserving contextual formatting for llm comprehension, standardizing \n",
            "financial data, and delivering user‐friendly insights —while laying the groundwork for an \n",
            "agentic, autogen‐driven architecture.  \n",
            "2. completed work  \n",
            "• data ingestion & storage  \n",
            "  - implemented pdf_parser.py in banking_chatbot.py.ipynb to extract transaction rows \n",
            "(date, description, debit/credit, balance) from diverse pdf layouts using pypdf2, pandas \n",
            "regex logic, and schema‐driven mappi ngs.  \n",
            "  - designed a normalized postgresql schema (transactions, accounts, metadata) and built \n",
            "db_loader.py to create tables and bulk‐load cleaned data via psycopg2.  \n",
            "• llm‐powered query engine  \n",
            "  - developed query_agent.py with two llm prompt templates: one to convert natural‐\n",
            "language questions into parameterized sql queries, and another to refine raw query results \n",
            "into coherent, context‐rich answers with source citations.  \n",
            "  - integrated openai’s gpt‐4 api, orchestrating the flow: user uploads pdf → ingestion  → \n",
            "sql generation → database execution → answer refinement.  \n",
            "  - validated the system on 100+ sample statements and 50+ question types, achieving > \n",
            "98% field‐extraction accuracy and 95% answer correctness.  \n",
            "• interactive interface  \n",
            "  - created a jupyter‐noteb ook ui (main_interface.py) with file‐upload widgets and chat \n",
            "input, enabling seamless user interaction and immediate feedback.  \n",
            "3. alignment with project synopsis  \n",
            "• preserving structural & contextual attributes: while initial parsing focuses on tabular \n",
            "data , the database metadata table already captures document‐level properties (document \n",
            "id, upload time, page count).  \n",
            "• data standardization: dates and currency formats are normalized during ingestion; \n",
            "transaction descriptions are cleaned and mapped to consiste nt categories.  \n",
            "• generative ai enrichment: the two‐step llm refinement ensures that answers retain \n",
            "semantic depth and cite original data sources, matching the synopsis’s goal of enriched llm \n",
            "understanding.  \n",
            "4. pending work & ongoing efforts  \n",
            "• document pre‐p rocessing & ocr enhancements (in progress)  \n",
            "  - implement noise removal, de‐skewing, and contrast enhancement using opencv and pil \n",
            "to improve ocr fidelity on scanned images.  \n",
            "  - integrate tesseract ocr with a google vision fallback to extract both text and layout \n",
            "metadata (headings, subheadings, font styles, tables, images, diagrams).  \n",
            "• ai‐based data enrichment & validation  \n",
            "  - develop generative‐ai modules for transaction classification (salary credits, loan repayments, rent, etc.) and anomaly detection (fr audulent patterns, missing entries).  \n",
            "  - implement nlp‐based cross‐verification routines to flag and correct inconsistencies in \n",
            "extracted data.  \n",
            "• structured output & system integration  \n",
            "  - extend the pipeline to export final structured data in excel/csv fo rmats and expose \n",
            "restful apis for real‐time integration with loan‐management crms and underwriting \n",
            "systems.  \n",
            "• agentic architecture with autogen  \n",
            "  - define and implement three autogen agents:  \n",
            "    1. ingestagent for automated monitoring and pre‐processing of  new document uploads  \n",
            "    2. structagent for schema validation, anomaly flagging, and metadata enrichment  \n",
            "    3. queryagent for managing user q&a workflows, chaining ocr, sql generation, \n",
            "execution, and answer refinement tools  \n",
            "  - integrate a vector‐databas e (e.g., faiss) for semantic search and build a human‐in‐the‐\n",
            "loop feedback dashboard to iteratively fine‐tune llm prompts and parsing heuristics.  \n",
            "5. upcoming milestones  \n",
            "• complete ocr enhancements and layout‐metadata extraction; begin ai‐based \n",
            "classificati on and anomaly modules.  \n",
            "• implement excel/csv export functionality and api integrations; finalize agent definitions \n",
            "and tool interfaces.  \n",
            "• deploy autogen agents in a test environment; iterate on human‐in‐the‐loop review \n",
            "dashboard; aim for sub‐2  s response latency and ≥  99% extraction accuracy.  \n",
            "6. conclusion  \n",
            "the foundational pipeline meets the core objectives of digitizing, structuring, and enabling \n",
            "llm‐driven q&a over financial documents. the upcoming phases will focus on enriching \n",
            "document metadata, automa ting workflows via autogen agents, and integrating with \n",
            "downstream loan‐processing systems —culminating in a scalable, production‐ready \n",
            "platform that revolutionizes loan application workflows.  \n",
            " \n",
            "thank you.  \n",
            "1\n",
            "trie++: towards end-to-end information\n",
            "extraction from visually rich documents\n",
            "zhanzhan cheng\u0003, peng zhang\u0003, can li\u0003, qiao liang, yunlu xu, pengfei li, shiliang pu, yi niu, and\n",
            "fei wu\n",
            "abstract —recently, automatically extracting information from visually rich documents ( e.g., tickets and resumes) has become a hot\n",
            "and vital research topic due to its widespread commercial value. most existing methods divide this task into two subparts: the text\n",
            "reading part for obtaining the plain text from the original document images and the information extraction part for extracting key\n",
            "contents. these methods mainly focus on improving the second, while neglecting that the two parts are highly correlated. this paper\n",
            "proposes a uniﬁed end-to-end information extraction framework from visually rich documents, where text reading and information\n",
            "extraction can reinforce each other via a well-designed multi-modal context block. speciﬁcally, the text reading part provides\n",
            "multi-modal features like visual, textual and layout features. the multi-modal context block is developed to fuse the generated\n",
            "multi-modal features and even the prior knowledge from the pre-trained language model for better semantic representation. the\n",
            "information extraction part is responsible for generating key contents with the fused context features. the framework can be trained in\n",
            "an end-to-end trainable manner, achieving global optimization. what is more, we deﬁne and group visually rich documents into four\n",
            "categories across two dimensions, the layout and text type. for each document category, we provide or recommend the corresponding\n",
            "benchmarks, experimental settings and strong baselines for remedying the problem that this research area lacks the uniform evaluation\n",
            "standard. extensive experiments on four kinds of benchmarks (from ﬁxed layout to variable layout, from full-structured text to\n",
            "semi-unstructured text) are reported, demonstrating the proposed method’s effectiveness. data, source code and models are available.\n",
            "index terms —end-to-end, information extraction, text reading, multi-modal context, visually rich documents.\n",
            "f\n",
            "1 i ntroduction\n",
            "extracting information from visually rich document ( abbr.\n",
            "vrd) is a traditional yet very important research topic\n",
            "[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]. this is because\n",
            "automatically understanding vrds can greatly facilitate the\n",
            "key information entry, retrieval and compliance check in\n",
            "enormous and various applications, including ﬁle under-\n",
            "standing in court trial, contract checking in the business\n",
            "system, statements analysis in accounting or ﬁnancial, case\n",
            "recognition in medical applications, invoice recognition in\n",
            "reimburses system, resume recognition in recruitment sys-\n",
            "tem, and automatically examining test paper in education\n",
            "applications, etc.\n",
            "in general, a vrd information extraction system can\n",
            "be divided into two separated parts: text reading and key\n",
            "information extraction. text reading module refers to ob-\n",
            "taining text positions as well as their character sequence in\n",
            "document images, which falls into the computer vision areas\n",
            "related to optical character recognition ( abbr. ocr) [11],\n",
            "[12], [13], [14], [15], [16], [17], [18]. information extraction\n",
            "(abbr. ie) module is responsible for mining key contents\n",
            "(e.g., entity, relation) from the captured plain text, related\n",
            "\u000fz. cheng and f. wu are with college of computer science and\n",
            "technology, zhejiang university, hangzhou, 310058, china (e-mail:\n",
            "11821104@zju.edu.cn, wufei@cs.zju.edu.cn). z. cheng is also with\n",
            "hikvision research institute, hangzhou, 310051, china.\n",
            "\u000fp. zhang, c. li, l. qiao, y. xu, p. li, s. pu and y. niu are with\n",
            "hikvision research institute, hangzhou, 310051, china (email: zhang-\n",
            "peng23@hikvision.com, lican9@hikvision.com, qiaoliang6@hikvision.com,\n",
            "xuyunlu@hikvision.com, lipengfei27@hikvision.com, pushil-\n",
            "iang.hri@hikvision.com, niuyi@hikvision.com).\n",
            "\u0003z. cheng, p. zhang and c. li contributed equally to this research.\n",
            "document\n",
            "imagestructured\n",
            "output\n",
            "text\n",
            "reading\n",
            "information\n",
            "extraction\n",
            "multi -modal\n",
            "contextfig. 1. illustration of the proposed end-to-end vrd information extrac-\n",
            "tion framework. it consists of three sub-modules: the text reading part for\n",
            "generating text layout and character strings, and the information extrac-\n",
            "tion module for outputting key contents. the multi-modal context block\n",
            "is responsible for fully assembling visual, textual, layout features, and\n",
            "even language knowledge, and bridges the text reading and information\n",
            "extraction parts in an end-to-end trainable manner. dashed lines denote\n",
            "back-propagation.\n",
            "to natural language processing ( abbr. nlp) techniques like\n",
            "named entity recognition ( abbr. ner) [19], [20], [21] and\n",
            "question-answer [22], [23], [24].\n",
            "early works [4], [5] implement the vrd information\n",
            "extraction frameworks by directly concatenating an ofﬂine\n",
            "ocr engine and the downstream ner-based ie module,\n",
            "which completely discards the visual features and posi-\n",
            "tion/layout1information from images. however, as appear-\n",
            "ing in many applications [1], [4], [8], [9], [25], [26], vrds\n",
            "are usually organized with both semantic text features and\n",
            "ﬂexible visual structure features in a regular way. for better\n",
            "results, researchers should consider the key characteristics\n",
            "of documents into their techniques, such as layout, tabular\n",
            "1. note that, terms of ‘position’ and ‘layout’ are two different but\n",
            "highly relevant concepts. the former refers to the speciﬁc coordinate lo-\n",
            "cations of candidate text regions generated by text reading module. the\n",
            "later means the abstract spatial information ( e.g., position arrangement\n",
            "of text regions) derived from the generated position results via some\n",
            "embedding operations. thus, layout can be treated as the high-level of\n",
            "spatial information in document understanding. in the follow-up, we\n",
            "use term ‘layout’ instead of term ‘position’ as one kind of modality.arxiv:2207.06744v1  [cs.cv]  14 jul 20222\n",
            "structure, or even font size in addition to the plain text. then\n",
            "recent works begin to incorporate these characteristics into\n",
            "the ie module by embedding multi-dimensional informa-\n",
            "tion such as text content and their layouts [2], [3], [27], [28],\n",
            "[29], and even image features [30], [31], [32].\n",
            "unfortunately, all existing methods suffer from two main\n",
            "problems: first, multi-modality features (like visual, textual\n",
            "and even layout features) are essential for vrd information\n",
            "extraction, but the exploitation of the multi-modal features\n",
            "is limited in previous methods. contributions of different\n",
            "kinds of features should be addressed for the ie part. for an-\n",
            "other, text reading and ie modules are highly correlated, but\n",
            "their contribution and relations have rarely been explored.\n",
            "therein, the real bottleneck of the whole framework has not\n",
            "been addressed well.\n",
            "intuitively, it is a way to combine an ocr engine ( e.g.,\n",
            "tesseract [33]) with a general ie module into a end-to-\n",
            "end framework, while it has several practical issues. (1) it\n",
            "is irresponsible to apply the general ocr engines on the\n",
            "speciﬁc scenes directly due to the domain gap problems,\n",
            "leading to the dramatic accuracy drop. (2) a general and big\n",
            "ie model like layoutlm [30], [32] may help achieve better\n",
            "ie performance, which however brings extra computational\n",
            "cost. (3) the non-trainable pipeline strategies will bring\n",
            "extra model maintenance costs and sub-optimal problems.\n",
            "considering the above issues, in this paper, we propose\n",
            "a novel end-to-end information extraction framework from\n",
            "vrds, named as trie++. the workﬂow is as shown in\n",
            "figure 1. instead of focusing on information extraction task\n",
            "only, we bridge text reading and information extraction tasks\n",
            "via a developed multi-modal context block. in this way, two\n",
            "separated tasks can reinforce each other amidst a uniﬁed\n",
            "framework. speciﬁcally, the text reading module produces\n",
            "diversiform features, including layout features, visual fea-\n",
            "tures and textual features. the multi-modal context block\n",
            "fuses multi-modal features with the following steps: (1)\n",
            "layout features, visual features and textual features are\n",
            "ﬁrst fed into the multi-modal embedding module, obtain-\n",
            "ing their embedding representation. (2) considering the\n",
            "effectiveness of the language model like bert [27], a prior\n",
            "language knowledge absorption mechanism is developed\n",
            "to incorporate the robust semantic representations of pre-\n",
            "trained language model. (3) the embedded features are\n",
            "then correlated with the spatial-aware attention to learn the\n",
            "instance-level interactions. it means different text instances\n",
            "may have explicit or implicit interactions, e.g., the ‘total-\n",
            "key’ and ‘total-value’ in receipts are highly correlated.\n",
            "consequently, the multi-modal context block can pro-\n",
            "vide robust features for the information extraction mod-\n",
            "ule, and the supervisions in information extraction also\n",
            "contribute to the optimization of text reading. since all\n",
            "the modules in the network are differentiable, the whole\n",
            "network could be trained in a global optimization way.\n",
            "to the best of our knowledge, this is the ﬁrst end-to-end\n",
            "trainable framework. to implement it, it is also challenging\n",
            "when considering the fusion of multi-modal information,\n",
            "the global optimization or even prior language knowledge\n",
            "absorption into a framework simultaneously, yet working\n",
            "well on different kinds of documents.\n",
            "we also notice that it is difﬁcult to compare existing\n",
            "methods directly due to the different benchmarks used(most of them are private), the non-uniform evaluation\n",
            "protocols, and even various experimental settings. as is\n",
            "known to all, text reading [43] is a rapidly growing research\n",
            "area, attributing to its various applications and its uniform\n",
            "benchmarks and evaluation protocols. we here reckon that\n",
            "these factors may restrict the study of document under-\n",
            "standing. to remedy this problem, we ﬁrst analyze many\n",
            "kinds of documents, and then categorize vrds into four\n",
            "groups along the dimensions of layout and text type .layout\n",
            "refers to the relative position distribution of texts or text\n",
            "blocks, which contains two modes: the ﬁxed mode and\n",
            "the variable mode. the former connotes documents that\n",
            "follow a uniform layout format, such as passport and the\n",
            "national value-added tax invoice, while the latter means\n",
            "that documents may appear in different layouts. referring\n",
            "to [44], [45], we deﬁne text type into two modalities2: the\n",
            "structured and the semi-structured. in detail, the structured\n",
            "type means that document information is organized in a\n",
            "predetermined schema, i.e., the key-value schema of the\n",
            "document is predeﬁned and often tabular in style, which\n",
            "delimits entities to be extracted directly. for example, taxi\n",
            "invoices usually have quite a uniform tabular-like layout\n",
            "and information structure like ‘invoice number’, ‘total’,\n",
            "‘date’ etc. the semi-structured type connotes that document\n",
            "content is usually ungrammatical, but each portion of the\n",
            "content is not necessarily organized in a predetermined for-\n",
            "mat. for example, a resume may include some predeﬁned\n",
            "ﬁelds such as job experience and education information.\n",
            "within the job experience ﬁelds, the document may include\n",
            "free text to describe the person’s job experience. then, the\n",
            "user may desire to search on free text only within the\n",
            "job experience ﬁeld. table 1 summarizes the categories of\n",
            "visually rich documents from the previous research litera-\n",
            "ture. secondly, we recommend or provide the corresponding\n",
            "benchmarks for each kind of documents, and also provide\n",
            "the uniform evaluation protocols, experimental settings and\n",
            "strong baselines, expecting to promote this research area.\n",
            "major contributions are summarized as follows. (1) we\n",
            "propose an end-to-end trainable framework trie++ for ex-\n",
            "tracting information from vrds, which can be trained from\n",
            "scratch, with no need for stage-wise training strategies. (2)\n",
            "we implement the framework by simultaneously learning\n",
            "text reading and information extraction tasks via a well-\n",
            "designed multi-modal context block, and also verify the mu-\n",
            "tual inﬂuence of text reading and information extraction. (3)\n",
            "to make evaluations more comprehensive and convincing,\n",
            "we deﬁne and divide vrds into four categories, in which\n",
            "three kinds of real-life benchmarks are collected with full\n",
            "annotations. for each kind of document, we provide or rec-\n",
            "ommend the corresponding benchmarks, experimental set-\n",
            "tings, and strong baselines. (4) extensive evaluations on four\n",
            "kinds of real-world benchmarks show superior performance\n",
            "compared with the state-of-the-art. those benchmarks cover\n",
            "diverse types of document images, from ﬁxed to variable\n",
            "layouts, from structured to semi-unstructured text types.\n",
            "declaration of major extensions compared to the con-\n",
            "ference version [1]: (1) instead of modelling context with\n",
            "2. another text type, the unstructured, is also deﬁned in [44], which\n",
            "means that document content is grammatically free text without explicit\n",
            "identiﬁers such as books. since such documents usually lack visually\n",
            "rich elements ( e.g., layout), we exclude it from the concept of vrd.3\n",
            "table 1\n",
            "categories of visually rich document scenarios for information extraction. a majority of them have been studied in existing research works.\n",
            "layouttext\n",
            "type structured semi-structured\n",
            "fixedcategory i :\n",
            "value-added tax invoice [29], passport [34], ﬁxed-format taxi invoice [1],\n",
            "national id card [35], train ticket [7], [31], business license [26]category ii :\n",
            "business email [36],\n",
            "national housing contract\n",
            "variablecategory iii :\n",
            "medical invoice [8], [31], paper head [37], bank card [35],\n",
            "free-format invoice [1], [2], [4], [28], [38], [39], [40], business card [34],\n",
            "purchase receipt [3], [7], [25], [29], [31], purchase orders [5], [30]category iv :\n",
            "personal resume [1],\n",
            "ﬁnancial report [36],newspaper [41],\n",
            "free-format sales contract [42]\n",
            "only layout and textual features in [1], we here enhance the\n",
            "multi-modal context block by fusing three kinds of features\n",
            "(i.e., layout, visual and textual features) with a spatial-aware\n",
            "attention mechanism. besides, we expand the application\n",
            "ranges of our method, showing the ability to handle with\n",
            "four kinds of vrds. (2) following the suggestions in the\n",
            "conference reviews that the prior knowledge may be helpful\n",
            "to our method, we also attempt to introduce the pre-trained\n",
            "language model [27] into the framework with a knowledge\n",
            "absorption module for further improving the information\n",
            "extraction performance. (3) we address the problem of per-\n",
            "formance comparison in existing methods, and then deﬁne\n",
            "the four categories of vrds. to promote the document un-\n",
            "derstanding area, we recommend the corresponding bench-\n",
            "marks, experimental settings, and strong baselines for each\n",
            "kind of document. (4) we explore the effects of the proposed\n",
            "framework with more extensive experimental evaluations\n",
            "and comparisons, which demonstrates its advantages.\n",
            "2 r elated works\n",
            "thanks to the rapid expansion of artiﬁcial intelligence tech-\n",
            "niques [46], advanced progress has been made in many\n",
            "isolated applications such as document layout analysis [30],\n",
            "[47], scene text spotting [48], [49], video understanding [50],\n",
            "named entities identiﬁcation [51], question answering [52],\n",
            "or even causal inference [53] etc. however, it is crucial to\n",
            "build multiple knowledge representations for understand-\n",
            "ing the complex and challenging world. vrd information\n",
            "extraction is such a real task greatly helping ofﬁce au-\n",
            "tomation, which relies on integrating multiple techniques,\n",
            "including object detection, sequence learning, information\n",
            "extraction and even the multi-modal knowledge representa-\n",
            "tion. here, we roughly brief techniques as follows.\n",
            "2.1 text reading\n",
            "text reading belongs to the ocr research ﬁeld and has been\n",
            "widely studied for decades. a text reading system usually\n",
            "consists of two parts: text detection and text recognition.\n",
            "intext detection , methods are usually divided into two\n",
            "categories: anchor-based methods and segmentation-based\n",
            "methods. following faster r-cnn [54], anchor-based meth-\n",
            "ods [14], [55], [56], [57], [58], [59], [60], [61] predicted the\n",
            "existence of texts and regress their location offsets at pre-\n",
            "deﬁned grid points of the input image. to localize arbitrary-\n",
            "shaped text, mask rcnn [62]-based methods [63], [64],\n",
            "[65] were developed to capture irregular text and achieve\n",
            "better performance. compared to anchor-based methods,segmentation can easily be used to describe the arbitrary-\n",
            "shaped text. therefore, many segmentation-based meth-\n",
            "ods [66], [67], [68], [69] were developed to learn the pixel-\n",
            "level classiﬁcation tasks to separate text regions apart from\n",
            "the background. in text recognition , the encoder-decoder\n",
            "architecture [70], [71], [72] dominates the research ﬁeld,\n",
            "including two mainstreaming routes: ctc [73]-based [17],\n",
            "[61], [74], [75] and attention-based [71], [72], [76] methods.\n",
            "to achieve the global optimization between detection and\n",
            "recognition, many end-to-end trainable methods [11], [12],\n",
            "[13], [48], [49], [77], [78], [79], [80] were proposed, and\n",
            "achieved better results than the pipeline approaches.\n",
            "global optimization is one of the importance research\n",
            "trend exactly in general object detection and text spotting.\n",
            "hence, it is easy to observe its importance in extracting\n",
            "information from vrds. based on this, we attempt to de-\n",
            "velop an end-to-end trainable framework from text reading\n",
            "to information extraction. unlike text spotting methods\n",
            "achieving global optimization via a roi-like operations [13],\n",
            "[48] only, the proposed framework relies on an all-around\n",
            "module, i.e., the multi-modal context block, to bridge the\n",
            "vision tasks and the nlp task. this module should have\n",
            "the ability of both handling complex multi-modal informa-\n",
            "tion and providing differentiable passages among different\n",
            "modules.\n",
            "2.2 information extraction\n",
            "information extraction is a traditional research topic and\n",
            "has been studied for many years. here, we divide existing\n",
            "methods into two categories as follows.\n",
            "2.2.1 rule-based methods\n",
            "before the advent of learning-based models, rule-based\n",
            "methods [8], [9], [47], [81], [82], [83] dominated this research\n",
            "area. it is intuitive that the key information can be identi-\n",
            "ﬁed by matching a predeﬁned pattern or template in the\n",
            "unstructured text. therefore, expressive pattern matching\n",
            "languages [81], [82] were developed to analyze syntactic\n",
            "sentence, and then output one or multiple target values.\n",
            "to extract information from general documents such as\n",
            "business documents, many solutions [8], [9], [39], [47], [84]\n",
            "were developed by using the pattern matching approaches.\n",
            "in detail, [9], [39], [85] required a predeﬁned document\n",
            "template with relevant key ﬁelds annotated, and then auto-\n",
            "matically generated patterns matching those ﬁelds. [8], [47],\n",
            "[84] all manually conﬁgured patterns based on keywords,\n",
            "parsing rules or positions. the rule-based methods heavily\n",
            "rely on the predeﬁned template, and are limited to the4\n",
            "documents with unseen templates. as a result, it usually\n",
            "requires deep expertise and a large time cost to conduct the\n",
            "templates’ design and maintenance.\n",
            "2.2.2 learning-based methods\n",
            "learning-based methods can automatically extract key in-\n",
            "formation by applying machine learning techniques to a\n",
            "prepared training dataset.\n",
            "traditionally machine learning techniques like logistic\n",
            "regression and svm were widely adopted in document\n",
            "analysis tasks. [86] proposed a general machine learning\n",
            "approach for the hierarchical segmentation and labeling of\n",
            "document layout structures. this approach modeled docu-\n",
            "ment layout as grammar and performed a global search for\n",
            "the optimal parse based on a grammatical cost function. this\n",
            "method utilized machine learning to discriminatively select\n",
            "features and set all parameters in the parsing process.\n",
            "the early methods often ignore the layout information in\n",
            "the document, and then the document understanding task is\n",
            "downgraded to the pure nlp problem. that is, many named\n",
            "entity recognition (ner) based methods [20], [21], [51], [87],\n",
            "[88], [89] can be applied to extract key information from the\n",
            "one-dimensional plain text. inspired by this idea, [4] pro-\n",
            "posed cloudscan, an invoice analysis system, which used\n",
            "recurrent neural networks to extract entities of interest from\n",
            "vrds instead of templates of invoice layout. [5] proposed\n",
            "a token level recurrent neural network for end-to-end table\n",
            "ﬁeld extraction that starts with the sequence of document\n",
            "tokens segmented by an ocr engine and directly tags each\n",
            "token with one of the possible ﬁeld types. however, they\n",
            "discard the layout information during the text serialization,\n",
            "which is crucial for document understanding.\n",
            "observing the rich layout and visual information con-\n",
            "tained in document images, researchers tended to incor-\n",
            "porate more details from vrds. some works [2], [3], [26],\n",
            "[27], [28] took the layout into consideration, and worked\n",
            "on the reconstructed character or word segmentation of\n",
            "the document. concretely, [2] ﬁrst achieved a new type\n",
            "of text representation by encoding each document page\n",
            "as a two-dimensional grid of characters. then they devel-\n",
            "oped a generic document understanding pipeline named\n",
            "chargrid for structured documents by a fully convolutional\n",
            "encoder-decoder network. as an extension of chargrid,\n",
            "[27] proposed bertgrid in combination with a fully con-\n",
            "volutional network on a semantic instance segmentation\n",
            "task for extracting ﬁelds from invoices. to further explore\n",
            "the effective information from both semantic meaning and\n",
            "spatial distribution of texts in documents, [3] proposed a\n",
            "convolutional universal text information extractor by apply-\n",
            "ing convolutional neural networks on gridding texts where\n",
            "texts are embedded as features with semantic connotations.\n",
            "[28] proposed the attend, copy, parse architecture, an end-\n",
            "to-end trainable model bypassing the need for word-level\n",
            "labels. [26] proposed a tag, copy or predict network by\n",
            "ﬁrst modelling the semantic and layout information in 2d\n",
            "ocr results, and then learning the information extraction\n",
            "in a weakly supervised manner. contemporaneous with\n",
            "the above-mentioned methods, there are methods [25], [29],\n",
            "[30], [32], [38], [90], [91] which resort to graph modeling\n",
            "to learn relations between multimodal inputs. [29] intro-\n",
            "duced a graph convolution-based model to combine tex-tual and layout information presented in vrds, in which\n",
            "graph embedding was trained to summarize the context\n",
            "of a text segment in the document, and further combined\n",
            "with text embedding for entity extraction. [38] presented a\n",
            "representation learning approach to extract structured in-\n",
            "formation from templatic documents, which worked in the\n",
            "pipeline of candidate generation, scoring and assignment.\n",
            "[25] modelled document images as dual-modality graphs by\n",
            "encoding both textual and visual features, then generated\n",
            "key information with the proposed spatial dual-modality\n",
            "graph reasoning method (sdmg-r). besides, they also\n",
            "released a new dataset named wildreceipt.\n",
            "recently, some researchers attempted to train the large\n",
            "pre-trained model for better results. [30] proposed the lay-\n",
            "outlm to jointly model interactions between text and layout\n",
            "information across scanned document images. then [32]\n",
            "released layoutlmv2 where new model characteristics and\n",
            "pre-training tasks were leveraged. [90] also proposed a pre-\n",
            "trained model and leveraged cell-level layout information\n",
            "instead of token-level used in [30], [32]. [91] proposed a\n",
            "uniﬁed framework to handle the entity labeling and entity\n",
            "linking sub-tasks, using token and segment level context.\n",
            "[92] proposed lambert by modifying the transformer\n",
            "encoder architecture for obtaining layout features from an\n",
            "ocr system, without the need to re-learn language se-\n",
            "mantics from scratch. [93] proposed tilt which relies on\n",
            "a pretrained encoder-decoder transformer to learns layout\n",
            "information, visual features, and textual semantics. [94]\n",
            "developed selfdoc by exploiting the positional, textual,\n",
            "and visual information of a document and modeling their\n",
            "contextualization. however, these works rely on the big\n",
            "models trained on the large datasets ( e.g., iit-cdip [95]),\n",
            "which suffers from the computational cost problem in real\n",
            "applications. on the other hand, they also ignore the mutual\n",
            "impacts between text reading and information extraction\n",
            "modules. they need to recompute visual features used in ie\n",
            "module, leading to double-computing problems, and cannot\n",
            "exploit supervision from ie on text reading.\n",
            "2.3 end-to-end information extraction from vrds\n",
            "two related concurrent works were presented in [34], [96].\n",
            "[34] proposed an entity-aware attention text extraction net-\n",
            "work to extract entities from vrds. however, it could only\n",
            "process documents of relatively ﬁxed layout and structured\n",
            "text, like train tickets, passports and business cards. [96]\n",
            "localized, recognized and classiﬁed each word in the doc-\n",
            "ument. since it worked in the word granularity, it required\n",
            "much more labeling efforts (layouts, content and category\n",
            "of each word) and had difﬁculties extracting those enti-\n",
            "ties which were embedded in word texts ( e.g., extracting\n",
            "‘51xxxx@xxx.com’ from ‘153-xxx97 j51xxxx@xxx.com’). be-\n",
            "sides, in its entity recognition branch, it still worked on the\n",
            "serialized word features, which were sorted and packed in\n",
            "the left to right and top to bottom order. the two existing\n",
            "works are strictly limited to documents of relatively ﬁxed\n",
            "layout and one type of text (structured or semi-structured).\n",
            "similar to the conference version [1] of our method, [37]\n",
            "recently proposed an end-to-end information extraction\n",
            "framework accompanied by a chinese examination paper\n",
            "head dataset. unlike them, our method acts as a general5\n",
            "information  \n",
            "extraction\n",
            "multimodal context block\n",
            "blstm\n",
            "text reading\n",
            "text\n",
            "detection\n",
            "instance \n",
            "feature\n",
            "text \n",
            "recognition \n",
            "shared\n",
            "conv net\n",
            "x0\n",
            "y0\n",
            "..\n",
            "x0\n",
            "y0\n",
            "..\n",
            "x0\n",
            "y0\n",
            "..position  features\n",
            "x0\n",
            "y0\n",
            "..\n",
            "visual  features\n",
            "m\n",
            "a\n",
            "r\n",
            "t\n",
            "i\n",
            "..\n",
            "a\n",
            "b\n",
            "c\n",
            "1\n",
            "2\n",
            "..\n",
            "2\n",
            "0\n",
            "1\n",
            "8\n",
            ".\n",
            "..textual  features\n",
            "g\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            "..\n",
            "name : \n",
            "e-mail: \n",
            "period : \n",
            "...: \n",
            "global \n",
            "context\n",
            "modelling\n",
            "multimodal\n",
            "embedding\n",
            "local \n",
            "context\n",
            "modelling\n",
            "fig. 2. the overall framework. the network predicts text locations, text\n",
            "contents and key entities in a single forward pass.\n",
            "vrd information extraction framework, and can handle\n",
            "documents of both ﬁxed and variable layouts, structured\n",
            "and semi-structured text types.\n",
            "3 m ethodology\n",
            "this section introduces the proposed framework, which has\n",
            "three parts: text reading, multi-modal context block and\n",
            "information extraction module, as shown in figure 2.\n",
            "3.1 text reading\n",
            "text reading module commonly includes a shared convo-\n",
            "lutional backbone, a text detection branch as well as a\n",
            "text recognition branch. we use resnet-d [97] and feature\n",
            "pyramid network (fpn) [98] as our backbone to extract the\n",
            "shared convolutional features. for an input image x, we\n",
            "denoteias the shared feature maps.\n",
            "text detection . the branch takes ias input and predicts\n",
            "the locations of all candidate text regions, i.e.,\n",
            "b=detector (i) (1)\n",
            "where the detector can be the anchor-based [14], [55], [59],\n",
            "[60] or segmentation-based [66], [67], [68] text detection\n",
            "heads.b= (b1;b2;:::;b m)is a set of mtext bounding\n",
            "boxes, and bi= (xi0;yi0; xi1;yi1)denotes the top-left\n",
            "and bottom-right positions of the i-th text. in mainstream\n",
            "methods, roi-like operations ( e.g., roi-pooling [54] used in\n",
            "[77], roi-align [62] used in [78], roi-rotate used in [48], or\n",
            "even roi-based arbitrary-shaped transformation [11], [12])\n",
            "are applied on the shared convolutional features ito get\n",
            "their text instance features. here, the text instance features\n",
            "are denoted asc= (c1;c2;:::;c m). the detailed network\n",
            "architecture is shown in section 5.1.\n",
            "text recognition . the branch predicts a character se-\n",
            "quence from each text region features ci. firstly, each in-\n",
            "stance feature ciis fed into an encoder ( e.g., cnn and\n",
            "lstm [99]) to extract a higher-level feature sequence h=\n",
            "(h1;h2;:::;h l), wherelis the length of the extracted feature\n",
            "sequence. then, a general sequence decoder ( e.g., attention-\n",
            "based [17], [72]) is adopted to generate the sequence of\n",
            "charactersy= (y1;y2;:::;y t), wheretis the length of\n",
            "label sequence. details are shown in section 5.1.\n",
            "we choose attention-based sequence decoder as the\n",
            "character recognizer. it is a recurrent neural network thatdirectly generates the character sequence yfrom an input\n",
            "feature sequence h. to generate the t-th character, the\n",
            "attention weight \u000btand the corresponding glimpse vec-\n",
            "torgtare computed as gt=pl\n",
            "k=1\u000bt;khkwhere\u000bt;k=\n",
            "exp(et;k)=pl\n",
            "j=1exp(et;j). hereet;j=wttanh(wsst\u00001+\n",
            "whhj+b), andw,ws,whandbare trainable weights. then\n",
            "the hidden state st\u00001is updated via,\n",
            "st=lstm (st\u00001;gt;yt\u00001): (2)\n",
            "finally,stis taken for predicting the current-step character,\n",
            "i.e.,p(yt) =softmax (wost+bo)where both woandboare\n",
            "learnable weights.\n",
            "3.2 multi-modal context block\n",
            "we design a multi-modal context block to consider layout\n",
            "features, visual features and textual features altogether.\n",
            "different modalities of information are complementary to\n",
            "each other, and fully fused for providing robust multi-modal\n",
            "feature representation.\n",
            "3.2.1 multi-modal feature generation\n",
            "document details such as the apparent color, font, layout\n",
            "and other informative features also play an important role\n",
            "in document understanding.\n",
            "a natural way of capturing the layout and visual\n",
            "features of a text is to resort to the convolutional neu-\n",
            "ral network. concretely, the position information of each\n",
            "text instance is obtained from the detection branch, i.e.,\n",
            "b= (b1;b2;:::;b m). for visual feature, different from [30],\n",
            "[32] which extract these features from scratch, we directly\n",
            "reuse text instance features c= (c1;c2;:::;c m)by text\n",
            "reading module as the visual features. thanks to the deep\n",
            "backbone and lateral connections introduced by fpn, each\n",
            "cisummarizes the rich local visual patterns of the i-th text.\n",
            "in sequence decoder, give the i-th text instance, its rep-\n",
            "resented feature of characters before softmax contain rich\n",
            "semantic information. for the attention-based decoder, we\n",
            "can directly use zi= (s1;s2;:::;s t)as its textual features.\n",
            "3.2.2 prior knowledge absorption\n",
            "since pre-trained language model contains general language\n",
            "knowledge like semantic properties, absorbing knowledge\n",
            "from the language model may help improve the perfor-\n",
            "mance of information extraction. compared to the confer-\n",
            "ence paper [1], we here attempt to bring the language model\n",
            "into our framework. however, prior language information\n",
            "has different contributions on different vrds. for example,\n",
            "on resume scenario that require semantics, prior language\n",
            "information contributes more, while on taxi scenario which\n",
            "requires less semantics, prior language information con-\n",
            "tributes less. inspired by the gating operation in lstm\n",
            "[99], we design a gated knowledge absorption mechanism\n",
            "to adjust the prior knowledge ﬂows in our framework, as\n",
            "shown in figure 3.\n",
            "concretely, we ﬁrst transform textual input zinto vo-\n",
            "cabulary space via a linear layer, then obtain the character\n",
            "sequence by conducting argmax operation ( ^), i.e.,\n",
            "z0=^(linear (z)): (3)6\n",
            "δ \n",
            " ＋\n",
            "z\n",
            "σzz\n",
            "o\n",
            "language\n",
            "model\n",
            "z' \n",
            "λ \n",
            "linearz\n",
            "a ɡ' \n",
            "r' \n",
            "fig. 3. the knowledge absorption mechanism. \u001band\u000eseparately mean\n",
            "the sigmoid and tanh operations. \u0002and +refer to the element-wise\n",
            "multiplication and addition, respectively. ^means the argmax operation.\n",
            "thenz0is fed into the language model, outputting its\n",
            "knowledge representation a.\n",
            "in order to dynamically determine the degree of depen-\n",
            "dency of the pre-trained model, we use an on-off gate g0\n",
            "g0=\u001b(wg0a+ug0z+bg0) (4)\n",
            "to balance the ﬂow of the prior knowledge activation r0\n",
            "r0=\u000e(wr0a+ur0z+br0): (5)\n",
            "here, the gate is used for determining whether general\n",
            "knowledge is needed. then the modulated textual feature\n",
            "ois calculated as\n",
            "o=g0\fr0+woz: (6)\n",
            "3.2.3 multi-modal context modelling\n",
            "we ﬁrst embed each modality information into feature se-\n",
            "quences with the same dimension, and fuse them with a\n",
            "normalization layer. inspired by the powerful transformer\n",
            "[32], [87], [100], [101], the self-attention mechanism is used\n",
            "to build deep relations among different modalities, whose\n",
            "input consists of queries qand keyskof dimension dk,\n",
            "and valuesvof dimension dv.\n",
            "multi-modal feature embedding given a document\n",
            "withmtext instance, we can capture the inputs of po-\n",
            "sitionb= (b1;b2;:::;b m), the inputs of visual feature\n",
            "c= (c1;c2;:::;c m)and the inputs of modulated textual\n",
            "featureo= (o1;o2;:::;o m).\n",
            "since position information provides layout information\n",
            "of documents, we introduce a position embedding layer to\n",
            "preserve layout information, for the i-th text instance in a\n",
            "document,\n",
            "pei=jbijx\n",
            "j=1embedding (bij); (7)\n",
            "whereembedding is a learnable embedding layer, bi=\n",
            "(xi0;yi0;xi1;yi1)andpei2rde.\n",
            "forcivisual feature, we embed it using a convolutional\n",
            "neural network layer with the same shape of pei,\n",
            "bci=convnet c(ci): (8)\n",
            "foroitextual feature, a convnet of multiple kernels\n",
            "similar to [102] is used to aggregate semantic character\n",
            "features inoiand outputs bzi2rde,\n",
            "bzi=convnet z(oi): (9)then, thei-th text’s embedding is fused of bci,bziandpei,\n",
            "followed by the layernorm normalization, deﬁned as\n",
            "embi=layernorm (bci+bzi+pei): (10)\n",
            "afterwards, we pack all the texts’ embedding vector to-\n",
            "gether, i.e., emb = (emb 1;emb 2;:::;emb m), which serves\n",
            "as thek,qandvin the scaled dot-product attention.\n",
            "spatial-aware self-attention to better learn pair-wise\n",
            "interactions between text instances, we use the spatial-\n",
            "aware self-attention mechanism instead of the original\n",
            "self-attention, and the correlative context features ec=\n",
            "(ec1;ec2;:::;fcm)are obtained by,\n",
            "ec=attention (q;k;v )\n",
            "=softmax (qkt\n",
            "pdinfo+pe\u0001b)v(11)\n",
            "wheredinfo is the dimension of text embedding, andpdinfo is the scaling factor. pe\u0001brefers to the spatial-aware\n",
            "information, and is calculated by embedding features of\n",
            "position relations \u0001bamong different text instances in b,\n",
            "i.e.,pe\u0001b=embedding (\u0001b). here, \u0001bis deﬁned as\n",
            "\u0001b=2\n",
            "6640b1\u0000b2\u0001\u0001\u0001b1\u0000bm\n",
            "b2\u0000b1 0\u0001\u0001\u0001b2\u0000bm\n",
            "\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001\n",
            "bm\u0000b1bm\u0000b2\u0001\u0001\u0001 03\n",
            "775: (12)\n",
            "to further improve the representation capacity of the at-\n",
            "tended feature, multi-head attention is introduced. each\n",
            "head corresponds to an independent scaled dot-product\n",
            "attention function and the text context features ecis given\n",
            "by:\n",
            "ec=multihead (q;k;v )\n",
            "= [head 1;head 2;:::;head n]winfo(13)\n",
            "head j=attention (qwq\n",
            "j;kwk\n",
            "j;vwv\n",
            "j) (14)\n",
            "wherewq\n",
            "j,wk\n",
            "jandwv\n",
            "j2r(dinfo\u0002dn)are the learned\n",
            "projection matrix for the j-th head,nis the number of\n",
            "heads, and winfo2r(dinfo\u0002dinfo). to prevent the multi-\n",
            "head attention model from becoming too large, we usually\n",
            "havedn=dinfo\n",
            "n.\n",
            "context fusion both the multi-modal context and tex-\n",
            "tual features matter in entity extraction. the multi-modal\n",
            "context features ( ec) provide necessary information to tell\n",
            "entities apart while the textual features oenable entity\n",
            "extraction in the character granularity, as they contain se-\n",
            "mantic features for each character in the text. thus, we need\n",
            "to fuse them further. that is, for the i-the text instance, we\n",
            "pack its multi-modal context vector eciand its modulated\n",
            "textual features oitogether along the channel dimension,\n",
            "i.e.,(ui1;ui2;:::;u it)whereuij= [oi;j;ci].\n",
            "3.3 information extraction\n",
            "then, a bidirectional-lstm is applied to further model the\n",
            "long dependencies within the characters,\n",
            "h0\n",
            "i= (h0\n",
            "i;1;h0\n",
            "i;2;:::;h0\n",
            "i;t) =bilstm (ui); (15)7\n",
            "which is followed by a fully connected network and a\n",
            "conditional random ﬁeld ( abbr, crf) layer, projecting the\n",
            "output to the dimension of iob3[103] label space.\n",
            "pinfo\n",
            "i;j=crf (linear (h0\n",
            "i;j)) (16)\n",
            "note that, unlike the spatial-aware self-attention learn-\n",
            "ing pair-wise interactions between text instances ( e.g,the\n",
            "‘phone number’ and ‘name’), bilstm is for depicting the\n",
            "relations among the characters in each instance ( e.g, the\n",
            "numbers in ‘phone number’ or characters in ‘name’).\n",
            "3.4 optimization\n",
            "the proposed network can be trained in an end-to-end\n",
            "manner and the losses are generated from three parts,\n",
            "l=ldet+\u0015recoglrecog +\u0015infolinfo (17)\n",
            "where hyper-parameters \u0015recog and\u0015info control the trade-\n",
            "off between losses.\n",
            "ldetis the loss of text detection branch, which can\n",
            "be formulated as different forms according to the selected\n",
            "detection heads. taking faster-rcnn [54] as the detection\n",
            "head, the detection part consists of a classiﬁcation loss and\n",
            "a regression loss.\n",
            "for sequence recognition part, the attention-based recog-\n",
            "nition loss is\n",
            "lrecog =\u00001\n",
            "tmx\n",
            "i=1tx\n",
            "t=1logp (^yi;tjh); (18)\n",
            "where ^yi;tis the ground-truth label of t-th character in i-th\n",
            "text from recognition branch.\n",
            "the information extraction loss is the crfloss, as used\n",
            "in [20], [37]. speciﬁcally, for the i-th text instance, the input\n",
            "sequence of lstm is ui= (ui1;ui2;:::;u it)2rt\u0002din,\n",
            "while the output is h0\n",
            "i2rt\u0002\n",
            ", where \n",
            "denotes the num-\n",
            "ber of entities. h0(we omit the subscript ifor convenience)\n",
            "denotes the scores of emissions matrix, and h0\n",
            "t;jrepresents\n",
            "the score of the j\u0000th entity of the t\u0000th character in ui. for\n",
            "a sequence of predictions y= (y1;y2;:::;y t), its scores can\n",
            "be deﬁned as follows\n",
            "s(ui;y) =tx\n",
            "j=0ayj;yj+1+tx\n",
            "j=1h0\n",
            "j;yj; (19)\n",
            "whereais a matrix of transition scores such that ai;j\n",
            "represents the score of a transition from the tag ito tagj.y0\n",
            "andytare the start and end tags of a sentence, that we add\n",
            "to the set of possible tags. ais therefore a square matrix of\n",
            "size\n",
            "+2 . a softmax over all possible tag sequences yields a\n",
            "probability for the sequence y, i.e.,p(yjui) =es(ui;y)\n",
            "p\n",
            "ey2y(ui)es(ui;ey)\n",
            "where y(ui)is all possible entity sequences for ui.\n",
            "during training, we minimize the negative log-\n",
            "likelihood estimation of the correct entity sequence as:\n",
            "linfo=\u0000log(p(yjui)) =\u0000s(ui;y) +log(x\n",
            "ey2y(ui)es(ui;ey)):\n",
            "(20)\n",
            "3. here, ‘i’ stands for ‘inside’, signifying that the word is inside a\n",
            "named entity. ‘o’ stands for ‘outside’, referring to the word is just\n",
            "a regular word outside of a named entity. ‘b’ stands for ‘beginning’,\n",
            "signifying beginning of a named entity.while decoding, we predict the output sequence that ob-\n",
            "tains the maximum score given by y\u0003= arg max\n",
            "ey2y(ui)p(eyjui).\n",
            "note that since text reading and information extrac-\n",
            "tion modules are bridged with the multi-modal context\n",
            "block, they can reinforce each other. speciﬁcally, the multi-\n",
            "modality features of text reading are fully fused and es-\n",
            "sential for information extraction. at the same time, the\n",
            "semantic feedback of information extraction also contributes\n",
            "to the optimization of the shared convolutions and text\n",
            "reading module.\n",
            "4 b enchmarks\n",
            "as addressed in section 1, most existing works verify their\n",
            "methods on private datasets due to their privacy policies. it\n",
            "leads to difﬁculties for fair comparisons between different\n",
            "approaches. though existing datasets like sroie [104] have\n",
            "been released, they mainly fall into category iii, i.e., doc-\n",
            "uments with variable layout and structured text type. the\n",
            "remaining three kinds of application scenarios (category i,\n",
            "ii and iv) have not been studied well because of the limited\n",
            "real-life datasets.\n",
            "4.1 dataset inventory\n",
            "to boost the research of vrd understanding, we here extend\n",
            "the benchmarks of vrd, especially on category i, ii and iv .\n",
            "table 2 shows the detailed statistics of these benchmarks.\n",
            "\u000fcategory i refers to document images with uniform\n",
            "layout and structured text type, which is very com-\n",
            "mon in everyday life. contrastively, its research\n",
            "datasets are very limited due to various privacy poli-\n",
            "cies. here, we ﬁnd only two available benchmarks,\n",
            "i.e., train ticket and passport dataset released by [34],\n",
            "which are generated with a synthetic data engine and\n",
            "provide only entity-level annotations. to remedy this\n",
            "issue, we release a new real-world dataset containing\n",
            "5000 taxi invoice images. except for providing the\n",
            "text position and character string information for\n",
            "ocr tasks (text detection and recognition), entity-\n",
            "level labels including 9 entities (invoice code, invoice\n",
            "number, date, get-on time, get-off time, price, dis-\n",
            "tance, wait time, total) are also provided. besides,\n",
            "this dataset is very challenging, as many images are\n",
            "in low-quality (such as blur and occlusion).\n",
            "\u000fcategory ii refers to those documents with ﬁxed\n",
            "layout and semi-structured text type, like business\n",
            "email or national housing contract. ner datasets\n",
            "like cluener2020 [107] are only collected for nlp\n",
            "tasks, and they provide only semantic content while\n",
            "ignoring the important layout information. as ad-\n",
            "dressed in section 1, the joint study of ocr and ie is\n",
            "essential. unfortunately, we have not found available\n",
            "datasets that contains both ocr and ie annotations.\n",
            "we also ascribe the issue to various privacy policies.\n",
            "we here collect a new business email dataset from\n",
            "rvl-cdip [36], which has 1645 email images with\n",
            "35346 text instances and 15 entities (to, from, cc,\n",
            "subject, bcc, text, attachment, date, to-key, from-\n",
            "key, cc-key, subject-key, bcc-key, attachment-key,\n",
            "date-key).8\n",
            "table 2\n",
            "statistics of popular vrd benchmarks. pos, text andentity mean the word position, character string and entity-level annotations, respectively.\n",
            "term ‘syn’ and ‘real’ separately mean the dataset is generated by a synthetic data engine and manual collection. the new collected datasets are\n",
            "available at https://davar-lab.github.io/dataset/vrd.html .\n",
            "category dataset #training #validation #testing #entity #instance annotation type source\n",
            "i train ticket [34] 271.44k 30.16k 400 5 - [ entity ] syn\n",
            "passport [34] 88.2k 9.8k 2k 5 - [ entity ] syn\n",
            "taxi invoice 4000 - 1000 9 136,240 [ pos, text, entity ] real\n",
            "ii business email 1146 - 499 16 35,346 [ pos, text, entity ] real\n",
            "iii sroie [104] 626 - 347 4 52,451 [ pos, text, entity ] real\n",
            "business card [34] 178.2k 19.8k 2k 9 - [ entity ] syn\n",
            "funsd [105] 149 - 50 4 31,485 [ pos, text, entity ] real\n",
            "cord3[106] 800 100 100 30 - [ pos, text, entity ] real\n",
            "ephoie [37] 1183 - 311 12 15,771 [ pos, text, entity ] real\n",
            "wildreceipt [25] 1267 - 472 26 69,000 [ pos, text, entity ] real\n",
            "iv kleister-nda [42] 254 83 203 4 - [ pos, text, entity ] real\n",
            "resume 1045 - 482 11 82,800 [ pos, text, entity ] real\n",
            "\u000fcategory iii means documents which are with vari-\n",
            "able layout and structured text type like purchase\n",
            "receipt dataset sroie [104]. these datasets are usu-\n",
            "ally composed of small documents ( e.g., purchase\n",
            "receipts, business cards, etc.), and entities are orga-\n",
            "nized in a predetermined schema. we note that most\n",
            "previous literature focus on this category. we here\n",
            "list ﬁve available datasets. sroie is a scanned receipt\n",
            "dataset widely evaluated in many methods, which is\n",
            "fully annotated and provides text position, character\n",
            "string and key-value labels. business card is a synthe-\n",
            "sized dataset released by [34], and has only key-value\n",
            "pair annotations without ocr annotations. funsd\n",
            "[105] is a dataset aiming at extracting and structuring\n",
            "the textual content from noisy scanned forms. it\n",
            "has only 199 forms with four kinds of entities, i.e.,\n",
            "question, answer, header and other. cord2[106]\n",
            "is a consolidated receipt dataset, in which images\n",
            "are with text position, character string and multi-\n",
            "level semantic labels. ephoie [37] is a chinese ex-\n",
            "amination paper head dataset, in which each image\n",
            "is cropped from the full examination paper. this\n",
            "dataset contains handwritten information, and is also\n",
            "fully annotated. wildreceipt [25] is a large receipt\n",
            "dataset collected from document images of unseen\n",
            "templates in the wild. it contains 25 key information\n",
            "categories, a total of about 69000 text boxes.\n",
            "\u000fcategory iv means documents that have variable\n",
            "layout and semi-structured text type. different from\n",
            "those datasets in category iii, kleister-nda [42]\n",
            "aims to understand long documents (i.e., non-\n",
            "disclosure agreements document), but it provides\n",
            "only 540 documents with four general entity classes.\n",
            "to enrich benchmarks in this category, we release a\n",
            "large-scale resume dataset, which has 1527 images\n",
            "with ten kinds of entities(name, time, school, de-\n",
            "gree, specialty, phone number, e-mail, birth, title, se-\n",
            "curity code). since resumes are personally designed\n",
            "and customized, it is a classic document dataset with\n",
            "variable layouts and semi-structured text.4.2 challenges in different kinds of documents\n",
            "it will be the most straightforward task to extract entities\n",
            "from documents in category i, which attributes to its com-\n",
            "plete ﬁxed layout and structured text type. for this kind\n",
            "of documents, challenges are mainly from the text reading\n",
            "part, such as the distorted interference. the standard object\n",
            "detection methods like faster-rcnn [54] also can be further\n",
            "developed to handle this task. in category ii, the layout is\n",
            "ﬁxed, but the text is semi-structured. thus, in addition to\n",
            "modelling layout information, we also should pay attention\n",
            "to mining textual information. then some nlp techniques\n",
            "like the pre-trained language model can be exploited. as to\n",
            "the text reading part, long text recognition is also challeng-\n",
            "ing. documents in category iii face the problem of complex\n",
            "layout. thus the layout modelling methods [29], [31] like\n",
            "graph neural networks are widely developed for coping\n",
            "with this issue. the documents in category iv are in the face\n",
            "of both complex layout and nlp problems, which becomes\n",
            "the most challenging task.\n",
            "we hope that the combed benchmarks can promote the\n",
            "increasingly important research topic. we want validate our\n",
            "model as well as some state-of-the-arts on the representative\n",
            "datasets, and also provide a group of solid baselines (see\n",
            "section 5.4) for this research community.\n",
            "5 e xperiments\n",
            "in subsection 5.1, we ﬁrst introduce the implementation\n",
            "details of network and training skills. in subsection 5.2,\n",
            "we perform ablation study to verify the effectiveness of\n",
            "the proposed method on four kinds of vrd datasets, i.e.,\n",
            "taxi invoice, business email, wildreceipt and resume.\n",
            "in subsection 5.3, we compare our method with existing\n",
            "approaches on several recent datasets like funsd, sroie,\n",
            "ephoie and wildreceipt, demonstrating the advantages of\n",
            "the proposed method. then, we provide a group of strong\n",
            "baselines on four kinds of vrds in subsection 5.4. finally,\n",
            "we discuss the challenges of the different categories of\n",
            "3. in the original paper, cord claim that 11,000 receipt images were\n",
            "collected with 54 sub-classes. up to now, only 1000 images have been\n",
            "released in total.9\n",
            "documents. codes and models are available at https://davar-\n",
            "lab.github.io/publication/trie++.html .\n",
            "5.1 implementation details\n",
            "5.1.1 data selecting\n",
            "to facilitate end-to-end document understanding ( text read-\n",
            "ingand information extraction ), datasets should have posi-\n",
            "tion, text and entity annotations. hence, we only consider\n",
            "those datasets which satisfy the above requirement. on the\n",
            "ablation and strong baseline experiments, we select one\n",
            "classic dataset from each category, which has the largest\n",
            "number of samples. they are taxi invoice dataset from\n",
            "category i, business email dataset from category ii, wil-\n",
            "dreceipt dataset from category iii and resume dataset from\n",
            "category iv . when compared with the state-of-the-arts,\n",
            "since they mainly report their results on popular sroie,\n",
            "funsd and ephoie benchmarks, we also include these\n",
            "benchmarks in section 5.3.\n",
            "5.1.2 network details\n",
            "the backbone of our model is resnet-d [97], followed by\n",
            "the fpn [98] to further enhance features. the text detection\n",
            "branch in text reading module adopts the faster r-cnn\n",
            "[54] network and outputs the predicted bounding boxes of\n",
            "possible texts for later sequential recognition. for each text\n",
            "region, its features are extracted from the shared convolu-\n",
            "tional features by roialign [62]. the shapes are represented\n",
            "as32\u0002256for taxi invoice and wildreceipt, and 32\u0002512\n",
            "for business email and resume. then, features are further\n",
            "decoded by lstm-based attention [72], where the number\n",
            "of hidden units is set to 256.\n",
            "in the multimodal context block , bert [87] is used as\n",
            "the pre-trained language model. then, convolutions of four\n",
            "kernel size [3;5;7;9]followed by max pooling are used\n",
            "to extract ﬁnal textual features. the dimension of linear\n",
            "layer in equation 3 is set as \n",
            "\u0002256, where \n",
            "denotes the\n",
            "vocabulary space.\n",
            "in the information extraction module , the number of hidden\n",
            "units of bilstm used in entity extraction is set to 128.\n",
            "hyper-parameters \u0015recog and\u0015info in equation 17 are all\n",
            "empirically set to 1 in our experiments.\n",
            "5.1.3 training details\n",
            "our model and its counterparts are implemented under the\n",
            "pytorch framework [108]. for our model, the adamw [109]\n",
            "optimization is used. we set the learning rate to 1e-4 at the\n",
            "beginning and decreased it to a tenth at 50, 70 and 80 epochs.\n",
            "the batch size is set to 2 per gpu. for the counterparts,\n",
            "we separately train text reading and information extraction\n",
            "tasks until they are fully converged. all the experiments are\n",
            "carried out on a workstation with 8 nvidia a100 gpus.\n",
            "5.1.4 evaluation protocols\n",
            "we also note that different evaluation protocols are adopted\n",
            "in previous works. for example in the evaluation of infor-\n",
            "mation extraction part, both eaten [34] and pick [31] used\n",
            "the deﬁned mean entity accuracy (mea) and mean entity\n",
            "f1-score (mef) as metrics. cutie [3] adopted the average\n",
            "precision (ap) as the metric, and chargrid [2] developed\n",
            "new evaluation metric like word error rate for evaluation.while the majority of methods [1], [30], [42] used the f1-\n",
            "score as the evaluation metric. as a result, the non-uniform\n",
            "evaluation protocols bring extra difﬁculties on comparisons.\n",
            "therefore, we attempt to describe a group of uniform evalu-\n",
            "ation protocols for vrd understanding by carefully analyz-\n",
            "ing previous methods, including the evaluation protocols of\n",
            "text reading and information extraction parts.\n",
            "text reading falls into the ocr community, and it has\n",
            "uniform evaluation standards by referring to mainstream\n",
            "text detection [14], [48], [65] and text recognition [70], [71],\n",
            "[72] methods. precision (abbr. pre d) and recall (abbr. rec d)\n",
            "are used to measure performance of text localization, and\n",
            "f-measure (abbr. fd-m) is the harmonic average of precision\n",
            "and recall . to evaluate text recognition, the accuracy (abbr.\n",
            "acc) used in [70], [71], [72] is treat as its measurement\n",
            "metric. when evaluating the performance of end-to-end text\n",
            "detection and recognition, the end-to-end level evaluating\n",
            "metrics like precision (denoted by pre r), recall (denoted\n",
            "by rec r) and f-measure (denoted by f r-m) following [110]\n",
            "without lexicon is used, in which all detection results are\n",
            "considered with an iou >0.5.\n",
            "for information extraction, we survey the evaluation\n",
            "metrics from recent research works [1], [29], [30], [32], [37],\n",
            "[42], [105], and ﬁnd that the precision, recall and f1-score of\n",
            "entity extraction are widely used. hereby, we recommend\n",
            "theentity precision (abbr. epre), entity recall (abbr. erec) and\n",
            "entity f1-score (ef1) as the evaluation metrics for this task.\n",
            "5.2 ablation study\n",
            "in this section, we perform the ablation study on taxi\n",
            "invoice, business email, wildreceipt and resume datasets\n",
            "to verify the effects of different components in the proposed\n",
            "framework.\n",
            "5.2.1 effects of multi-modality features\n",
            "to examine the contributions of visual, layout and textual\n",
            "features to information extraction, we perform the following\n",
            "ablation study on four kinds of datasets, and the results\n",
            "are shown in table 3. textual feature means that entities are\n",
            "extracted using features from the text reading module only.\n",
            "since the layout information is completely lost, this method\n",
            "presents the worst performance. introducing either the vi-\n",
            "sual features orlayout features brings signiﬁcant performance\n",
            "gains. further fusion of the above multi-modality features\n",
            "gives the best performance, which veriﬁes the effects. we\n",
            "also show examples in figure. 4 to verify their effects. by\n",
            "using the textual feature only, the model misses the ‘store-\n",
            "name’ and has confusion between ‘total’ and ‘product-\n",
            "price’ entities. combined with the layout feature , the model\n",
            "can recognize ‘product-price’ correctly. when combined\n",
            "with the visual feature , the model can recognize store-name,\n",
            "because the visual feature contains obvious visual clues such\n",
            "as the large font size. it shows the best result by integrating\n",
            "all modality features.\n",
            "5.2.2 effects of different components\n",
            "here we attempt to analyze the effects of the spatial-aware\n",
            "self-attention mechanism, pre-trained language model, the\n",
            "gating mechanism and bi-lstm in ie on four kinds of\n",
            "datasets, as shown in table 4.10\n",
            "textual textual + layout textual + visual textual + layout + visual groundtruth\n",
            "fig. 4. illustration of modality contributions. different colors denote different entities, such as store-name, date, time, product-item, product-\n",
            "quantity, product-price, total. best viewed in color.\n",
            "table 3\n",
            "accuracy results (ef1) with multi-modal features on information\n",
            "extraction.\n",
            "textual featurep p p p\n",
            "layout featurep p\n",
            "visual featurep p\n",
            "taxi invoice 90.34 98.45 98.71 98.73\n",
            "business email 74.51 82.88 86.02 87.33\n",
            "wildreceipt 72.9 87.75 83.62 89.62\n",
            "resume 76.73 82.26 82.62 83.16\n",
            "table 4\n",
            "accuracy results (ef1) with different components.\n",
            "original self-attentionp\n",
            "spatial-aware self-attentionp p p\n",
            "language model (bert)p\n",
            "language model (llm v2)p\n",
            "taxi invoice 98.54 98.56 98.73 98.52\n",
            "business email 85.25 85.73 87.33 86.14\n",
            "wildreceipt 85.11 88.84 89.62 90.10\n",
            "resume 80.43 80.48 83.16 82.57\n",
            "evaluation of spatial-aware self-attention (short by\n",
            "sasa). sasa is devoted to learn layout features to improve\n",
            "the ﬁnal performance. from table 4, we see that sasa can\n",
            "boost performance, especially on the wildreceipt. this is\n",
            "because, compared to the original self-attention using enti-\n",
            "ties’ absolute positions only, the spatial-aware self-attention\n",
            "also makes use of relative position offsets between entities,\n",
            "and learns their pairwise relations. visual examples are\n",
            "shown in figure. 5. we see that ‘product-item’ and ‘product-\n",
            "price’ always appear in pairs. spatial-aware self-attention\n",
            "can capture such pairwise relations and then improve model\n",
            "performances. its attention map is visualized in figure. 6,\n",
            "which demonstrates that the spatial-aware self-attention\n",
            "indeed learns the pairwise relations between entities ( e.g.,\n",
            "pair of ‘total-key’ and ‘total-value’, and pair of ‘product-\n",
            "item’ and ‘product-price’).\n",
            "we also note that improvements by sasa is not always\n",
            "signiﬁcant on all benchmarks like taxi invoice and resume.\n",
            "this is because they are dominated with similar layouts\n",
            "(e.g., see the layout distribution of taxi invoice in figure. 7).\n",
            "original self -attention\n",
            " spatial -aware self -attention groundtruthfig. 5. visual examples of original self-attention and spatial-aware self-\n",
            "attention. different colors denote different entities, such as store-name,\n",
            "date, time, product-item, product-quantity, product-price, total. best\n",
            "viewed in color.\n",
            "fig. 6. visualization of spatial-aware self-attention. total-key (total)\n",
            "and total-value ( $49:70), product-item (b8-20top) and product-price\n",
            "(6:40) always appear together, and their pairwise relations can be\n",
            "learned. best viewed in color and zoom in to observe other pairwise\n",
            "relations.\n",
            "thus, effectiveness of sasa on diversity of layouts has not\n",
            "been measured well. while for wildreceipt with complex\n",
            "layouts, the effects are well demonstrated.\n",
            "to further demonstrate effects of sasa, we supplement\n",
            "experiments on the complex scenes constructed by ran-\n",
            "domly zooming out the foreground over the background11\n",
            "taxi invoice business email\n",
            "wildreceipt resume\n",
            "fig. 7. layout distribution on four kinds of benchmarks, where positions\n",
            "of entity of ‘code’, ‘to’, ‘date’ and ‘school’ are projected on their canvas.\n",
            "table 5\n",
            "accuracy results (ef1) by sasa on more complex scenes.\n",
            "strategy taxi invoice business email wildreceipt resume\n",
            "wo.sasa 98.45 79.93 82.28 75.96\n",
            "w.sasa 98.55 83.23 87.63 78.55\n",
            "from 1.2x to 3x. table 5 shows the results, which denotes\n",
            "that sasa can signiﬁcantly improve the ﬁnal performance\n",
            "without any side effects.\n",
            "evaluation of pre-trained language models. when\n",
            "introducing the prior knowledge from bert [87], the perfor-\n",
            "mance of information extraction is signiﬁcantly improved\n",
            "on the scenarios that require semantics like wildreceipt,\n",
            "business email and resume. as shown in figure 8, in the\n",
            "resume case, introducing the pre-trained language model\n",
            "helps recognize ‘school’ and ‘specialty’ entities, which are\n",
            "hard to be extracted solely using textual features.\n",
            "we also attempt to apply layoutlm v2 as the prior\n",
            "model, showing effective improvements on most of scenes.\n",
            "however, it is not always better than results with bert on\n",
            "the four kinds of documents, as shown in table 4. we\n",
            "think that bert conveys the pure semantic features while\n",
            "layoutlm contains both vision and semantic features. the\n",
            "layoutlm has much overlap to the proposed multi-modal\n",
            "context block, i.e., information redundancy, and bears the\n",
            "problem of double-computing visual features. hence, we\n",
            "use bert as the pre-trained model by default.\n",
            "evaluation of gating mechanism in equation 6. we also\n",
            "compare the gating fusion with two classical feature fusion\n",
            "strategies, i.e., the element-wise summation and concatena-\n",
            "tion. the two classical fusion strategies have their strong\n",
            "points on corresponding datasets. for example, concatena-\n",
            "tion operation has better result on business email, while\n",
            "falls behind summation on other datasets. beneﬁting from\n",
            "the on-off role, gating mechanism can help achieve the best\n",
            "performance. table 6 shows the results.\n",
            "evaluation of bi-lstm in ie. as addressed in sec-\n",
            "tion 3.3, lstm can help learn relations between characters,\n",
            "which is useful on the token-level scenes like business email\n",
            "and resume. table 7 shows that lstm can signiﬁcantlytable 6\n",
            "effects (ef1) of gating mechanism.\n",
            "strategy concatenation summation gating\n",
            "taxi invoice 98.41 98.62 98.73\n",
            "business email 87.06 86.19 87.33\n",
            "wildreceipt 87.95 88.47 89.62\n",
            "resume 81.55 82.26 83.16\n",
            "without pretrained language model\n",
            " with pretrained language model\n",
            "fig. 8. illustration of pre-trained language model’s effects. best viewed\n",
            "in color and zoom in.\n",
            "boost the ﬁnal performance.\n",
            "table 7\n",
            "effects (ef1) of lstm in ie part.\n",
            "strategy email resume\n",
            "wo.lstm 85.29 78.06\n",
            "w.lstm 87.40 83.24\n",
            "5.2.3 effects of different number of layers and heads\n",
            "table 8 analyzes the effects of different numbers of layers\n",
            "and heads in the spatial-aware self-attention. taxi invoices\n",
            "is relatively simple and has a ﬁxed layout. thus the model\n",
            "with 1 or 2 layers and the small number of heads achieves\n",
            "promising results. for scenes with complex layout struc-\n",
            "tures like resumes and wildreceipt, deeper layers and\n",
            "heads can help improve the accuracy results. in practice,\n",
            "one can adjust these settings according to the complexity of\n",
            "a task.\n",
            "5.2.4 effects of the end-to-end training\n",
            "to verify the effects of the end-to-end framework on text\n",
            "reading and information extraction, we perform the follow-\n",
            "ing experiments on four kinds of vrd datasets. we ﬁrst\n",
            "deﬁne two strong baselines for comparison. (1) base1 . the\n",
            "detection, recognition and information extraction modules\n",
            "are separately trained, and then pipelined as an inference\n",
            "model. (2) base2 . the detection and recognition tasks are\n",
            "jointly optimized, and then pipelined with the separately\n",
            "trained information extraction task. while joint training of\n",
            "the three modules is denoted as our end-to-end (short by\n",
            "e2e) framework. notice that all multi-modal features (see\n",
            "section 5.2.1) are integrated. the layer and head numbers in\n",
            "self-attention are set as (2, 2, 4, 2) and (32, 32, 16, 32) for four\n",
            "different tasks (taxi invoice, business email, wildreceipt,\n",
            "resume in order), respectively. results are as shown in12\n",
            "table 8\n",
            "accuracy results (ef1) with different number of layers and heads in\n",
            "spatial-aware self-attention.\n",
            "datasets layersheads\n",
            "2 4 8 16 32\n",
            "taxi\n",
            "invoice1 98.27 98.57 98.45 98.62 98.00\n",
            "2 98.31 98.39 98.58 98.52 98.74\n",
            "3 98.51 98.54 98.48 98.51 98.56\n",
            "4 98.44 98.58 98.41 98.70 98.59\n",
            "business\n",
            "email1 86.05 86.41 85.74 86.94 86.43\n",
            "2 85.95 87.51 86.78 87.33 87.59\n",
            "3 86.52 87.86 87.24 87.15 88.01\n",
            "4 86.48 87.45 87.82 87.88 87.64\n",
            "wildreceipt1 78.17 87.8 88.73 88.18 88.67\n",
            "2 86.26 88.11 88.21 89.16 89.11\n",
            "3 77.1 88.62 88.95 89.48 89.69\n",
            "4 85.48 89.00 88.63 89.66 90.15\n",
            "resume1 82.18 82.52 81.99 81.83 82.49\n",
            "2 82.7 82.56 82.97 82.83 83.57\n",
            "3 82.86 82.09 83.05 82.78 82.96\n",
            "4 82.75 83.12 82.43 82.98 83.46\n",
            "table 9\n",
            "comparisons of pipelines and end-to-end training framework.\n",
            "dataset methoddetection\n",
            "(fd-m)spotting\n",
            "(fr-m)ie\n",
            "(ef1)\n",
            "taxi invoicebase1 95.72 91.15 88.29\n",
            "base2 95.21 91.05 88.28\n",
            "e2e 94.85 91.07 88.46\n",
            "business emailbase1 97.12 55.88 45.24\n",
            "base2 97.10 56.18 45.47\n",
            "e2e 97.22 56.83 45.71\n",
            "wildreceiptbase1 90.31 73.52 69.37\n",
            "base2 90.55 74.98 71.15\n",
            "e2e 90.73 76.50 73.12\n",
            "resumebase1 96.71 55.15 58.53\n",
            "base2 96.86 55.56 58.31\n",
            "e2e 96.88 55.66 58.77\n",
            "table 9, in which the end-to-end training framework can\n",
            "beneﬁt both text reading and information extraction tasks.\n",
            "concretely, on text reading ,e2esurpasses base1 and base2\n",
            "on most scenes (wildreceipt, business email and resume),\n",
            "while slightly falls behind base1 by 0.08 on taxi invoice\n",
            "dataset. in fact, the ﬁnal results on information extraction\n",
            "are important. we see that e2eobtains the best performance\n",
            "on all datasets, achieving the largest ef1 performance gain\n",
            "(3.75%) on wildreceipt. we also note that, on taxi invoice\n",
            "and resume cases, improvements by e2eare not signiﬁcant.\n",
            "there are two main reasons. (1) the ie part has achieved\n",
            "the saturated performance w.r.t the cared text in text spot-\n",
            "ting. and the slight performance gain on ie is hard-won.\n",
            "therefore, we calculate the relative ef1of ie w.r.t the cared\n",
            "text in text spotting (i.e.,ef1of ie\n",
            "cared fr-mof spotting) to obtain the\n",
            "relative ie performance, which gets rid of the impacts of text\n",
            "spotting. table 10 shows the results. for taxi invoice, the\n",
            "performance is approximate to the upper-bound (100%), the\n",
            "improvement is limited. and for wildreceipt and resume,\n",
            "the improvements are signiﬁcant. as to business email,\n",
            "though base2 has better result (table 10) on the evaluation\n",
            "of relative performance, e2eboosts the ﬁnal results in table\n",
            "9. (2) the text spotting capability limits the ﬁnal results\n",
            "directly. it means that text spotting maybe the real perfor-\n",
            "mance bottleneck for end-to-end vrd information extrac-table 10\n",
            "the relative ef1for ie w.r.tfr-m(cared text instances) of text spotting.\n",
            "methodtaxi\n",
            "invoicebusiness\n",
            "emailwildreceipt resume\n",
            "base2 99.41 95.76 94.68 95.12\n",
            "e2e 99.45 95.01 96.11 97.41\n",
            "tion. however, taxi invoice contains many low-quality ( e.g.,\n",
            "blur or occlusion text) text, while resume has amount of\n",
            "long text instance (once one character is recognized wrongly,\n",
            "the ﬁnal results is wrong.). techniques for handling the\n",
            "above obstinate problems are needed.\n",
            "in sum, compared to the strong baselines, the end-to-\n",
            "end trainable strategy is actually working, and then boost\n",
            "the ﬁnal performance.\n",
            "5.3 comparisons with the state-of-the-arts\n",
            "recent methods [30], [32], [90], [91] focused on the informa-\n",
            "tion extraction task by adding great number of extra training\n",
            "samples like iit-cdip dataset [95] and docbank [115], and\n",
            "then have impressive results on the downstream datasets.\n",
            "following the typical routine, we also compare our method\n",
            "with them on several popular benchmarks. note that, all\n",
            "results are reported by using the ofﬁcial ocr annotations.\n",
            "evaluation on funsd the dataset is a noisy scanned\n",
            "from the dataset with 200 images. the results are shown\n",
            "in funsd column of table 11. to be fair, we ﬁrst compare\n",
            "our method with those without introducing extra data. our\n",
            "method signiﬁcantly outperforms them with a large margin\n",
            "(83.53 v.s.81.33 of matchvie [114]). when comparing with\n",
            "models trained with extra data, our method is still competi-\n",
            "tive. it only falls behind the llmv2 [32] and slm [90].\n",
            "evaluation on sroie the dataset has 963 scanned\n",
            "receipt images, which is evaluated on four entities in many\n",
            "works. most of the results are impressive, as shown in\n",
            "sroie column of table 11. this is because methods tend\n",
            "to achieve the performance upper bound of this dataset.\n",
            "for example, structext [91] (with extra data) has achieved\n",
            "96.88 of ef1, which only has slight advantage over 96.57\n",
            "of matchvie [114]. our method shows promising results\n",
            "on this benchmark, with 96.80 ef1in the token granularity\n",
            "(same to most works [1], [26], [30], [31], [32], [37], [114]) and\n",
            "98.37 in the segment granularity (same to structext [91]).\n",
            "evaluation on ephoie the dataset is a chinese exami-\n",
            "nation paper head dataset. our method obviously surpasses\n",
            "previous methods without extra data, achieving the best re-\n",
            "sult (98.85%), and is competitive to the large model llmv2\n",
            "[32]. similar to sroie, its performance upper bound is\n",
            "limited. that is, only 1.15% of improvement space is left.\n",
            "evaluation on wildreceipt this receipt dataset [25]\n",
            "is more challenging than sroie, which is collected from\n",
            "document images with unseen templates in the wild. most\n",
            "of the methods like gat [112] have rapid performance\n",
            "degradation compared to results in sroie and ephoie.\n",
            "while our method still has the best result (90.15% of ef1)\n",
            "compared to existing methods trained without extra data,\n",
            "which veriﬁes the advantages of the proposed method.13\n",
            "table 11\n",
            "comparison ( ef1) with the state-of-the-arts on funsd, sroie, ephoie, and wildreceipt.\u0003refers to the results are reported by our\n",
            "re-implemented model. on sroie dataset, italic denotes the segment-level accuracy while others are token-level accuracy.\n",
            "methodsfunsd sroie ephoie wildreceipt speed\n",
            "erec epre ef 1erec epre ef 1erec epre ef 1erec epre ef 1 avg fps\n",
            "llm [30] 82.19 75.96 78.95 95.24 95.24 95.24 - - - - - - -\n",
            "llmv2 [32] 85.19 83.24 84.20 96.61 96.61 96.61 99.51\u000399.06\u000399.28\u000391.78\u000392.45\u000392.05\u00033.93\n",
            "slm [90] 86.81 83.52 85.14 - - - - - - - - - -\n",
            "structext [91] 80.97 85.68 83.0998.81\n",
            "98.5292.77\n",
            "95.8495.62\n",
            "96.88- - 97.95 - - - -\n",
            "lambert [92] - - - - - 98.17 - - - - - - -\n",
            "tilt [93] - - - - - 98.10 - - - - - - -\n",
            "selfdoc [94] - - 83.36 - - - - - - - - - -\n",
            "spade [111] - - 70.50 - - - - - - - - - -\n",
            "lstm-crf [20] - - 62.13 - - 90.85 - - 89.10 82.60\u000383.90\u000383.20\u00035.43\n",
            "chargrid [2] 39.98\u000373.45\u000350.50\u0003- - 80.9 77.82\u000373.41\u000375.23\u000375.64\u000375.22\u000375.39\u000312.42\n",
            "gat [112] 70.81\u000371.03\u000370.73\u000383.14\u000391.73\u000387.23\u000397.36\u000396.48\u000396.90\u000384.57\u000386.37\u000385.43\u000315.73\n",
            "vrd [29] - - 72.43 - - 95.10 - - 92.55 84.57 86.37 85.70 -\n",
            "graphie [113] - - 72.12 - - 94.46 - - 90.26 - - - -\n",
            "vies [37] - - - - - 96.12 - - 95.23 - - - -\n",
            "pick [31] - - - 95.46 96.79 96.12 - - - - - - -\n",
            "matchvie [114] - - 81.33 - - 96.57 - - 96.87 - - - -\n",
            "textlattice [26] - - - - - 96.54 - - 98.06 - - - -\n",
            "sdmg-r [25] - - - - - 87.10 - - - - - 88.70 -\n",
            "trie [1] w. gt - - 78.86 - - 96.18 - - 93.21 84.69 87.58 85.99 14.52\n",
            "trie++ w. gt 82.94 84.37 83.5395.89\n",
            "98.4097.72\n",
            "98.3596.80\n",
            "98.3798.96 98.75 98.85 89.59 90.84 90.15 13.88\n",
            "inference speed . we test the inference speed (the av-\n",
            "erage frames per second, avg fps) of the re-implemented\n",
            "models for evaluating their efﬁciency. our model surpass\n",
            "llmv2 [32] largely, and has competitive efﬁciency with\n",
            "chargrid [2] and gat [112].\n",
            "in sum, compared to methods trained without extra data,\n",
            "the proposed method achieves the new state-of-the-arts on\n",
            "the pure information extraction task, and also shows strong\n",
            "generalization ability across various vrds. even compared\n",
            "with the large models, our method is also competitive.\n",
            "5.4 strong baselines on four categories of vrd\n",
            "for the pure information extraction task, their results (as\n",
            "shown in table 11) are calculated based on the ground\n",
            "truth of detection and recognition. however, the inﬂuence\n",
            "of ocr should not be neglected in reality. considering\n",
            "the real applications, i.e., end-to-end extracting information\n",
            "from vrd, one way is to divide the task as two pipelined\n",
            "steps: (1) obtaining text spotting results with a public ocr\n",
            "engines, (2) and then performing the information extraction.\n",
            "we here provide several groups of solid baselines on four\n",
            "kinds of vrds.\n",
            "5.4.1 comparisons among different experimental settings\n",
            "we ﬁrst build the basic baselines by combining the public\n",
            "ocr engines (including ppocr [116] and tesseract [33])\n",
            "and the pre-trained information extraction model (denoted\n",
            "as ie trie) used in section 5.3. furthermore, we use the\n",
            "text reading (denoted as tr) part in base2 (see table 9)\n",
            "as the ocr engine, and take the results as the input of\n",
            "information extraction counterparts, i.e., gat [112], lstm-\n",
            "crf [20], [21], chargrid [2], bert-crf [87] and layoutlm\n",
            "[32]. concretely, lstm-crf [21] is a classic 1d information\n",
            "extraction paradigm. its input embedding and hidden units\n",
            "in the following bilstm are all set to 128, followed by\n",
            "a crf layer. chargrid [2] is a 2d-cnn information ex-\n",
            "traction framework. the input character embedding is setto128 and the rest of network is identical to the paper.\n",
            "as for graph-based information extraction framework, we\n",
            "adopt gat [112], which is similar to [29]. bert-based and\n",
            "layoutlm-based model are treated as the large model for\n",
            "information extraction. as a result, ﬁve kinds of strong base-\n",
            "lines are constructed. they are ‘tr+gat’, ‘tr+lstm-crf’,\n",
            "‘tr+chargrid’, ‘tr+bert-crf’ and ‘tr+llm v2’. results\n",
            "are shown in table 12.\n",
            "comparison with public ocr engines . as expected,\n",
            "it is irresponsible to apply open ocr engines on the these\n",
            "speciﬁc scenes directly, showing poor results on text spot-\n",
            "ting and information extraction.\n",
            "comparison with classical methods . we compare our\n",
            "method with several classic methods (including ‘tr+gat’,\n",
            "‘tr+lstm-crf’ and ‘tr+chargrid’), and our method ob-\n",
            "viously outperforms them on all scenes.\n",
            "comparison with large models . since large models\n",
            "(e.g., bert or layoutlm) can boost the ie performance at-\n",
            "tributing to its big model of capacity as well as amount of\n",
            "samples, we here also construct such experimental settings\n",
            "for comparison. we see that our method is superior to\n",
            "the bert-based method, and achieves competitive results\n",
            "compared to layoutlm v2-based method, i.e., only falling\n",
            "behind it on the wildreceipt and business email scenes.\n",
            "5.4.2 comparison of inference speed\n",
            "we evaluate the running time of our model and its coun-\n",
            "terparts in frames per second ( abbr. fps). results are as\n",
            "shown in the last column of table 12. thanks to feature\n",
            "sharing between text reading and information extraction mod-\n",
            "ules, our end-to-end framework runs faster than its pipeline\n",
            "counterparts, especially compared to the large models ( e.g.,\n",
            "layoutlm v2). because of the enhanced multi-modal con-\n",
            "text block, the proposed method is slightly slower than its\n",
            "conference version (trie [1]) a more prominent trend is\n",
            "that the algorithm runs faster in scenarios where the length\n",
            "of texts is short in a document ( e.g., taxi invoice and wil-14\n",
            "table 12\n",
            "strong baselines for detection, end-to-end text spotting and information extraction on four kinds of datasets, i.e., taxi invoice, business email,\n",
            "wildreceipt and resume. their entity-level results are shown at https://davar-lab.github.io/publication/trie++.html. ‘tr’ means the text reading\n",
            "module in base2 (see table 9) is used for end-to-end evaluation.\n",
            "dataset methodstext detection text spotting end-to-end ie speed\n",
            "rec dpre dfd-mrec rpre rfr-merecepreef1fps\n",
            "taxi\n",
            "invoiceppocr+ie trie++ 76.73 59.13 66.79 50.23 38.72 43.73 49.66 52.85 50.87 4.78\n",
            "tesseract+ie trie++ 21.96 45.20 29.56 2.91 5.99 3.92 6.98 18.24 9.68 1.53\n",
            "tr+chargrid\n",
            "99.18 91.55 95.21 94.85 87.55 91.0592.11 84.6 88.14 2.83\n",
            "tr+gat 91.58 84.35 87.78 3.86\n",
            "tr+lstm-crf 90.73 83.2 86.74 3.62\n",
            "tr+bert-crf 88.24 85.25 86.74 3.25\n",
            "tr+llm v2 88.91 85.42 87.14 2.86\n",
            "tr+ie trie++ 99.18 91.55 95.21 94.85 87.55 91.05 92.15 84.8 88.28 3.69\n",
            "trie 99.18 91.45 95.16 94.78 87.39 90.94 91.95 84.74 88.16 5.10\n",
            "trie++ 99.22 90.85 94.85 95.27 87.23 91.07 92.5 84.85 88.46 4.26\n",
            "business\n",
            "emailppocr+ie trie++ 89.79 82.92 86.22 11.30 10.43 10.85 8.45 8.74 8.45 2.55\n",
            "tesseract+ie trie++ 68.96 81.07 74.53 25.25 29.68 27.28 23.90 27.39 25.39 1.73\n",
            "tr+chargrid\n",
            "96.4 97.81 97.10 55.77 56.59 56.1832.18 33.27 32.64 2.30\n",
            "tr+gat 42.34 44.22 43.09 2.30\n",
            "tr+lstm-crf 43.27 45.43 44.2 1.30\n",
            "tr+bert-crf 42.62 47.84 44.76 1.80\n",
            "tr+llm v2 45.22 48.49 46.68 1.50\n",
            "tr+ie trie++ 96.4 97.81 97.10 55.77 56.59 56.18 44.92 46.32 45.47 2.22\n",
            "trie 96.66 97.72 97.18 56.02 56.64 56.33 40.43 44.96 42.01 2.65\n",
            "trie++ 96.62 97.83 97.22 56.48 57.18 56.83 45.7 45.85 45.71 2.30\n",
            "wildreceiptppocr+ie trie++ 70.32 77.42 73.70 33.74 37.14 35.36 33.91 40.14 35.73 3.75\n",
            "tesseract+ie trie++ 41.11 58.25 48.20 12.20 17.29 14.31 11.14 21.99 14.59 1.26\n",
            "tr+chargrid\n",
            "90.65 90.45 90.55 75.06 74.89 74.9861.27 60.92 61.04 3.97\n",
            "tr+gat 68.16 69.21 68.66 4.25\n",
            "tr+lstm-crf 65.85 67.95 66.84 2.81\n",
            "tr+bert-crf 69.21 72.24 70.71 3.26\n",
            "tr+llm v2 72.49 74.07 73.26 2.31\n",
            "tr++ie trie++ 90.65 90.45 90.55 75.06 74.89 74.98 71.05 71.32 71.15 4.10\n",
            "trie 90.42 90.79 90.61 75.29 75.60 75.44 70.24 66.81 68.19 6.21\n",
            "trie++ 90.23 91.22 90.73 76.09 76.92 76.50 72.33 74.04 73.12 6.21\n",
            "resumeppocr+ie trie++ 92.18 85.67 88.81 22.49 20.90 21.66 26.65 26.41 26.51 1.27\n",
            "tesseract+ie trie++ 60.35 80.17 68.86 23.46 31.16 26.76 27.48 28.56 27.99 1.20\n",
            "tr+chargrid\n",
            "96.03 97.71 96.86 55.09 56.05 55.5641.46 38.18 39.62 2.10\n",
            "tr+gat 55.96 53.42 54.65 2.20\n",
            "tr+lstm-crf 55.63 54.02 54.64 0.71\n",
            "tr+bert-crf 56.96 57.67 57.31 1.43\n",
            "tr+llm v2 57.33 57.70 57.49 0.90\n",
            "tr+ie trie++ 96.03 97.71 96.86 55.09 56.05 55.56 59.08 57.62 58.33 2.09\n",
            "trie 95.99 97.82 96.89 55.16 56.21 55.68 55.94 56.67 56.29 2.39\n",
            "trie++ 95.96 97.82 96.88 55.13 56.20 55.66 59.43 58.16 58.77 2.31\n",
            "dreceipt), while on resume/business email datasets with\n",
            "long texts, the fps drops slightly.\n",
            "5.4.3 evaluations among different modules\n",
            "in the detection part, all methods achieve the satisfactory\n",
            "performance of fd-m(larger than 90%), while the perfor-\n",
            "mance on wildreceipt is the lowest. this is because the\n",
            "receipt images in wildreceipt are captured in the wild,\n",
            "and they are of non-front views, even with folds. when\n",
            "considering the end-to-end text spotting task, results on\n",
            "business and resume are poor due to the problems of char-\n",
            "acter distortion and long text. this problem will be a new\n",
            "research direction for ocr. for the end-to-end information\n",
            "extraction, results on business email are the worst, and the\n",
            "second-worst is resume. it reveals that there is plenty of\n",
            "work to do concerning end-to-end information extraction.\n",
            "from the perspective of systems, we surprisingly dis-\n",
            "cover that the text recognition may be the top bottleneck for\n",
            "end-to-end understanding vrd on category ii, iii and iv .\n",
            "the information extraction is another bottleneck due to the\n",
            "complex layouts and long character sentence (referring totable 12, 3 and 4). luckily, the end-to-end training strategy\n",
            "can enhance both the text reading and the ﬁnal information\n",
            "extraction task. in future, more attention should be paid to\n",
            "the effects of text reading w.r.t information extraction.\n",
            "6 d iscussion\n",
            "6.1 effects of large pre-trained model\n",
            "the large pre-trained model (short by lpm) becomes a\n",
            "hot research topic in many research communities, while\n",
            "discussions on its industrial application are rarely reported.\n",
            "from our point of view, lpm can help achieve bet-\n",
            "ter baseline performance, and has good generalization on\n",
            "various scenarios. it is suited to platform applications like\n",
            "ai open platform, providing basic trials for various ap-\n",
            "plications. however, it is not ‘green’ to training or main-\n",
            "taining lpm, where ‘green’ means lightweight and electric\n",
            "economic. lpm also suffers from the computational cost\n",
            "problem, which is impractical when applied in terminal\n",
            "devices ( e.g., receipt scanner used in logistics industry).\n",
            "perhaps it is a practical problem on how transfer beneﬁcial15\n",
            "knowledge from a large model to a small model. in fact,\n",
            "when applied model on the speciﬁc scene, ﬁnetuning on the\n",
            "corresponding data is essential for better results. in this way,\n",
            "performance gain from lpm may be limited. unlike them,\n",
            "our method belongs to another researching routine.\n",
            "6.2 necessity of end-to-end model\n",
            "we here point out that the public ocr engines like pad-\n",
            "dleocr or tesseract are irresponsible to apply on the\n",
            "vertical commerce scenes directly, as shown in table 12.\n",
            "therefore, it is inevitable to train/ﬁnetune ocr models\n",
            "on the corresponding scenarios. end-to-end framework can\n",
            "help achieve the global optimization, and also reduce model\n",
            "maintenance cost.\n",
            "6.3 annotation cost problem\n",
            "annotation cost is a realistic problem. unfortunately, it is\n",
            "hard to be avoided in real commerce applications ( e.g.,\n",
            "ﬁnancial auditing). once users (party a) pay money for\n",
            "applications, they tend to have high accuracy requirements\n",
            "but with few samples. for deep model, better application\n",
            "performance means more data or annotations. developers\n",
            "(party b) have to annotate labels as detailed as possible on\n",
            "the given datasets for better results. then some researchers\n",
            "attempt to pre-train large model with big data ( e.g.,collected\n",
            "from internet or other public ways) to relieve the problem.\n",
            "however, once when one applies model on speciﬁc scenes,\n",
            "developers have to ﬁnetune models on the given dataset\n",
            "for better performance. in fact, previous methods ( e.g., lay-\n",
            "outlm) also relied on the ﬁne-grained annotations to obtain\n",
            "the state-of-the-arts. thus, more annotations will be better.\n",
            "6.4 limitations\n",
            "first, our method currently requires the annotations of\n",
            "position, character string and entity labels of texts in a\n",
            "document, and the labeling process is cost-expensive. we\n",
            "will resort to semi/weakly-supervised learning algorithms\n",
            "to alleviate the problem in the future. another limitation is\n",
            "that the multi-modal context block captures context in the\n",
            "instance granularity, which can be much more ﬁne-grained\n",
            "if introduced token/ character granularity context. much\n",
            "more ﬁne-grained context is beneﬁcial to extracting entities\n",
            "across text instances.\n",
            "7 c onclusion\n",
            "in this paper, we present an end-to-end trainable network\n",
            "integrating text reading and information extraction for doc-\n",
            "ument understanding. these two tasks can mutually rein-\n",
            "force each other via a multi-modal context block, i.e., the\n",
            "multi-modal features, like visual, layout and textual fea-\n",
            "tures, can boost the performances of information extraction,\n",
            "while the loss of information extraction can also supervise\n",
            "the optimization of text reading. on various benchmarks,\n",
            "from structured to unstructured text type and ﬁxed to vari-\n",
            "able layout, the proposed method signiﬁcantly outperforms\n",
            "previous methods. to promote the vrd understanding\n",
            "research, we provide four kinds of benchmarks along the\n",
            "dimensions of layout and text type, and also contribute four\n",
            "groups of strong baselines for the future study.references\n",
            "[1] p . zhang, y. xu, z. cheng, s. pu, j. lu, l. qiao, y. niu, and f. wu,\n",
            "“trie: end-to-end text reading and information extraction for\n",
            "document understanding,” in acm mm , 2020, pp. 1413–1422.\n",
            "[2] a. r. katti, c. reisswig, c. guder, s. brarda, s. bickel, j. h ¨ohne,\n",
            "and j. b. faddoul, “chargrid: towards understanding 2d docu-\n",
            "ments,” in emnlp , 2018, pp. 4459–4469.\n",
            "[3] x. zhao, z. wu, and x. wang, “cutie: learning to understand\n",
            "documents with convolutional universal text information ex-\n",
            "tractor,” arxiv preprint arxiv:1903.12363 , 2019.\n",
            "[4] r. b. palm, o. winther, and f. laws, “cloudscan - a\n",
            "conﬁguration-free invoice analysis system using recurrent\n",
            "neural networks,” in icdar , 2017, pp. 406–413.\n",
            "[5] c. sage, a. aussem, h. elghazel, v . eglin, and j. espinas, “re-\n",
            "current neural network approach for table field extraction in\n",
            "business documents,” in icdar , 2019, pp. 1308–1313.\n",
            "[6] e. aslan, t. karakaya, e. unver, and y. akg ¨ul, “a part based\n",
            "modeling approach for invoice parsing,” in international confer-\n",
            "ence on computer vision theory and applications , vol. 4, 2016, pp.\n",
            "390–397.\n",
            "[7] b. janssen, e. saund, e. a. bier, p . wall, and m. sprague, “re-\n",
            "ceipts2go: the big world of small documents,” in proceedings of\n",
            "the 2012 acm symposium on document engineering , 2012, pp. 121–\n",
            "124.\n",
            "[8] a. dengel and b. klein, “smartfix: a requirements-driven\n",
            "system for document analysis and understanding,” in das , ser.\n",
            "lecture notes in computer science, vol. 2423, 2002, pp. 433–444.\n",
            "[9] d. schuster, k. muthmann, d. esser, a. schill, m. berger, c. wei-\n",
            "dling, k. aliyev, and a. hofmeier, “intellix - end-user trained\n",
            "information extraction for document archiving,” in icdar ,\n",
            "2013, pp. 101–105.\n",
            "[10] a. simon, j.-c. pret, and a. p . johnson, “a fast algorithm for\n",
            "bottom-up document layout analysis,” ieee tpami , vol. 19,\n",
            "pp. 273–277, 1997.\n",
            "[11] h. wang, p . lu, h. zhang, m. yang, x. bai, y. xu, m. he, y. wang,\n",
            "and w. liu, “all you need is boundary: toward arbitrary-\n",
            "shaped text spotting,” in aaai , vol. 34, no. 07, 2020, pp. 12 160–\n",
            "12 167.\n",
            "[12] l. qiao, s. tang, z. cheng, y. xu, y. niu, s. pu, and f. wu,\n",
            "“text perceptron: towards end-to-end arbitrary-shaped text\n",
            "spotting,” in aaai , vol. 34, no. 07, 2020, pp. 11 899–11 907.\n",
            "[13] w. feng, w. he, f. yin, x. zhang, and c. liu, “textdragon: an\n",
            "end-to-end framework for arbitrary shaped text spotting,” in\n",
            "iccv , 2019, pp. 9075–9084.\n",
            "[14] m. liao, b. shi, x. bai, x. wang, and w. liu, “textboxes: a fast\n",
            "text detector with a single deep neural network,” in aaai ,\n",
            "2017, pp. 4161–4167.\n",
            "[15] m. jaderberg, k. simonyan, a. vedaldi, and a. zisserman, “read-\n",
            "ing text in the wild with convolutional neural networks,” ijcv ,\n",
            "vol. 116, no. 1, pp. 1–20, 2016.\n",
            "[16] t. wang, d. j. wu, a. coates, and a. y. ng, “end-to-end text\n",
            "recognition with convolutional neural networks,” in icpr , 2012,\n",
            "pp. 3304–3308.\n",
            "[17] b. shi, x. bai, and c. yao, “an end-to-end trainable neural net-\n",
            "work for image-based sequence recognition and its application\n",
            "to scene text recognition,” ieee tpami , vol. 39, no. 11, pp. 2298–\n",
            "2304, 2017.\n",
            "[18] p . lyu, m. liao, c. yao, w. wu, and x. bai, “mask textspotter:\n",
            "an end-to-end trainable neural network for spotting text with\n",
            "arbitrary shapes,” in eccv , ser. lecture notes in computer\n",
            "science, vol. 11218, 2018, pp. 71–88.\n",
            "[19] k. shaalan, “a survey of arabic named entity recognition and\n",
            "classiﬁcation,” comput. linguistics , vol. 40, no. 2, pp. 469–510,\n",
            "2014.\n",
            "[20] g. lample, m. ballesteros, s. subramanian, k. kawakami, and\n",
            "c. dyer, “neural architectures for named entity recognition,”\n",
            "innaacl-hlt , 2016, pp. 260–270.\n",
            "[21] x. ma and e. h. hovy, “end-to-end sequence labeling via bi-\n",
            "directional lstm-cnns-crf,” in acl , 2016.\n",
            "[22] z. yang, x. he, j. gao, l. deng, and a. j. smola, “stacked\n",
            "attention networks for image question answering,” in cvpr ,\n",
            "2016, pp. 21–29.\n",
            "[23] p . anderson, x. he, c. buehler, d. teney, m. johnson, s. gould,\n",
            "and l. zhang, “bottom-up and top-down attention for image\n",
            "captioning and visual question answering,” in cvpr , 2018, pp.\n",
            "6077–6086.16\n",
            "[24] a. fukui, d. h. park, d. yang, a. rohrbach, t. darrell, and\n",
            "m. rohrbach, “multimodal compact bilinear pooling for visual\n",
            "question answering and visual grounding,” in emnlp , 2016,\n",
            "pp. 457–468.\n",
            "[25] h. sun, z. kuang, x. yue, c. lin, and w. zhang, “spatial dual-\n",
            "modality graph reasoning for key information extraction,”\n",
            "arxiv preprint arxiv:2103.14470 , 2021.\n",
            "[26] j. wang, t. wang, g. tang, l. jin, w. ma, k. ding, and y. huang,\n",
            "“tag, copy or predict: a uniﬁed weakly-supervised learning\n",
            "framework for visual information extraction using sequences,”\n",
            "arxiv preprint arxiv:2106.10681 , 2021.\n",
            "[27] t. i. denk and c. reisswig, “bertgrid: contextualized embed-\n",
            "ding for 2d document representation and understanding,” in\n",
            "workshop on document intelligence at neurips 2019 , 2019.\n",
            "[28] r. b. palm, f. laws, and o. winther, “attend, copy, parse end-\n",
            "to-end information extraction from documents,” in icdar , 2019,\n",
            "pp. 329–336.\n",
            "[29] x. liu, f. gao, q. zhang, and h. zhao, “graph convolution\n",
            "for multimodal information extraction from visually rich docu-\n",
            "ments,” in naacl-hlt , 2019, pp. 32–39.\n",
            "[30] y. xu, m. li, l. cui, s. huang, f. wei, and m. zhou, in layoutlm:\n",
            "pre-training of text and layout for document image understanding ,\n",
            "2020, pp. 1192–1200.\n",
            "[31] w. yu, n. lu, x. qi, p . gong, and r. xiao, “pick: processing key\n",
            "information extraction from documents using improved graph\n",
            "learning-convolutional networks,” icpr , 2020.\n",
            "[32] y. xu, y. xu, t. lv, l. cui, f. wei, g. wang, y. lu, d. flor ˆencio,\n",
            "c. zhang, w. che, m. zhang, and l. zhou, “layoutlmv2: multi-\n",
            "modal pre-training for visually-rich document understanding,”\n",
            "arxiv , vol. abs/2012.14740, 2020.\n",
            "[33] r. smith, “an overview of the tesseract ocr engine,” in icdar ,\n",
            "2007, pp. 629–633.\n",
            "[34] h. guo, x. qin, j. liu, j. han, j. liu, and e. ding, “eaten:\n",
            "entity-aware attention for single shot visual text extraction,”\n",
            "inicdar , 2019, pp. 254–259.\n",
            "[35] x. zhen-long, z. shui-geng, z. cheng, b. fan, n. yi, and p . shil-\n",
            "iang, “towards pure end-to-end learning for recognizing mul-\n",
            "tiple text sequences from an image,” arxiv: computer vision and\n",
            "pattern recognition , 2019.\n",
            "[36] a. w. harley, a. ufkes, and k. derpanis, “evaluation of deep\n",
            "convolutional nets for document image classiﬁcation and re-\n",
            "trieval,” icdar , pp. 991–995, 2015.\n",
            "[37] j. wang, c. liu, l. jin, g. tang, j. zhang, s. zhang, q. wang,\n",
            "y. wu, and m. cai, “towards robust visual information extrac-\n",
            "tion in real world: new dataset and novel solution,” vol. 35,\n",
            "no. 4, 2021, pp. 2738–2745.\n",
            "[38] b. p . majumder, n. potti, s. tata, j. b. wendt, q. zhao, and\n",
            "m. najork, “representation learning for information extraction\n",
            "from form-like documents,” in acl , 2020, pp. 6495–6504.\n",
            "[39] m. rusi ˜nol, t. benkhelfallah, and v . p . d’andecy, “field extrac-\n",
            "tion from administrative documents by incremental structural\n",
            "templates,” in icdar , 2013, pp. 1100–1104.\n",
            "[40] h. t. ha, m. medved, z. neverilov ´a, and a. horak, “recognition\n",
            "of ocr invoice metadata block types,” in international conference\n",
            "on text, speech, and dialogue . springer, 2018, pp. 304–312.\n",
            "[41] x. yang, e. yumer, p . asente, m. kraley, d. kifer, and c. l. giles,\n",
            "“learning to extract semantic structure from documents using\n",
            "multimodal fully convolutional neural networks,” in cvpr ,\n",
            "2017, pp. 4342–4351.\n",
            "[42] f. gralinski, t. stanislawek, a. wr ´oblewska, d. lipi ´nski, a. k.\n",
            "kaliska, p . rosalska, b. topolski, and p . biecek, “kleister: a novel\n",
            "task for information extraction involving long documents with\n",
            "complex layout,” arxiv , vol. abs/2003.02356, 2020.\n",
            "[43] x. chen, l. jin, y. zhu, c. luo, and t. wang, “text recognition\n",
            "in the wild: a survey,” acm computing surveys (csur) , vol. 54,\n",
            "no. 2, pp. 1–35, 2021.\n",
            "[44] d. r. judd, b. karsh, r. subbaroyan, t. toman, r. lahiri, and\n",
            "p . lok, “apparatus and method for searching and retrieving\n",
            "structured, semi-structured and unstructured content,” mar. 4\n",
            "2004, us patent app. 10/439,338.\n",
            "[45] s. soderland, “learning information extraction rules for semi-\n",
            "structured and free text,” mach. learn. , vol. 34, no. 1-3, pp. 233–\n",
            "272, 1999.\n",
            "[46] y. zhuang, m. cai, x. li, x. luo, q. yang, and f. wu, “the next\n",
            "breakthroughs of artiﬁcial intelligence: the interdisciplinary\n",
            "nature of ai,” engineering , vol. 6, no. 3, pp. 245–247, 2020.[47] d. esser, d. schuster, k. muthmann, m. berger, and a. schill,\n",
            "“automatic indexing of scanned documents: a layout-based ap-\n",
            "proach,” in document recognition and retrieval xix, part of the\n",
            "is&t-spie electronic imaging symposium , ser. spie proceedings,\n",
            "vol. 8297, p. 82970h.\n",
            "[48] x. liu, d. liang, s. yan, d. chen, y. qiao, and j. yan, “fots: fast\n",
            "oriented text spotting with a uniﬁed network,” in cvpr , 2018,\n",
            "pp. 5676–5685.\n",
            "[49] l. qiao, y. chen, z. cheng, y. xu, y. niu, s. pu, and f. wu,\n",
            "“mango: a mask attention guided one-stage scene text\n",
            "spotter,” in aaai , vol. 35, no. 3, 2021, pp. 2467–2476.\n",
            "[50] y. xu, c. zhang, z. cheng, j. xie, y. niu, s. pu, and f. wu,\n",
            "“segregated temporal assembly recurrent networks for weakly\n",
            "supervised multiple action detection,” in aaai , vol. 33, 2019,\n",
            "pp. 9070–9078.\n",
            "[51] v . yadav and s. bethard, “a survey on recent advances in\n",
            "named entity recognition from deep learning models,” in\n",
            "coling , 2018, pp. 2145–2158.\n",
            "[52] x.-y. duan, s.-l. tang, s.-y. zhang, y. zhang, z. zhao, j.-r. xue,\n",
            "y.-t. zhuang, and f. wu, “temporality-enhanced knowledge\n",
            "memory network for factoid question answering,” frontiers of\n",
            "information technology & electronic engineering , vol. 19, no. 1, pp.\n",
            "104–115, 2018.\n",
            "[53] k. kuang, l. li, z. geng, l. xu, k. zhang, b. liao, h. huang,\n",
            "p . ding, w. miao, and z. jiang, “causal inference,” engineering ,\n",
            "vol. 6, pp. 253–263, 2020.\n",
            "[54] s. ren, k. he, r. b. girshick, and j. sun, “faster r-cnn: towards\n",
            "real-time object detection with region proposal networks,” in\n",
            "neurips , 2015, pp. 91–99.\n",
            "[55] p . he, w. huang, t. he, q. zhu, y. qiao, and x. li, “single shot\n",
            "text detector with regional attention,” in iccv , 2017, pp. 3066–\n",
            "3074.\n",
            "[56] m. liao, b. shi, and x. bai, “textboxes++: a single-shot oriented\n",
            "scene text detector,” ieee tip , vol. 27, no. 8, pp. 3676–3690, 2018.\n",
            "[57] m. liao, z. zhu, b. shi, g. xia, and x. bai, “rotation-sensitive\n",
            "regression for oriented scene text detection,” in cvpr , 2018,\n",
            "pp. 5909–5918.\n",
            "[58] j. ma, w. shao, h. ye, l. wang, h. wang, y. zheng, and x. xue,\n",
            "“arbitrary-oriented scene text detection via rotation propos-\n",
            "als,” ieee tmm , vol. 20, no. 11, pp. 3111–3122, 2018.\n",
            "[59] y. liu and l. jin, “deep matching prior network: toward tighter\n",
            "multi-oriented text detection,” in cvpr , 2017, pp. 3454–3461.\n",
            "[60] b. shi, x. bai, and s. j. belongie, “detecting oriented text in\n",
            "natural images by linking segments,” in cvpr .\n",
            "[61] f. borisyuk, a. gordo, and v . sivakumar, “rosetta: large scale\n",
            "system for text detection and recognition in images,” in kdd ,\n",
            "2018, pp. 71–79.\n",
            "[62] k. he, g. gkioxari, p . doll ´ar, and r. b. girshick, “mask r-cnn,”\n",
            "iniccv , 2017, pp. 2980–2988.\n",
            "[63] e. xie, y. zang, s. shao, g. yu, c. yao, and g. li, “scene text\n",
            "detection with supervised pyramid context network,” in aaai ,\n",
            "vol. 33, no. 01, 2019, pp. 9038–9045.\n",
            "[64] c. zhang, b. liang, z. huang, m. en, j. han, e. ding, and\n",
            "x. ding, “look more than once: an accurate detector for text\n",
            "of arbitrary shapes,” in cvpr , 2019, pp. 10 552–10 561.\n",
            "[65] z. liu, g. lin, s. yang, f. liu, w. lin, and w. l. goh, “towards\n",
            "robust curve text detection with conditional spatial expan-\n",
            "sion,” in cvpr , 2019, pp. 7269–7278.\n",
            "[66] x. zhou, c. yao, h. wen, y. wang, s. zhou, w. he, and j. liang,\n",
            "“east: an efﬁcient and accurate scene text detector,” in cvpr ,\n",
            "2017, pp. 2642–2651.\n",
            "[67] s. long, j. ruan, w. zhang, x. he, w. wu, and c. yao,\n",
            "“textsnake: a flexible representation for detecting text of arbi-\n",
            "trary shapes,” in eccv , 2018, pp. 19–35.\n",
            "[68] w. wang, e. xie, x. li, w. hou, t. lu, g. yu, and s. shao,\n",
            "“shape robust text detection with progressive scale expansion\n",
            "network,” in cvpr , june 2019.\n",
            "[69] y. xu, y. wang, w. zhou, y. wang, z. yang, and x. bai, “textﬁeld:\n",
            "learning a deep direction ﬁeld for irregular scene text detection,”\n",
            "ieee tip , vol. 28, no. 11, pp. 5566–5579, 2019.\n",
            "[70] b. shi, x. bai, and c. yao, “an end-to-end trainable neural\n",
            "network for image-based sequence recognition and its appli-\n",
            "cation to scene text recognition,” ieee tpami. , vol. 39, no. 11,\n",
            "pp. 2298–2304, 2017.\n",
            "[71] b. shi, m. yang, x. wang, p . lyu, c. yao, and x. bai, “aster: an\n",
            "attentional scene text recognizer with flexible rectiﬁcation,”\n",
            "ieee tpami , pp. 1–1, 2018.17\n",
            "[72] z. cheng, f. bai, y. xu, g. zheng, s. pu, and s. zhou, “focus-\n",
            "ing attention: towards accurate text recognition in natural\n",
            "images,” in iccv , 2017, pp. 5086–5094.\n",
            "[73] a. graves, s. fern ´andez, f. gomez, and j. schmidhuber, “con-\n",
            "nectionist temporal classiﬁcation : labelling unsegmented se-\n",
            "quence data with recurrent neural networks,” in icml , 2006,\n",
            "pp. 369–376.\n",
            "[74] j. wang and x. hu, “gated recurrent convolution neural network\n",
            "for ocr,” in neurips , 2017, pp. 335–344.\n",
            "[75] c. lee and s. osindero, “recursive recurrent nets with atten-\n",
            "tion modeling for ocr in the wild,” in cvpr , 2016, pp. 2231–\n",
            "2239.\n",
            "[76] z. cheng, y. xu, f. bai, y. niu, s. pu, and s. zhou, “aon:\n",
            "towards arbitrarily-oriented text recognition,” in cvpr , 2018,\n",
            "pp. 5571–5579.\n",
            "[77] h. li, p . wang, and c. shen, “towards end-to-end text spotting\n",
            "with convolutional recurrent neural networks,” in iccv , 2017,\n",
            "pp. 5248–5256.\n",
            "[78] t. he, z. tian, w. huang, c. shen, y. qiao, and c. sun, “an end-\n",
            "to-end textspotter with explicit alignment and attention,” in\n",
            "cvpr , 2018, pp. 5020–5029.\n",
            "[79] m. busta, l. neumann, and j. matas, “deep textspotter: an\n",
            "end-to-end trainable scene text localization and recognition\n",
            "framework,” in iccv , 2017, pp. 2223–2231.\n",
            "[80] p . lyu, m. liao, c. yao, w. wu, and x. bai, “mask textspotter:\n",
            "an end-to-end trainable neural network for spotting text with\n",
            "arbitrary shapes,” in eccv , 2018, pp. 71–88.\n",
            "[81] e. riloff, “automatically constructing a dictionary for informa-\n",
            "tion extraction tasks,” in ncai , 1993, pp. 811–816.\n",
            "[82] s. b. huffman, “learning information extraction patterns from\n",
            "examples,” in connectionist, statistical, and symbolic approaches\n",
            "to learning for natural language processing , ser. lecture notes in\n",
            "computer science, vol. 1040, 1995, pp. 246–260.\n",
            "[83] i. muslea et al. , “extraction patterns for information extraction\n",
            "tasks: a survey,” in aaai , vol. 2, no. 2, 1999.\n",
            "[84] e. medvet, a. bartoli, and g. davanzo, “a probabilistic approach\n",
            "to printed document understanding,” ijdar , vol. 14, pp. 335–\n",
            "347, 2010.\n",
            "[85] f. cesarini, e. francesconi, m. gori, and g. soda, “analysis and\n",
            "understanding of multi-class invoices,” dar , vol. 6, pp. 102–114,\n",
            "2003.\n",
            "[86] m. shilman, p . liang, and p . viola, “learning nongenerative\n",
            "grammatical models for document analysis,” in iccv , 2005, pp.\n",
            "962–969.\n",
            "[87] j. devlin, m. chang, k. lee, and k. toutanova, “bert: pre-\n",
            "training of deep bidirectional transformers for language un-\n",
            "derstanding,” in naacl-hlt , 2019, pp. 4171–4186.\n",
            "[88] z. dai, z. yang, y. yang, j. g. carbonell, q. v . le, and r. salakhut-\n",
            "dinov, “transformer-xl: attentive language models beyond a\n",
            "fixed-length context,” in acl , 2019, pp. 2978–2988.\n",
            "[89] z. yang, z. dai, y. yang, j. g. carbonell, r. salakhutdinov, and\n",
            "q. v . le, “xlnet: generalized autoregressive pretraining for\n",
            "language understanding,” in neurips , 2019, pp. 5754–5764.\n",
            "[90] c. li, b. bi, m. yan, w. wang, s. huang, f. huang, and l. si,\n",
            "“structurallm: structural pre-training for form understanding,”\n",
            "arxiv preprint arxiv:2105.11210 , 2021.\n",
            "[91] y. li, y. qian, y. yu, x. qin, c. zhang, y. liu, k. yao, j. han,\n",
            "j. liu, and e. ding, “structext: structured text understanding\n",
            "with multi-modal transformers,” in acm mm , 2021, pp. 1912–\n",
            "1920.\n",
            "[92] ł. garncarek, r. powalski, t. stanisławek, b. topolski, p . halama,\n",
            "m. turski, and f. grali ´nski, “lambert: layout-aware language\n",
            "modeling for information extraction,” in icdar . springer, 2021,\n",
            "pp. 532–547.\n",
            "[93] r. powalski, ł. borchmann, d. jurkiewicz, t. dwojak,\n",
            "m. pietruszka, and g. pałka, “going full-tilt boogie on doc-\n",
            "ument understanding with text-image-layout transformer,” in\n",
            "icdar . springer, 2021, pp. 732–747.\n",
            "[94] p . li, j. gu, j. kuen, v . i. morariu, h. zhao, r. jain, v . manjunatha,\n",
            "and h. liu, “selfdoc: self-supervised document representation\n",
            "learning,” in cvpr , 2021, pp. 5652–5660.\n",
            "[95] d. lewis, g. agam, s. argamon, o. frieder, d. grossman, and\n",
            "j. heard, “building a test collection for complex document infor-\n",
            "mation processing,” in proceedings of the 29th annual international\n",
            "acm sigir conference on research and development in information\n",
            "retrieval , 2006, pp. 665–666.[96] m. carbonell, a. forn ´es, m. villegas, and j. llad ´os, “treynet:\n",
            "a neural model for text localization, transcription and named\n",
            "entity recognition in full pages,” pattern recognition letters , vol.\n",
            "136, pp. 219–227, 2020.\n",
            "[97] t. he, z. zhang, h. zhang, z. zhang, j. xie, and m. li,\n",
            "“bag of tricks for image classiﬁcation with convolutional neural\n",
            "networks,” in cvpr , 2019, pp. 558–567.\n",
            "[98] t. lin, p . doll ´ar, r. b. girshick, k. he, b. hariharan, and s. j.\n",
            "belongie, “feature pyramid networks for object detection,” in\n",
            "cvpr , 2017, pp. 936–944.\n",
            "[99] s. hochreiter and j. schmidhuber, “long short-term memory,”\n",
            "neural computation , vol. 9, no. 8, pp. 1735–1780, 1997.\n",
            "[100] l. h. li, m. yatskar, d. yin, c. hsieh, and k. chang, “visualbert:\n",
            "a simple and performant baseline for vision and language,”\n",
            "arxiv preprint arxiv:1908.03557 , 2019.\n",
            "[101] j. lu, d. batra, d. parikh, and s. lee, “vilbert: pretrain-\n",
            "ing task-agnostic visiolinguistic representations for vision-and-\n",
            "language tasks,” in neurips , 2019, pp. 13–23.\n",
            "[102] x. zhang, j. j. zhao, and y. lecun, “character-level convolu-\n",
            "tional networks for text classiﬁcation,” in neurips , 2015, pp.\n",
            "649–657.\n",
            "[103] e. f. t. k. sang and j. veenstra, “representing text chunks,” in\n",
            "eacl , 1999, pp. 173–179.\n",
            "[104] z. huang, k. chen, j. he, x. bai, d. karatzas, s. lu, and c. v .\n",
            "jawahar, “icdar2019 competition on scanned receipt ocr and\n",
            "information extraction,” in icdar , 2019, pp. 1516–1520.\n",
            "[105] g. jaume, h. k. ekenel, and j. thiran, “funsd: a dataset for\n",
            "form understanding in noisy scanned documents,” icdarw ,\n",
            "vol. 2, pp. 1–6, 2019.\n",
            "[106] s. park, s. shin, b. lee, j. lee, j. surh, m. seo, and h.-s. lee,\n",
            "“cord: a consolidated receipt dataset for post-ocr parsing,”\n",
            "inworkshop on document intelligence at neurips 2019 , 2019.\n",
            "[107] l. xu, q. dong, y. liao, c. yu, y. tian, w. liu, l. li, c. liu,\n",
            "x. zhang et al. , “cluener2020: ﬁne-grained named entity\n",
            "recognition dataset and benchmark for chinese,” arxiv preprint\n",
            "arxiv:2001.04351 , 2020.\n",
            "[108] a. paszke, s. gross, f. massa, a. lerer, j. bradbury, g. chanan,\n",
            "t. killeen, z. lin, n. gimelshein, l. antiga, a. desmaison,\n",
            "a. k ¨opf, e. yang, z. devito, m. raison, a. tejani, s. chil-\n",
            "amkurthy, b. steiner, l. fang, j. bai, and s. chintala, “pytorch:\n",
            "an imperative style, high-performance deep learning library,”\n",
            "inneurips , 2019, pp. 8024–8035.\n",
            "[109] i. loshchilov and f. hutter, “decoupled weight decay regulariza-\n",
            "tion,” in international conference on learning representations , 2018.\n",
            "[110] k. wang, b. babenko, and s. j. belongie, “end-to-end scene text\n",
            "recognition,” in iccv , d. n. metaxas, l. quan, a. sanfeliu, and\n",
            "l. v . gool, eds., 2011, pp. 1457–1464.\n",
            "[111] w. hwang, j. yim, s. park, s. yang, and m. seo, “spatial de-\n",
            "pendency parsing for semi-structured document information\n",
            "extraction,” arxiv preprint arxiv:2005.00642 , 2020.\n",
            "[112] p . veli ˇckovi ´c, g. cucurull, a. casanova, a. romero, p . li `o, and\n",
            "y. bengio, “graph attention networks,” in iclr , 2018.\n",
            "[113] y. qian, e. santus, z. jin, j. guo, and r. barzilay, “graphie: a\n",
            "graph-based framework for information extraction,” in naacl-\n",
            "hlt, 2019.\n",
            "[114] g. tang, l. xie, l. jin, j. wang, j. chen, z. xu, q. wang, y. wu,\n",
            "and h. li, “matchvie: exploiting match relevancy between\n",
            "entities for visual information extraction,” ijcai , 2021.\n",
            "[115] m. li, y. xu, l. cui, s. huang, f. wei, z. li, and m. zhou,\n",
            "“docbank: a benchmark dataset for document layout analysis,”\n",
            "incoling , 2020, pp. 949–960.\n",
            "[116] y. du, c. li, r. guo, x. yin, w. liu, j. zhou, y. bai, z. yu, y. yang,\n",
            "q. dang et al. , “pp-ocr: a practical ultra lightweight ocr\n",
            "system,” arxiv preprint arxiv:2009.09941 , 2020.\n",
            "layoutlmv3: pre-training for document ai\n",
            "with unified text and image masking\n",
            "yupan huang∗\n",
            "sun yat-sen university\n",
            "huangyp28@mail2.sysu.edu.cntengchao lv\n",
            "microsoft research asia\n",
            "tengchaolv@microsoft.comlei cui\n",
            "microsoft research asia\n",
            "lecu@microsoft.com\n",
            "yutong lu\n",
            "sun yat-sen university\n",
            "luyutong@mail.sysu.edu.cnfuru wei\n",
            "microsoft research asia\n",
            "fuwei@microsoft.com\n",
            "abstract\n",
            "self-supervised pre-training techniques have achieved remarkable\n",
            "progress in document ai. most multimodal pre-trained models\n",
            "use a masked language modeling objective to learn bidirectional\n",
            "representations on the text modality, but they differ in pre-training\n",
            "objectives for the image modality. this discrepancy adds difficulty\n",
            "to multimodal representation learning. in this paper, we propose\n",
            "layoutlmv3 to pre-train multimodal transformers for document\n",
            "ai with unified text and image masking. additionally, layoutlmv3\n",
            "is pre-trained with a word-patch alignment objective to learn cross-\n",
            "modal alignment by predicting whether the corresponding image\n",
            "patch of a text word is masked. the simple unified architecture\n",
            "and training objectives make layoutlmv3 a general-purpose pre-\n",
            "trained model for both text-centric and image-centric document ai\n",
            "tasks. experimental results show that layoutlmv3 achieves state-\n",
            "of-the-art performance not only in text-centric tasks, including\n",
            "form understanding, receipt understanding, and document visual\n",
            "question answering, but also in image-centric tasks such as docu-\n",
            "ment image classification and document layout analysis. the code\n",
            "and models are publicly available at https://aka.ms/layoutlmv3.\n",
            "ccs concepts\n",
            "•applied computing →document analysis ;•computing\n",
            "methodologies→natural language processing .\n",
            "keywords\n",
            "document ai, layoutlm, multimodal pre-training, vision-and-language\n",
            "acm reference format:\n",
            "yupan huang, tengchao lv, lei cui, yutong lu, and furu wei. 2022. lay-\n",
            "outlmv3: pre-training for document ai with unified text and image mask-\n",
            "ing. in proceedings of the 30th acm international conference on multimedia\n",
            "(mm ’22), october 10–14, 2022, lisboa, portugal. acm, new york, ny, usa,\n",
            "10 pages. https://doi.org/10.1145/3503161.3548112\n",
            "∗contribution during internship at microsoft research. corresponding authors: lei\n",
            "cui and furu wei.\n",
            "permission to make digital or hard copies of all or part of this work for personal or\n",
            "classroom use is granted without fee provided that copies are not made or distributed\n",
            "for profit or commercial advantage and that copies bear this notice and the full citation\n",
            "on the first page. copyrights for components of this work owned by others than acm\n",
            "must be honored. abstracting with credit is permitted. to copy otherwise, or republish,\n",
            "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
            "fee. request permissions from permissions@acm.org.\n",
            "mm ’22, october 10–14, 2022, lisboa, portugal\n",
            "©2022 association for computing machinery.\n",
            "acm isbn 978-1-4503-9203-7/22/10. . . $15.00\n",
            "https://doi.org/10.1145/3503161.3548112\n",
            "(a) text-centric form under-\n",
            "standing on funsd\n",
            "(b) image-centric layout anal-\n",
            "ysis on publaynet\n",
            "figure 1: examples of document ai tasks.\n",
            "1 introduction\n",
            "in recent years, pre-training techniques have been making waves\n",
            "in the document ai community by achieving remarkable progress\n",
            "on document understanding tasks [ 2,13–15,17,25,28,31,32,40,\n",
            "41,50,52,54–56]. as shown in figure 1, a pre-trained document\n",
            "ai model can parse layout and extract key information for various\n",
            "documents such as scanned forms and academic papers, which is\n",
            "important for industrial applications and academic research [8].\n",
            "self-supervised pre-training techniques have made rapid progress\n",
            "in representation learning due to their successful applications of re-\n",
            "constructive pre-training objectives. in nlp research, bert firstly\n",
            "proposed “masked language modeling” (mlm) to learn bidirec-\n",
            "tional representations by predicting the original vocabulary id of\n",
            "a randomly masked word token based on its context [9]. whereas\n",
            "most performant multimodal pre-trained document ai models use\n",
            "the mlm proposed by bert for text modality, they differ in pre-\n",
            "training objectives for image modality as depicted in figure 2. for\n",
            "example, docformer learns to reconstruct image pixels through a\n",
            "cnn decoder [ 2], which tends to learn noisy details rather than\n",
            "high-level structures such as document layouts [ 43,45]. selfdoc\n",
            "proposes to regress masked region features [ 31], which is noisier\n",
            "and harder to learn than classifying discrete features in a smaller\n",
            "vocabulary [ 6,18]. the different granularities of image (e.g., dense\n",
            "image pixels or contiguous region features) and text (i.e., discretearxiv:2204.08387v3  [cs.cl]  19 jul 2022faster r -cnnlinear embedding\n",
            "cnnword embeddingmasked word \n",
            "token classification\n",
            "visual transformer (optional)origin image \n",
            "reconstruction\n",
            "(a) patch\n",
            "(layoutlmv3)“a few \n",
            "number of \n",
            "tumor cell…”masked patch \n",
            "token classification\n",
            "(c) region\n",
            "(e.g., selfdoc )(b) grid\n",
            "(e.g., docformer )masked region \n",
            "feature regression\n",
            "𝑖𝑑𝑡1,𝑖𝑑𝑡2,𝑖𝑑𝑡3,\n",
            "𝑖𝑑𝑡4,𝑖𝑑𝑡5,𝑖𝑑𝑡6,…\n",
            "textmultimodal transformer(2)pre-training objectives of image modality\n",
            "(1)image embedding\n",
            "image\n",
            "𝑖𝑑𝑖1𝑖𝑑𝑖2𝑖𝑑𝑖3\n",
            "𝑖𝑑𝑖4𝑖𝑑𝑖5𝑖𝑑𝑖6\n",
            "𝑖𝑑𝑖7𝑖𝑑𝑖8𝑖𝑑𝑖9figure 2: comparisons with existing works (e.g., docformer\n",
            "[2] and selfdoc [31]) on (1) image embedding: our lay-\n",
            "outlmv3 uses linear patches to reduce the computational\n",
            "bottleneck of cnns and eliminate the need for region super-\n",
            "vision in training object detectors; (2) pre-training objectives\n",
            "on image modality: our layoutlmv3 learns to reconstruct\n",
            "discrete image tokens of masked patches instead of raw pix-\n",
            "els or region features to capture high-level layout structures\n",
            "rather than noisy details.\n",
            "tokens) objectives further add difficulty to cross-modal alignment\n",
            "learning, which is essential to multimodal representation learning.\n",
            "to overcome the discrepancy in pre-training objectives of text\n",
            "and image modalities and facilitate multimodal representation learn-\n",
            "ing, we propose layoutlmv3 to pre-train multimodal transform-\n",
            "ers for document ai with unified text and image masking objectives\n",
            "mlm and mim. as shown in figure 3, layoutlmv3 learns to recon-\n",
            "struct masked word tokens of the text modality and symmetrically\n",
            "reconstruct masked patch tokens of the image modality. inspired\n",
            "by dall-e [ 43] and beit [ 3], we obtain the target image tokens\n",
            "from latent codes of a discrete vae. for documents, each text word\n",
            "corresponds to an image patch. to learn this cross-modal alignment,\n",
            "we propose a word-patch alignment (wpa) objective to predict\n",
            "whether the corresponding image patch of a text word is masked.\n",
            "inspired by vit [ 11] and vilt [ 22], layoutlmv3 directly lever-\n",
            "ages raw image patches from document images without complex\n",
            "pre-processing steps such as page object detection. layoutlmv3\n",
            "jointly learns image, text and multimodal representations in a trans-\n",
            "former model with unified mlm, mim and wpa objectives. this\n",
            "makes layoutlmv3 the first multimodal pre-trained document aimodel without cnns for image embeddings, which significantly\n",
            "saves parameters and gets rid of region annotations. the simple\n",
            "unified architecture and objectives make layoutlmv3 a general-\n",
            "purpose pre-trained model for both text-centric tasks and image-\n",
            "centric document ai tasks.\n",
            "we evaluated pre-trained layoutlmv3 models across five pub-\n",
            "lic benchmarks, including text-centric benchmarks: funsd [ 20]\n",
            "for form understanding, cord [ 39] for receipt understanding,\n",
            "docvqa [ 38] for document visual question answering, and image-\n",
            "centric benchmarks: rvl-cdip [ 16] for document image classifi-\n",
            "cation, publaynet [ 59] for document layout analysis. experiment\n",
            "results demonstrate that layoutlmv3 achieves state-of-the-art per-\n",
            "formance on these benchmarks with parameter efficiency. further-\n",
            "more, layoutlmv3 is easy to reproduce for its simple and neat\n",
            "architecture and pre-training objectives.\n",
            "our contributions are summarized as follows:\n",
            "•layoutlmv3 is the first multimodal model in document ai\n",
            "that does not rely on a pre-trained cnn or faster r-cnn\n",
            "backbone to extract visual features, which significantly saves\n",
            "parameters and eliminates region annotations.\n",
            "•layoutlmv3 mitigates the discrepancy between text and\n",
            "image multimodal representation learning with unified dis-\n",
            "crete token reconstructive objectives mlm and mim. we\n",
            "further propose a word-patch alignment (wpa) objective\n",
            "to facilitate cross-modal alignment learning.\n",
            "•layoutlmv3 is a general-purpose model for both text-centric\n",
            "and image-centric document ai tasks. for the first time, we\n",
            "demonstrate the generality of multimodal transformers to\n",
            "vision tasks in document ai.\n",
            "•experimental results show that layoutlmv3 achieves state-\n",
            "of-the-art performance in text-centric tasks and image-centric\n",
            "tasks in document ai. the code and models are publicly\n",
            "available at https://aka.ms/layoutlmv3.\n",
            "2 layoutlmv3\n",
            "figure 3 gives an overview of the layoutlmv3.\n",
            "2.1 model architecture\n",
            "layoutlmv3 applies a unified text-image multimodal transformer\n",
            "to learn cross-modal representations. the transformer has a multi-\n",
            "layer architecture and each layer mainly consists of multi-head\n",
            "self-attention and position-wise fully connected feed-forward net-\n",
            "works [ 49]. the input of transformer is a concatenation of text\n",
            "embedding y=y1:𝐿and image embedding x=x1:𝑀sequences,\n",
            "where𝐿and𝑀are sequence lengths for text and image respectively.\n",
            "through the transformer, the last layer outputs text-and-image\n",
            "contextual representations.\n",
            "text embedding. text embedding is a combination of word em-\n",
            "beddings and position embeddings. we pre-processed document\n",
            "images with an off-the-shelf ocr toolkit to obtain textual con-\n",
            "tent and corresponding 2d position information. we initialize the\n",
            "word embeddings with a word embedding matrix from a pre-trained\n",
            "model roberta [ 36].the position embeddings include 1d position\n",
            "and 2d layout position embeddings, where the 1d position refers\n",
            "to the index of tokens within the text sequence, and the 2d lay-\n",
            "out position refers to the bounding box coordinates of the textmultimodal transformer\n",
            "seg1\n",
            "[mask]1+\n",
            "+seg1\n",
            "[mask]2+\n",
            "+seg2\n",
            "t33+\n",
            "+\n",
            "v1[mask]1\n",
            "+2\n",
            "+3\n",
            "+\n",
            "[mask] [spe]0\n",
            "+seg3\n",
            "t44+\n",
            "+segpad\n",
            "[cls]0+\n",
            "+4\n",
            "+\n",
            "v4hmlm head mim headt2\n",
            "hv2\n",
            "hv3\n",
            "pre-training\n",
            "objectives\n",
            "word/patch\n",
            "embedding1d position\n",
            "embedding\n",
            "flatten \n",
            "maskingocr parser\n",
            "masking2d position\n",
            "embeddinght1\n",
            "segpad\n",
            "[sep]5+\n",
            "+hwpa headaligned\n",
            "hunaligned\n",
            "patch1\n",
            "+patch2\n",
            "+patch3\n",
            "+patchpad\n",
            "+patch4\n",
            "+\n",
            "image patchesdocument imageresize\n",
            "split(unaligned)\n",
            "(aligned)(t1)(t2) (v2)(v3)\n",
            "linear\n",
            "embeddingword\n",
            "embedding\n",
            "figure 3: the architecture and pre-training objectives of layoutlmv3. layoutlmv3 is a pre-trained multimodal transformer\n",
            "for document ai with unified text and image masking objectives. given an input document image and its corresponding text\n",
            "and layout position information, the model takes the linear projection of patches and word tokens as inputs and encodes them\n",
            "into contextualized vector representations. layoutlmv3 is pre-trained with discrete token reconstructive objectives of masked\n",
            "language modeling (mlm) and masked image modeling (mim). additionally, layoutlmv3 is pre-trained with a word-patch\n",
            "alignment (wpa) objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text\n",
            "word is masked. “seg” denotes segment-level positions. “[cls]”, “[mask]”, “[sep]” and “[spe]” are special tokens.\n",
            "sequence. following the layoutlm, we normalize all coordinates\n",
            "by the size of images, and use embedding layers to embed x-axis,\n",
            "y-axis, width and height features separately [ 54]. the layoutlm\n",
            "and layoutlmv2 adopt word-level layout positions, where each\n",
            "word has its positions. instead, we adopt segment-level layout posi-\n",
            "tions that words in a segment share the same 2d position since the\n",
            "words usually express the same semantic meaning [28].\n",
            "image embedding. existing multimodal models in document ai\n",
            "either extract cnn grid features [ 2,56] or rely on an object detector\n",
            "like faster r-cnn [ 44] to extract region features [ 14,31,40,54]\n",
            "for image embeddings, which accounts for heavy computation\n",
            "bottleneck or require region supervision. inspired by vit [ 11] and\n",
            "vilt [ 22], we represent document images with linear projection\n",
            "features of image patches before feeding them into the multimodal\n",
            "transformer. specifically, we resize a document image into 𝐻×𝑊\n",
            "and denote the image with i∈r𝐶×𝐻×𝑊, where𝐶,𝐻and𝑊are the\n",
            "channel size, width and height of the image respectively. we then\n",
            "split the image into a sequence of uniform 𝑃×𝑃patches, linearly\n",
            "project the image patches to 𝐷dimensions and flatten them into\n",
            "a sequence of vectors, which length is 𝑀=𝐻𝑊/𝑃2. then we addlearnable 1d position embeddings to each patch since we have\n",
            "not observed improvements from using 2d position embeddings in\n",
            "our preliminary experiments. layoutlmv3 is the first multimodal\n",
            "model in document ai that does not rely on cnns to extract image\n",
            "features, which is vital to document ai models to reduce parameters\n",
            "or remove complex pre-processing steps.\n",
            "we insert semantic 1d relative position and spatial 2d relative\n",
            "position as bias terms in self-attention networks for text and image\n",
            "modalities following layoutlmv2[56].\n",
            "2.2 pre-training objectives\n",
            "layoutlmv3 is pre-trained with the mlm, mim, and wpa objec-\n",
            "tives to learn multimodal representation in a self-supervised learn-\n",
            "ing manner. full pre-training objectives of layoutlmv3 is defined\n",
            "as𝐿=𝐿𝑀𝐿𝑀+𝐿𝑀𝐼𝑀+𝐿𝑊𝑃𝐴 .\n",
            "objective i: masked language modeling (mlm). for the lan-\n",
            "guage side, our mlm is inspired by the masked language mod-\n",
            "eling in bert [ 9] and masked visual-language modeling in lay-\n",
            "outlm [ 54] and layoutlmv2 [ 56]. we mask 30% of text tokens witha span masking strategy with span lengths drawn from a poisson\n",
            "distribution ( 𝜆=3) [21,27]. the pre-training objective is to maxi-\n",
            "mize the log-likelihood of the correct masked text tokens y𝑙based\n",
            "on the contextual representations of corrupted sequences of image\n",
            "tokens x𝑀′and text tokens y𝐿′, where𝑀′and𝐿′represent the\n",
            "masked positions. we denote parameters of the transformer model\n",
            "with𝜃and minimize the subsequent cross-entropy loss:\n",
            "𝐿𝑀𝐿𝑀(𝜃)=−𝐿′∑︁\n",
            "𝑙=1log𝑝𝜃\u0010\n",
            "yℓ|x𝑀′,y𝐿′\u0011\n",
            "(1)\n",
            "as we keep the layout information unchanged, this objective fa-\n",
            "cilitates the model to learn the correspondence between layout\n",
            "information and text and image context.\n",
            "objective ii: masked image modeling (mim). to encourage\n",
            "the model to interpret visual content from contextual text and\n",
            "image representations, we adapt the mim pre-training objective in\n",
            "beit [ 3] to our multimodal transformer model. the mim objective\n",
            "is a symmetry to the mlm objective, that we randomly mask a\n",
            "percentage of about 40% image tokens with the blockwise masking\n",
            "strategy [ 3]. the mim objective is driven by a cross-entropy loss\n",
            "to reconstruct the masked image tokens x𝑚under the context of\n",
            "their surrounding text and image tokens.\n",
            "𝐿𝑀𝐼𝑀(𝜃)=−𝑀′∑︁\n",
            "𝑚=1log𝑝𝜃\u0010\n",
            "x𝑚|x𝑀′,y𝐿′\u0011\n",
            "(2)\n",
            "the labels of image tokens come from an image tokenizer, which\n",
            "can transform dense image pixels into discrete tokens according to\n",
            "a visual vocabulary [ 43]. thus mim facilitates learning high-level\n",
            "layout structures rather than noisy low-level details.\n",
            "objective iii: word-patch alignment (wpa). for documents,\n",
            "each text word corresponds to an image patch. as we randomly\n",
            "mask text and image tokens with mlm and mim respectively, there\n",
            "is no explicit alignment learning between text and image modalities.\n",
            "we thus propose a wpa objective to learn a fine-grained alignment\n",
            "between text words and image patches. the wpa objective is to\n",
            "predict whether the corresponding image patches of a text word\n",
            "are masked. specifically, we assign an aligned label to an unmasked\n",
            "text token when its corresponding image tokens are also unmasked.\n",
            "otherwise, we assign an unaligned label. we exclude the masked\n",
            "text tokens when calculating wpa loss to prevent the model from\n",
            "learning a correspondence between masked text words and image\n",
            "patches. we use a two-layer mlp head that inputs contextual text\n",
            "and image and outputs the binary aligned/unaligned labels with a\n",
            "binary cross-entropy loss:\n",
            "𝐿𝑊𝑃𝐴(𝜃)=−𝐿−𝐿′∑︁\n",
            "ℓ=1log𝑝𝜃\u0010\n",
            "zℓ|x𝑀′,y𝐿′\u0011\n",
            ", (3)\n",
            "where𝐿−𝐿′is the number of unmasked text tokens, zℓis the binary\n",
            "label of language token in the ℓposition.\n",
            "3 experiments\n",
            "3.1 model configurations\n",
            "the network architecture of layoutlmv3 follows that of layoutlm [ 54]\n",
            "and layoutlmv2 [ 56] for a fair comparison. we use base and large\n",
            "model sizes for layoutlmv3. layoutlmv3 base adopts a 12-layertransformer encoder with 12-head self-attention, hidden size of\n",
            "𝐷=768, and 3,072 intermediate size of feed-forward networks.\n",
            "layoutlmv3 large adopts a 24-layer transformer encoder with\n",
            "16-head self-attention, hidden size of 𝐷=1,024, and 4,096 interme-\n",
            "diate size of feed-forward networks. to pre-process the text input,\n",
            "we tokenize the text sequence with byte-pair encoding (bpe) [ 46]\n",
            "with a maximum sequence length 𝐿=512. we add a [cls] and\n",
            "a [sep] token at the beginning and end of each text sequence.\n",
            "when the length of the text sequence is shorter than 𝐿, we append\n",
            "[pad] tokens to it. the bounding box coordinates of these spe-\n",
            "cial tokens are all zeros. the parameters for image embedding are\n",
            "𝐶×𝐻×𝑊=3×224×224,𝑃=16,𝑀=196.\n",
            "we adopt distributed and mixed-precision training to reduce\n",
            "memory costs and speed up training procedures. we also use a\n",
            "gradient accumulation mechanism to split the batch of samples into\n",
            "several mini-batches to overcome memory constraints for large\n",
            "batch sizes. we further use a gradient checkpointing technique for\n",
            "document layout analysis to reduce memory costs. to stabilize train-\n",
            "ing, we follow cogview [ 10] to change the computation of atten-\n",
            "tion to softmax\u0010q𝑇k√\n",
            "𝑑\u0011\n",
            "=softmax\u0010\u0010q𝑇\n",
            "𝛼√\n",
            "𝑑k−max\u0010q𝑇\n",
            "𝛼√\n",
            "𝑑k\u0011\u0011\n",
            "×𝛼\u0011\n",
            ",\n",
            "where𝛼is 32.\n",
            "3.2 pre-training layoutlmv3\n",
            "to learn a universal representation for various document tasks,\n",
            "we pre-train layoutlmv3 on a large iit-cdip dataset. the iit-\n",
            "cdip test collection 1.0 is a large-scale scanned document im-\n",
            "age dataset, which contains about 11 million document images\n",
            "and can split into 42 million pages [ 26]. we only use 11 million\n",
            "of them to train layoutlmv3. we do not do image augmentation\n",
            "following layoutlm models [ 54,56]. for the multimodal trans-\n",
            "former encoder along with the text embedding layer, layoutlmv3\n",
            "is initialized from the pre-trained weights of roberta [ 36]. our\n",
            "image tokenizer is initialized from a pre-trained image tokenizer\n",
            "in dit, a self-supervised pre-trained document image transformer\n",
            "model [ 30]. the vocabulary size of image tokens is 8,192. we ran-\n",
            "domly initialized the rest model parameters. we pre-train lay-\n",
            "outlmv3 using adam optimizer [ 23] with a batch size of 2,048 for\n",
            "500,000 steps. we use a weight decay of 1𝑒−2, and (𝛽1,𝛽2) = (0.9,\n",
            "0.98). for the layoutlmv3 base model, we use a learning rate of\n",
            "1𝑒−4, and we linearly warm up the learning rate over the first 4.8%\n",
            "steps. for layoutlmv3 large , the learning rate and warm-up ratio\n",
            "are5𝑒−5and 10%, respectively.\n",
            "3.3 fine-tuning on multimodal tasks\n",
            "we compare layoutlmv3 with typical self-supervised pre-training\n",
            "approaches and categorize them by their pre-training modalities.\n",
            "•[t] text modality : bert [ 9] and roberta [ 36] are typical\n",
            "pre-trained language models which only use text information\n",
            "with transformer architecture. we use funsd and rvl-\n",
            "cdip results of the roberta from layoutlm [ 54] and results\n",
            "of bert from layoutlmv2 [ 56]. we reproduce and report\n",
            "the cord and docvqa results of the roberta.\n",
            "•[t+l] text and layout modalities : layoutlm incorporates\n",
            "layout information by adding word-level spatial embeddings\n",
            "to embeddings of bert [ 54]. structurallm leverages segment-\n",
            "level layout information [ 28]. bros encodes relative layouttable 1: comparison with existing published models on the cord [39], funsd [20], rvl-cdip [16], and docvqa [38] datasets.\n",
            "“t/l/i” denotes “text/layout/image” modality. “r/g/p” denotes “region/grid/patch” image embedding. we multiply all values\n",
            "by a hundred for better readability.†in the udoc paper [14], the cord splits are 626/247 receipts for training/test instead of\n",
            "the official 800/100 training/test receipts adopted by other works. thus the score†is not directly comparable to other scores.\n",
            "models denoted with‡use more data to train docvqa and are expected to score higher. for example, tilt introduces one\n",
            "more supervised training stage on more qa datasets [40]. structurallm additionally uses the validation set in training [28].\n",
            "model parameters modality image embeddingfunsd cord rvl-cdip docvqa\n",
            "f1↑ f1↑ accuracy↑ anls↑\n",
            "bert base [9] 110m t none 60.26 89.68 89.81 63.72\n",
            "roberta base [36] 125m t none 66.48 93.54 90.06 66.42\n",
            "bros base [17] 110m t+l none 83.05 95.73 - -\n",
            "lilt base [50] - t+l none 88.41 96.07 95.68* -\n",
            "layoutlmbase [54] 160m t+l+i (r) resnet-101 (fine-tune) 79.27 - 94.42 -\n",
            "selfdoc [31] - t+l+i (r) resnext-101 83.36 - 92.81 -\n",
            "udoc [14] 272m t+l+i (r) resnet-50 87.93 98.94†95.05 -\n",
            "tilt base [40] 230m t+l+i (r) u-net - 95.11 95.25 83.92‡\n",
            "xylayoutlmbase [15] - t+l+i (g) resnext-101 83.35 - - -\n",
            "layoutlmv2base [56] 200m t+l+i (g) resnext101-fpn 82.76 94.95 95.25 78.08\n",
            "docformer base [2] 183m t+l+i (g) resnet-50 83.34 96.33 96.17 -\n",
            "layoutlmv3base (ours) 133m t+l+i (p) linear 90.29 96.56 95.44 78.76\n",
            "bert large [9] 340m t none 65.63 90.25 89.92 67.45\n",
            "roberta large [36] 355m t none 70.72 93.80 90.11 69.52\n",
            "layoutlmlarge [54] 343m t+l none 77.89 - 91.90 -\n",
            "bros large [17] 340m t+l none 84.52 97.40 - -\n",
            "structurallm large [28] 355m t+l none 85.14 - 96.08 83.94‡\n",
            "formnet [25] 217m t+l none 84.69 - - -\n",
            "formnet [25] 345m t+l none - 97.28 - -\n",
            "tilt large [40] 780m t+l+i (r) u-net - 96.33 95.52 87.05‡\n",
            "layoutlmv2large [56] 426m t+l+i (g) resnext101-fpn 84.20 96.01 95.64 83.48\n",
            "docformer large [2] 536m t+l+i (g) resnet-50 84.55 96.99 95.50 -\n",
            "layoutlmv3large (ours) 368m t+l+i (p) linear 92.08 97.46 95.93 83.37\n",
            "* lilt uses image features with resnext101-fpn backbone in fine-tuning rvl-cdip.\n",
            "positions [ 17]. lilt fine-tunes on different languages with\n",
            "pre-trained textual models [ 50]. formnet leverages the spa-\n",
            "tial relationship between tokens in a form [25].\n",
            "•[t+l+i (r)] text, layout and image modalities with faster\n",
            "r-cnn region features : this line of works extract im-\n",
            "age region features from roi heads in the faster r-cnn\n",
            "model [ 44]. among them, layoutlm [ 54] and tilt [ 40] use\n",
            "ocr words’ bounding box to serve as region proposals and\n",
            "add the region features to corresponding text embeddings.\n",
            "selfdoc [ 31] and udoc [ 14] use document object proposals\n",
            "and concatenate region features with text embeddings.\n",
            "•[t+l+i (g)] text, layout and image modalities with cnn\n",
            "grid features : layoutlmv2 [ 56] and docformer [ 2] extract\n",
            "image grid features with a cnn backbone without object\n",
            "detection. xylayoutlm [ 15] adopts the architecture of lay-\n",
            "outlmv2 and improves layout representation.\n",
            "•[t+l+i (p)] text, layout, and image modalities with lin-\n",
            "ear patch features : layoutlmv3 replaces cnn backbones\n",
            "with simple linear embedding to encode image patches.we fine-tune layoutlmv3 on multimodal tasks on publicly avail-\n",
            "able benchmarks. results are shown in table 1.\n",
            "task i: form and receipt understanding. form and receipt\n",
            "understanding tasks require extracting and structuring forms and\n",
            "receipts’ textual content. the tasks are a sequence labeling problem\n",
            "aiming to tag each word with a label. we predict the label of the\n",
            "last hidden state of each text token with a linear layer and an mlp\n",
            "classifier for form and receipt understanding tasks, respectively.\n",
            "we conduct experiments on the funsd dataset and the cord\n",
            "dataset. the funsd [20] is a noisy scanned form understand-\n",
            "ing dataset sampled from the rvl-cdip dataset [16]. the funsd\n",
            "dataset contains 199 documents with comprehensive annotations\n",
            "for 9,707 semantic entities. we focus on the semantic entity labeling\n",
            "task on the funsd dataset to assign each semantic entity a label\n",
            "among “question”, “answer”, “header” or “other”. the training and\n",
            "test splits contain 149 and 50 samples, respectively. cord [39] is a\n",
            "receipt key information extraction dataset with 30 semantic labels\n",
            "defined under 4 categories. it contains 1,000 receipts of 800 training,\n",
            "100 validation, and 100 test examples. we use officially-provided\n",
            "images and ocr annotations. we fine-tune layoutlmv3 for 1,000table 2: document layout analysis map @ iou [0.50:0.95] on publaynet validation set. all models use only information from\n",
            "the vision modality. layoutlmv3 outperforms the compared resnets [14, 59] and vision transformer [30] backbones.\n",
            "model framework backbone text title list table figure overall\n",
            "publaynet[59] mask r-cnn resnet-101 91.6 84.0 88.6 96.0 94.9 91.0\n",
            "dit base [30] mask r-cnn transformer 93.4 87.1 92.9 97.3 96.7 93.5\n",
            "udoc [14] faster r-cnn resnet-50 93.9 88.5 93.7 97.3 96.4 93.9\n",
            "dit base [30] cascade r-cnn transformer 94.4 88.9 94.8 97.6 96.9 94.5\n",
            "layoutlmv3base (ours) cascade r-cnn transformer 94.5 90.6 95.5 97.9 97.0 95.1\n",
            "steps with a learning rate of 1𝑒−5and a batch size of 16 for funsd,\n",
            "and5𝑒−5and 64 for cord.\n",
            "we report f1 scores for this task. for the large model size, the\n",
            "layoutlmv3 achieves an f1 score of 92.08 on the funsd dataset,\n",
            "which significantly outperforms the sota result of 85.14 provided\n",
            "by structurallm [ 28]. note that layoutlmv3 and structurallm use\n",
            "segment-level layout positions, while the other works use word-\n",
            "level layout positions. using segment-level positions may benefit\n",
            "the semantic entity labeling task on funsd [ 28], so the two types\n",
            "of work are not directly comparable. the layoutlmv3 also achieves\n",
            "sota f1 scores on the cord dataset for both base and large model\n",
            "sizes. the results show that layoutlmv3 can significantly benefit\n",
            "the text-centric form and receipt understanding tasks.\n",
            "task ii: document image classification. the document image\n",
            "classification task aims to predict the category of document images.\n",
            "we feed the output hidden state of the special classification token\n",
            "([cls]) into an mlp classifier to predict the class labels.\n",
            "we conduct experiments on the rvl-cdip dataset. it is a subset\n",
            "of the iit-cdip collection labeled with 16 categories [ 16]. rvl-cdip\n",
            "dataset contains 400,000 document images, among them 320,000\n",
            "are training images, 40,000 are validation images, and 40,000 are\n",
            "test images. we extract text and layout information using microsoft\n",
            "read api. we fine-tune layoutlmv3 for 20,000 steps with a batch\n",
            "size of 64 and a learning rate of 2𝑒−5.\n",
            "the evaluation metric is the overall classification accuracy. lay-\n",
            "outlmv3 achieves better or comparable results with a much smaller\n",
            "model size than previous works. for example, compared to lay-\n",
            "outlmv2, layoutlmv3 achieves an absolute improvement of 0.19%\n",
            "and 0.29% in the base model and large model size, respectively, with\n",
            "a much simpler image embedding (i.e., linear vs. resnext101-fpn).\n",
            "the results show that our simple image embeddings can achieve\n",
            "desirable results on image-centric tasks.\n",
            "task iii: document visual question answering. document\n",
            "visual question answering requires a model to take a document\n",
            "image and a question as input and output an answer [ 38]. we\n",
            "formalize this task as an extractive qa problem, where the model\n",
            "predicts start and end positions by classifying the last hidden state\n",
            "of each text token with a binary classifier.\n",
            "we conduct experiments on the docvqa dataset, a standard\n",
            "dataset for visual question answering on document images [ 38]. the\n",
            "official partition of the docvqa dataset consists of 10,194/1,286/1,287\n",
            "images and 39,463/5,349/5,188 questions for training/validation/test\n",
            "set, respectively. we train our model on the training set, evalu-\n",
            "ate the model on the test set, and report results by submitting\n",
            "them to the official evaluation website. we use microsoft read apito extract text and bounding boxes from images and use heuris-\n",
            "tics to find given answers in the extracted text as in layoutlmv2.\n",
            "we fine-tune layoutlmv3base for 100,000 steps with a batch size\n",
            "of 128, a learning rate of 3𝑒−5, and a warmup ratio of 0.048.\n",
            "forlayoutlmv3large , the step size, batch size, learning rate and\n",
            "warmup ratio are 200,000, 32, 1𝑒−5, and 0.1, respectively.\n",
            "we report the commonly-used edit distance-based metric anls\n",
            "(also known as average normalized levenshtein similarity). the\n",
            "layoutlmv3base improves the anls score of layoutlmv2base\n",
            "from 78.08 to 78.76, with much simpler image embedding (i.e., from\n",
            "resnext101-fpn to linear embedding). the layoutlmv3large\n",
            "further gains an absolute anls score of 4.61 over layoutlmv3base .\n",
            "the results show that layoutlmv3 is effective for the document\n",
            "visual question answering task.\n",
            "3.4 fine-tuning on a vision task\n",
            "to demonstrate the generality of layoutlmv3 from the multimodal\n",
            "domain to the visual domain, we transfer layoutlmv3 to a docu-\n",
            "ment layout analysis task. this task is about detecting the layouts\n",
            "of unstructured digital documents by providing bounding boxes and\n",
            "categories such as tables, figures, texts, etc. this task helps parse the\n",
            "documents into a machine-readable format for downstream appli-\n",
            "cations. we model this task as an object detection problem without\n",
            "text embedding, which is effective in existing works [ 14,30,59].\n",
            "we integrate the layoutlmv3 as feature backbone in the cascade\n",
            "r-cnn detector [ 4] with fpn [ 34] implemented using the detec-\n",
            "tron2 [ 53]. we adopt the standard practice to extract single-scale\n",
            "features from different transformer layers, such as layers 4, 6, 8, and\n",
            "12 of the layoutlmv3 base model. we use resolution-modifying\n",
            "modules to convert the single-scale features into the multiscale fpn\n",
            "features [1, 30, 33].\n",
            "we conduct experiments on publaynet dataset [ 59]. the dataset\n",
            "contains research paper images annotated with bounding boxes and\n",
            "polygonal segmentation across five document layout categories:\n",
            "text, title, list, figure, and table. the official splits contain 335,703\n",
            "training images, 11,245 validation images, and 11,405 test images.\n",
            "we train our model on the training split and evaluate our model\n",
            "on the validation split following standard practice [ 14,30,59]. we\n",
            "train our model for 60,000 steps using the adamw optimizer with\n",
            "1,000 warm-up steps and a weight decay of 0.05 following dit [ 30].\n",
            "since layoutlmv3 is pre-trained with inputs from both vision and\n",
            "language modalities, we use a larger batch size of 32 and a lower\n",
            "learning rate of 2𝑒−4empirically. we do not use flipping or crop-\n",
            "ping augmentation strategy in the fine-tuning stage to be consistenttable 3: ablation study on image embeddings and pre-training objectives on typical text-centric tasks (form and receipt under-\n",
            "standing on funsd and cord) and image-centric tasks (document image classification on rvl-cdip and document layout\n",
            "analysis on publaynet). all models were trained at base size on 1 million data for 150,000 steps with learning rate 3𝑒−4.\n",
            "#imageparameterspre-training funsd cord rvl-cdip publaynet\n",
            "embed objective(s) f1↑ f1↑ accuracy↑ map↑\n",
            "1 none 125m mlm 88.64 96.27 95.33 not applicable\n",
            "2 linear 126m mlm 89.39 96.11 95.00 loss divergence\n",
            "3 linear 132m mlm+mim 89.19 96.30 95.42 94.38\n",
            "4 linear 133m mlm+mim+wpa 89.78 96.49 95.53 94.43\n",
            "figure 4: loss convergence curves of fine-tuning the ablated\n",
            "models of layoutlmv3 on publaynet dataset. the loss of\n",
            "model #2 did not converge. by incorporating the mim objec-\n",
            "tive, the loss converges normally. the wpa objective further\n",
            "decreases the loss. best viewed in color.\n",
            "with our pre-training stage. we do not use relative positions in self-\n",
            "attention networks as dit.\n",
            "we measure the performance using the mean average preci-\n",
            "sion (map) @ intersection over union (iou) [0.50:0.95] of bound-\n",
            "ing boxes and report results in table 2. we compare with the\n",
            "resnets [ 14,59] and the concurrent vision transformer [ 30] back-\n",
            "bones. layoutlmv3 outperforms the other models in all metrics,\n",
            "achieving an overall map score of 95.1. layoutlmv3 achieves a\n",
            "high gain in the “title” category. since titles are typically much\n",
            "smaller than other categories and can be identified by their tex-\n",
            "tual content, we attribute this improvement to our incorporation\n",
            "of language modality in pre-training layoutlmv3. these results\n",
            "demonstrate the generality and superiority of layoutlmv3.\n",
            "3.5 ablation study\n",
            "in table 3 we study the effect of our image embeddings and pre-\n",
            "training objectives. we first build a baseline model #1 that uses text\n",
            "and layout information, pre-trained with mlm objective. then we\n",
            "use linearly projected image patches as the image embedding of the\n",
            "baseline model, denoted as model #2. we further pre-train model#2 with mim and wpa objectives step by step and denote the new\n",
            "models as #3 and #4, respectively.\n",
            "in figure 4, we visualize losses of models #2, #3, and #4 when\n",
            "fine-tuned on the publaynet dataset with a batch size of 16 and a\n",
            "learning rate of 2𝑒−4. we have tried to train the model #2 with\n",
            "learning rates of { 1𝑒−4,2𝑒−4,4𝑒−4} combined with batch sizes\n",
            "of {16,32}, but the loss of model #2 did not converge and the map\n",
            "score on publaynet is near zero.\n",
            "effect of linear image embedding. we observe that model #1\n",
            "without image embedding has achieved good results on some tasks.\n",
            "this suggests that language modality, including text and layout\n",
            "information, plays a vital role in document understanding. how-\n",
            "ever, the results are still unsatisfactory. moreover, model #1 cannot\n",
            "conduct some image-centric document analysis tasks without vi-\n",
            "sion modality. for example, the vision modality is critical for the\n",
            "document layout analysis task on publaynet because bounding\n",
            "boxes are tightly integrated with images. our simple design of\n",
            "linear image embedding combined with appropriate pre-training\n",
            "objectives can consistently improve not only image-centric tasks,\n",
            "but also some text-centric tasks further.\n",
            "effect of mim pre-training objective. simply concatenating lin-\n",
            "ear image embedding with text embedding as input to model #2\n",
            "deteriorates performance on cord and rvl-cdip, while the loss\n",
            "on publaynet diverges. we speculate that the model failed to learn\n",
            "meaningful visual representation on the linear patch embeddings\n",
            "without any pre-training objective associated with image modality.\n",
            "the mim objective mitigates this problem by preserving the image\n",
            "information until the last layer of the model by randomly masking\n",
            "out a portion of input image patches and reconstructing them in\n",
            "the output [ 22]. comparing the results of model #3 and model #2,\n",
            "the mim objective benefits cord and rvl-cdip. as simply using\n",
            "linear image embedding has improved funsd, mim does not fur-\n",
            "ther contribute to funsd. by incorporating the mim objective in\n",
            "training, the loss converges when fine-tuning publaynet as shown\n",
            "in figure 4, and we obtain a desirable map score. the results indi-\n",
            "cate that mim can help regularize the training. thus mim is critical\n",
            "for vision tasks like document layout analysis on publaynet.\n",
            "effect of wpa pre-training objective. by comparing models #3\n",
            "and #4 in table 3, we observe that the wpa objective consistently\n",
            "improves all tasks. moreover, the wpa objective decreases the loss\n",
            "of the vision task on publaynet in figure 4. these results confirm\n",
            "the effectiveness of wpa not only in cross-modal representation\n",
            "learning, but also in image representation learning.parameter comparisons. the table shows that incorporating\n",
            "image embedding for a 16×16patch projection (#1 →#2) introduces\n",
            "only 0.6m parameters. the parameters are negligible compared to\n",
            "the parameters of cnn backbones (e.g., 44m for resnet-101). a\n",
            "mim head and a wpa head introduce 6.9m and 0.6m parameters\n",
            "in the pre-training stage. the parameter overhead introduced by\n",
            "image embedding is marginal compared to the mlm head, which\n",
            "has 39.2m parameters for a text vocabulary size of 50,265. we did\n",
            "not take count of the image tokenizer when calculating parameters\n",
            "as the tokenizer is a standalone module for generating the labels of\n",
            "mim but is not integrated into the transformer backbone.\n",
            "4 related work\n",
            "multimodal self-supervised pre-training technique has made a\n",
            "rapid progress in document intelligence due to its successful applica-\n",
            "tions of document layout and image representation learning [ 2,13–\n",
            "15,17,25,28,31,32,40,41,50,52,54–56]. layoutlm and following\n",
            "works joint layout representation learning by encoding spatial co-\n",
            "ordinates of text [ 17,25,28,54]. various works then joint image\n",
            "representation learning by combining cnns with transformer [ 49]\n",
            "self-attention networks. these works either extract cnn grid fea-\n",
            "tures [ 2,56] or rely on an object detector to extract region fea-\n",
            "tures [ 14,31,40,54], which accounts for heavy computation bottle-\n",
            "neck or requires region supervision. in the field of natural images\n",
            "vision-and-language pre-training (vlp), research works have seen\n",
            "a shift from region features [ 5,47,48] to grid features [ 19] to lift\n",
            "limitations of pre-defined object classes and region supervision. in-\n",
            "spired by vision transformer (vit) [ 11], there have also been recent\n",
            "efforts in vlp without cnns to overcome the weakness of cnn.\n",
            "still, most rely on separate self-attention networks to learn visual\n",
            "features; thus, their computational cost is not reduced [ 12,29,57].\n",
            "an exception is vilt, which learns visual features with a light-\n",
            "weight linear layer and significantly cuts down the model size and\n",
            "running time [ 22]. inspired by vilt, our layoutlmv3 is the first\n",
            "multimodal model in document ai that utilizes image embeddings\n",
            "without cnns.\n",
            "reconstructive pre-training objectives revolutionized repre-\n",
            "sentation learning. in nlp research, bert firstly proposed “masked\n",
            "language modeling” (mlm) to learn bidirectional representations\n",
            "and advanced the state of the arts on broad language understanding\n",
            "tasks [ 9]. in the field of cv, masked image modeling (mim) aims\n",
            "to learn rich visual representations via predicting masked content\n",
            "conditioning in visible context. for example, vit reconstructs the\n",
            "mean color of masked patches, which leads to performance gains\n",
            "in imagenet classification [ 11]. beit reconstructs visual tokens\n",
            "learned by a discrete vae, achieving competitive results in image\n",
            "classification and semantic segmentation [ 3]. dit extends beit to\n",
            "document images to document layout analysis [30].\n",
            "inspired by mlm and mim, researchers in the field of vision-\n",
            "and-language have explored reconstructive objectives for multi-\n",
            "modal representation learning . whereas most well-performing\n",
            "vision-and-language pre-training (vlp) models use the mlm pro-\n",
            "posed by bert on text modality, they differ in their pre-training ob-\n",
            "jectives for the image modality. there are three variants of mim cor-\n",
            "responding to different image embeddings: masked region modeling\n",
            "(mrm), masked grid modeling (mgm), and masked patch modeling(mpm). mrm has been proven to be effective in regressing original\n",
            "region features [ 5,31,48] or classifying object labels [ 5,37,48] for\n",
            "masked regions. mgm has also been explored in the soho, whose\n",
            "objective is to predict the mapping index in a visual dictionary for\n",
            "masked grid features [ 19]. for patch-level image embedding, visual\n",
            "parsing [ 57] proposed to mask visual tokens according to the atten-\n",
            "tion weights in their self-attention image encoder, which does not\n",
            "apply to simple linear image encoders. vilt [ 22] and meter [ 12]\n",
            "attempt to leverage mpm similar to vit [ 11] and beit [ 3], which re-\n",
            "spectively reconstruct the mean color and discrete tokens in visual\n",
            "vocabularies for image patches, but resulted in degraded perfor-\n",
            "mance on downstream tasks. our layoutlmv3 firstly demonstrates\n",
            "the effectiveness of mim for linear patch image embedding.\n",
            "various cross-modal objectives are further developed for vi-\n",
            "sion and language (vl) alignment learning in multimodal models.\n",
            "image-text matching is widely used to learn a coarse-grained vl\n",
            "alignment [ 2,5,19,22,56]. to learn a fine-grained vl alignment,\n",
            "uniter proposes a word-region alignment objective based on opti-\n",
            "mal transports, which calculates the minimum cost of transporting\n",
            "the contextualized image embeddings to word embeddings [ 5].\n",
            "vilt extends this objective to patch-level image embeddings [ 22].\n",
            "unlike natural images, document images imply an explicit fine-\n",
            "grained alignment relationship between text words and image ar-\n",
            "eas. using this relationship, udoc uses contrastive learning and\n",
            "similarity distillation to align the image and text belonging to the\n",
            "same area [ 14]. layoutlmv2 covers some text lines in raw images\n",
            "and predicts whether each text token is covered [ 56]. in contrast,\n",
            "we naturally utilize the masking operations in mim to construct\n",
            "aligned/unaligned pairs in an effective and unified way.\n",
            "5 conclusion and future work\n",
            "in this paper, we present layoutlmv3 to pre-train the multimodal\n",
            "transformer for document ai, which redesigns the model archi-\n",
            "tecture and pre-training objectives for layoutlm. distinguishing\n",
            "from the existing multimodal model in document ai, layoutlmv3\n",
            "does not rely on a pre-trained cnn or faster r-cnn backbone to\n",
            "extract visual features, significantly saving parameters and elimi-\n",
            "nating region annotations. we use unified text and image masking\n",
            "pre-training objectives: masked language modeling, masked image\n",
            "modeling, and word-patch alignment, to learn multimodal repre-\n",
            "sentations. extensive experimental results have demonstrated the\n",
            "generality and superiority of layoutlmv3 for both text-centric and\n",
            "image-centric document ai tasks with the simple architecture and\n",
            "unified objectives. in future research, we will investigate scaling up\n",
            "pre-trained models so that the models can leverage more training\n",
            "data to drive sota results further. in addition, we will explore few-\n",
            "shot and zero-shot learning capabilities to facilitate more real-world\n",
            "business scenarios in the document ai industry.\n",
            "6 acknowledgement\n",
            "we are grateful to yiheng xu for fruitful discussions and inspiration.\n",
            "this work was supported by the nsfc (u1811461) and the program\n",
            "for guangdong introducing innovative and entrepreneurial teams\n",
            "under grant no.2016zt06d211.table 4: visual information extraction in chinese f1 score on the ephoie test set.\n",
            "model subject test time name school #examination #seat class #student grade score mean\n",
            "bilstm+crf [24] 98.51 100.0 98.87 98.80 75.86 72.73 94.04 84.44 98.18 69.57 89.10\n",
            "gcn-based [35] 98.18 100.0 99.52 100.0 88.17 86.00 97.39 80.00 94.44 81.82 92.55\n",
            "graphie [42] 94.00 100.0 95.84 97.06 82.19 84.44 93.07 85.33 94.44 76.19 90.26\n",
            "trie [58] 98.79 100.0 99.46 99.64 88.64 85.92 97.94 84.32 97.02 80.39 93.21\n",
            "vies [51] 99.39 100.0 99.67 99.28 91.81 88.73 99.29 89.47 98.35 86.27 95.23\n",
            "structext [32] 99.25 100.0 99.47 99.83 97.98 95.43 98.29 97.33 99.25 93.73 97.95\n",
            "layoutlmv3-chinesebase (ours) 98.99 100.0 99.77 99.20 100.0 100.0 98.82 99.78 98.31 97.27 99.21\n",
            "a appendix\n",
            "a.1 layoutlmv3 in chinese\n",
            "pre-training layoutlmv3 in chinese. to demonstrate the effec-\n",
            "tiveness of layoutlmv3 in not only english but also in the chinese\n",
            "language, we pre-train a layoutlmv3-chinese model in base size.\n",
            "it is trained on 50 million document pages in chinese. we collect\n",
            "large-scale chinese documents by downloading publicly available\n",
            "digital-born documents and following the principles of common\n",
            "crawl (https://commoncrawl.org/) to process these documents. for\n",
            "the multimodal transformer encoder along with the text embed-\n",
            "ding layer, layoutlmv3-chinese is initialized from the pre-trained\n",
            "weights of xlm-r [ 7]. we randomly initialized the rest model pa-\n",
            "rameters. other training setting is the same as layoutlmv3.\n",
            "fine-tuning on visual information extraction. the visual in-\n",
            "formation extraction (vie) requires extracting key information from\n",
            "document images. the task is a sequence labeling problem aiming\n",
            "to tag each word with a pre-defined label. we predict the label of\n",
            "the last hidden state of each text token with a linear layer.\n",
            "we conduct experiments on the ephoie dataset. the ephoie [51]\n",
            "is a visual information extraction dataset consisting of examina-\n",
            "tion paper heads with diverse layouts and backgrounds. it contains\n",
            "1,494 images with comprehensive annotations for 15,771 chinese\n",
            "text instances. we focus on a token-level entity labeling task on\n",
            "the ephoie dataset to assign each character a label among ten\n",
            "pre-defined categories. the training and test sets contain 1,183 and\n",
            "311 images, respectively. we fine-tune layoutlmv3-chinese for\n",
            "100 epochs. the batch size is 16, and the learning rate is 5𝑒−5with\n",
            "linear warmup over the first epoch.\n",
            "we report f1 scores for this task and report results in table 4. the\n",
            "layoutlmv3-chinese shows superior performance on most metrics\n",
            "and achieves a sota mean f1 score of 99.21%. the results show\n",
            "that layoutlmv3 significantly benefits the vie task in chinese.\n",
            "references\n",
            "[1]alaaeldin ali, hugo touvron, mathilde caron, piotr bojanowski, matthijs douze,\n",
            "armand joulin, ivan laptev, natalia neverova, gabriel synnaeve, jakob verbeek,\n",
            "et al. 2021. xcit: cross-covariance image transformers. in neurips .\n",
            "[2]srikar appalaraju, bhavan jasani, bhargava urala kota, yusheng xie, and r. man-\n",
            "matha. 2021. docformer: end-to-end transformer for document understanding.\n",
            "iniccv .\n",
            "[3]hangbo bao, li dong, songhao piao, and furu wei. 2022. beit: bert pre-\n",
            "training of image transformers. in iclr .\n",
            "[4]zhaowei cai and nuno vasconcelos. 2018. cascade r-cnn: delving into high\n",
            "quality object detection. in cvpr .\n",
            "[5]yen-chun chen, linjie li, licheng yu, ahmed el kholy, faisal ahmed, zhe gan,\n",
            "yu cheng, and jingjing liu. 2020. uniter: universal image-text representation\n",
            "learning. in eccv .[6]jaemin cho, jiasen lu, dustin schwenk, hannaneh hajishirzi, and aniruddha\n",
            "kembhavi. 2020. x-lxmert: paint, caption and answer questions with multi-\n",
            "modal transformers. in emnlp .\n",
            "[7]alexis conneau, kartikay khandelwal, naman goyal, vishrav chaudhary, guil-\n",
            "laume wenzek, francisco guzmán, édouard grave, myle ott, luke zettlemoyer,\n",
            "and veselin stoyanov. 2020. unsupervised cross-lingual representation learning\n",
            "at scale. in acl.\n",
            "[8]lei cui, yiheng xu, tengchao lv, and furu wei. 2021. document ai: benchmarks,\n",
            "models and applications. arxiv preprint arxiv:2111.08609 (2021).\n",
            "[9]jacob devlin, ming-wei chang, kenton lee, and kristina toutanova. 2019. bert:\n",
            "pre-training of deep bidirectional transformers for language understanding. in\n",
            "naacl .\n",
            "[10] ming ding, zhuoyi yang, wenyi hong, wendi zheng, chang zhou, da yin,\n",
            "junyang lin, xu zou, zhou shao, hongxia yang, et al .2021. cogview: mastering\n",
            "text-to-image generation via transformers. in neurips .\n",
            "[11] alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xi-\n",
            "aohua zhai, thomas unterthiner, mostafa dehghani, matthias minderer, georg\n",
            "heigold, sylvain gelly, jakob uszkoreit, and neil houlsby. 2021. an image is\n",
            "worth 16x16 words: transformers for image recognition at scale. in iclr .\n",
            "[12] zi-yi dou, yichong xu, zhe gan, jianfeng wang, shuohang wang, lijuan wang,\n",
            "chenguang zhu, zicheng liu, michael zeng, et al .2021. an empirical study\n",
            "of training end-to-end vision-and-language transformers. arxiv preprint\n",
            "arxiv:2111.02387 (2021).\n",
            "[13] łukasz garncarek, rafał powalski, tomasz stanisławek, bartosz topolski, piotr\n",
            "halama, michał turski, and filip graliński. 2021. lambert: layout-aware\n",
            "language modeling for information extraction. in icdar .\n",
            "[14] jiuxiang gu, jason kuen, vlad morariu, handong zhao, rajiv jain, nikolaos\n",
            "barmpalios, ani nenkova, and tong sun. 2021. unidoc: unified pretraining\n",
            "framework for document understanding. in neurips .\n",
            "[15] zhangxuan gu, changhua meng, ke wang, jun lan, weiqiang wang, ming\n",
            "gu, and liqing zhang. 2022. xylayoutlm: towards layout-aware multimodal\n",
            "networks for visually-rich document understanding. in cvpr .\n",
            "[16] adam w harley, alex ufkes, and konstantinos g derpanis. 2015. evaluation of\n",
            "deep convolutional nets for document image classification and retrieval. in\n",
            "icdar .\n",
            "[17] teakgyu hong, donghyun kim, mingi ji, wonseok hwang, daehyun nam, and\n",
            "sungrae park. 2022. bros: a pre-trained language model focusing on text and\n",
            "layout for better key information extraction from documents. in aaai .\n",
            "[18] yupan huang, hongwei xue, bei liu, and yutong lu. 2021. unifying multimodal\n",
            "transformer for bi-directional image and text generation. in acm multimedia .\n",
            "[19] zhicheng huang, zhaoyang zeng, yupan huang, bei liu, dongmei fu, and\n",
            "jianlong fu. 2021. seeing out of the box: end-to-end pre-training for vision-\n",
            "language representation learning. in cvpr .\n",
            "[20] guillaume jaume, hazim kemal ekenel, and jean-philippe thiran. 2019. funsd:\n",
            "a dataset for form understanding in noisy scanned documents. in icdarw .\n",
            "[21] mandar joshi, danqi chen, yinhan liu, daniel s weld, luke zettlemoyer, and\n",
            "omer levy. 2020. spanbert: improving pre-training by representing and predict-\n",
            "ing spans. transactions of the association for computational linguistics 8 (2020),\n",
            "64–77.\n",
            "[22] wonjae kim, bokyung son, and ildoo kim. 2021. vilt: vision-and-language\n",
            "transformer without convolution or region supervision. in icml .\n",
            "[23] diederik p kingma and jimmy ba. 2014. adam: a method for stochastic opti-\n",
            "mization. arxiv preprint arxiv:1412.6980 (2014).\n",
            "[24] guillaume lample, miguel ballesteros, sandeep subramanian, kazuya kawakami,\n",
            "and chris dyer. 2016. neural architectures for named entity recognition. in\n",
            "naacl hlt .\n",
            "[25] chen-yu lee, chun-liang li, timothy dozat, vincent perot, guolong su, nan\n",
            "hua, joshua ainslie, renshen wang, yasuhisa fujii, and tomas pfister. 2022.\n",
            "formnet: structural encoding beyond sequential modeling in form document\n",
            "information extraction. in acl.\n",
            "[26] d. lewis, g. agam, s. argamon, o. frieder, d. grossman, and j. heard. 2006.\n",
            "building a test collection for complex document information processing. in\n",
            "sigir .[27] mike lewis, yinhan liu, naman goyal, marjan ghazvininejad, abdelrahman\n",
            "mohamed, omer levy, veselin stoyanov, and luke zettlemoyer. 2020. bart:\n",
            "denoising sequence-to-sequence pre-training for natural language generation,\n",
            "translation, and comprehension. in acl.\n",
            "[28] chenliang li, bin bi, ming yan, wei wang, songfang huang, fei huang, and luo\n",
            "si. 2021. structurallm: structural pre-training for form understanding. in acl.\n",
            "[29] junnan li, ramprasaath selvaraju, akhilesh gotmare, shafiq joty, caiming xiong,\n",
            "and steven chu hong hoi. 2021. align before fuse: vision and language repre-\n",
            "sentation learning with momentum distillation. in neurips .\n",
            "[30] junlong li, yiheng xu, tengchao lv, lei cui, cha zhang, and furu wei. 2022. dit:\n",
            "self-supervised pre-training for document image transformer. arxiv preprint\n",
            "arxiv:2203.02378 (2022).\n",
            "[31] peizhao li, jiuxiang gu, jason kuen, vlad i morariu, handong zhao, rajiv jain,\n",
            "varun manjunatha, and hongfu liu. 2021. selfdoc: self-supervised document\n",
            "representation learning. in cvpr .\n",
            "[32] yulin li, yuxi qian, yuechen yu, xiameng qin, chengquan zhang, yan liu, kun\n",
            "yao, junyu han, jingtuo liu, and errui ding. 2021. structext: structured text\n",
            "understanding with multi-modal transformers. in acm multimedia .\n",
            "[33] yanghao li, saining xie, xinlei chen, piotr dollar, kaiming he, and ross girshick.\n",
            "2021. benchmarking detection transfer learning with vision transformers. arxiv\n",
            "preprint arxiv:2111.11429 (2021).\n",
            "[34] tsung-yi lin, piotr dollár, ross girshick, kaiming he, bharath hariharan, and\n",
            "serge belongie. 2017. feature pyramid networks for object detection. in cvpr .\n",
            "[35] xiaojing liu, feiyu gao, qiong zhang, and huasha zhao. 2019. graph convolu-\n",
            "tion for multimodal information extraction from visually rich documents. in\n",
            "naacl hlt .\n",
            "[36] yinhan liu, myle ott, naman goyal, jingfei du, mandar joshi, danqi chen, omer\n",
            "levy, mike lewis, luke zettlemoyer, and veselin stoyanov. 2019. roberta: a\n",
            "robustly optimized bert pretraining approach. arxiv preprint arxiv:1907.11692\n",
            "(2019).\n",
            "[37] jiasen lu, dhruv batra, devi parikh, and stefan lee. 2019. vilbert: pretraining\n",
            "task-agnostic visiolinguistic representations for vision-and-language tasks. in\n",
            "neurips .\n",
            "[38] minesh mathew, dimosthenis karatzas, and cv jawahar. 2021. docvqa: a dataset\n",
            "for vqa on document images. in wacv .\n",
            "[39] seunghyun park, seung shin, bado lee, junyeop lee, jaeheung surh, minjoon\n",
            "seo, and hwalsuk lee. 2019. cord: a consolidated receipt dataset for post-\n",
            "ocr parsing. in document intelligence workshop at neural information processing\n",
            "systems .\n",
            "[40] rafal powalski, łukasz borchmann, dawid jurkiewicz, tomasz dwojak, michal\n",
            "pietruszka, and gabriela pałka. 2021. going full-tilt boogie on document\n",
            "understanding with text-image-layout transformer. in icdar .\n",
            "[41] subhojeet pramanik, shashank mujumdar, and hima patel. 2020. towards a\n",
            "multi-modal, multi-task learning based pre-training framework for document\n",
            "representation learning. arxiv preprint arxiv:2009.14457 (2020).\n",
            "[42] yujie qian. 2019. a graph-based framework for information extraction . ph. d.\n",
            "dissertation. massachusetts institute of technology.[43] aditya ramesh, mikhail pavlov, gabriel goh, scott gray, chelsea voss, alec\n",
            "radford, mark chen, and ilya sutskever. 2021. zero-shot text-to-image generation.\n",
            "inicml .\n",
            "[44] shaoqing ren, kaiming he, ross b. girshick, and jian sun. 2015. faster r-cnn:\n",
            "towards real-time object detection with region proposal networks. tpami 39,\n",
            "1137–1149.\n",
            "[45] tim salimans, andrej karpathy, xi chen, and diederik p. kingma. 2017. pixel-\n",
            "cnn++: improving the pixelcnn with discretized logistic mixture likelihood\n",
            "and other modifications. in iclr .\n",
            "[46] rico sennrich, barry haddow, and alexandra birch. 2016. neural machine\n",
            "translation of rare words with subword units. in acl.\n",
            "[47] weijie su, xizhou zhu, yue cao, bin li, lewei lu, furu wei, and jifeng dai. 2019.\n",
            "vl-bert: pre-training of generic visual-linguistic representations. in iclr .\n",
            "[48] hao tan and mohit bansal. 2019. lxmert: learning cross-modality encoder\n",
            "representations from transformers. in emnlp .\n",
            "[49] ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones,\n",
            "aidan n gomez, łukasz kaiser, and illia polosukhin. 2017. attention is all\n",
            "you need. in neurips .\n",
            "[50] jiapeng wang, lianwen jin, and kai ding. 2022. lilt: a simple yet effective\n",
            "language-independent layout transformer for structured document under-\n",
            "standing. in acl.\n",
            "[51] jiapeng wang, chongyu liu, lianwen jin, guozhi tang, jiaxin zhang, shuaitao\n",
            "zhang, qianying wang, yaqiang wu, and mingxiang cai. 2021. towards robust\n",
            "visual information extraction in real world: new dataset and novel solution. in\n",
            "aaai .\n",
            "[52] te-lin wu, cheng li, mingyang zhang, tao chen, spurthi amba hombaiah, and\n",
            "michael bendersky. 2021. lampret: layout-aware multimodal pretraining for\n",
            "document understanding. arxiv preprint arxiv:2104.08405 (2021).\n",
            "[53] yuxin wu, alexander kirillov, francisco massa, wan-yen lo, and ross girshick.\n",
            "2019. detectron2. https://github.com/facebookresearch/detectron2.\n",
            "[54] yiheng xu, minghao li, lei cui, shaohan huang, furu wei, and ming zhou. 2020.\n",
            "layoutlm: pre-training of text and layout for document image understanding. in\n",
            "kdd .\n",
            "[55] yiheng xu, tengchao lv, lei cui, guoxin wang, yijuan lu, dinei florencio, cha\n",
            "zhang, and furu wei. 2021. layoutxlm: multimodal pre-training for multilingual\n",
            "visually-rich document understanding. arxiv preprint arxiv:2104.08836 (2021).\n",
            "[56] yang xu, yiheng xu, tengchao lv, lei cui, furu wei, guoxin wang, yijuan lu,\n",
            "dinei florencio, cha zhang, wanxiang che, min zhang, and lidong zhou. 2021.\n",
            "layoutlmv2: multi-modal pre-training for visually-rich document understand-\n",
            "ing. in acl.\n",
            "[57] hongwei xue, yupan huang, bei liu, houwen peng, jianlong fu, houqiang li,\n",
            "and jiebo luo. 2021. probing inter-modality: visual parsing with self-attention\n",
            "for vision-and-language pre-training. in neurips .\n",
            "[58] peng zhang, yunlu xu, zhanzhan cheng, shiliang pu, jing lu, liang qiao, yi\n",
            "niu, and fei wu. 2020. trie: end-to-end text reading and information extraction\n",
            "for document understanding. in acm multimedia .\n",
            "[59] xu zhong, jianbin tang, and antonio jimeno yepes. 2019. publaynet: largest\n",
            "dataset ever for document layout analysis. in icdar .\n",
            "ocr-free document understanding transformer\n",
            "geewook kim1∗, teakgyu hong4†, moonbin yim2†, jeongyeon nam1,\n",
            "jinyoung park5†, jinyeong yim6†, wonseok hwang7†, sangdoo yun3,\n",
            "dongyoon han3, and seunghyun park1\n",
            "1naver clova2naver search3naver ai lab\n",
            "4upstage5tmax6google7lbox\n",
            "abstract. understanding document images ( e.g., invoices) is a core but\n",
            "challenging task since it requires complex functions such as reading text\n",
            "and a holistic understanding of the document . current visual document\n",
            "understanding (vdu) methods outsource the task of reading text to off-\n",
            "the-shelf optical character recognition (ocr) engines and focus on the\n",
            "understanding task with the ocr outputs. although such ocr-based\n",
            "approaches have shown promising performance, they suffer from 1) high\n",
            "computational costs for using ocr; 2) inflexibility of ocr models on\n",
            "languages or types of documents; 3) ocr error propagation to the sub-\n",
            "sequent process. to address these issues, in this paper, we introduce a\n",
            "novel ocr-free vdu model named donut , which stands for document\n",
            "understanding transformer. as the first step in ocr-free vdu research,\n",
            "we propose a simple architecture ( i.e., transformer) with a pre-training\n",
            "objective ( i.e.,cross-entropy loss). donut is conceptually simple yet ef-\n",
            "fective. through extensive experiments and analyses, we show a simple\n",
            "ocr-free vdu model, donut, achieves state-of-the-art performances on\n",
            "various vdu tasks in terms of both speed and accuracy. in addition, we\n",
            "offer a synthetic data generator that helps the model pre-training to be\n",
            "flexible in various languages and domains. the code, trained model, and\n",
            "synthetic data are available at https://github.com/clovaai/donut .\n",
            "keywords: visual document understanding, document information\n",
            "extraction, optical character recognition, end-to-end transformer\n",
            "1 introduction\n",
            "document images, such as commercial invoices, receipts, and business cards,\n",
            "are easy to find in modern working environments. to extract useful informa-\n",
            "tion from such document images, visual document understanding (vdu) has\n",
            "not been only an essential task for industry, but also a challenging topic for re-\n",
            "searchers, with applications including document classification [27,1], information\n",
            "extraction [22,42], and visual question answering [44,57].\n",
            "∗corresponding author: gwkim.rsrch@gmail.com\n",
            "†this work was done while the authors were at naver clova.arxiv:2111.15664v5  [cs.lg]  6 oct 20222 g. kim et al.\n",
            "document image{ \"items\": [     {      \"name\": \"3002-kyoto choco mochi\",      \"count\": 2,      \"priceinfo\": {        \"unitprice\": 14000,        \"price\": 28000      }    }, ...  ],  \"total\": [ {       \"menuqty_cnt\": 4,       \"total_price\": 50000    }   ]}{ \"words\": [        {            \"bbox\":[[0.11,0.21],...,[0.19,0.22]],            \"text\": \"3002-kyoto\"        }, {            \"bbox\":[[0.21,0.22],...,[0.45,0.23]],            \"text\": \"choco\"        }, {            \"bbox\":[[0.46,0.22],...,[0.52,0.23]],            \"text\": \"mochi\"        }, …, {            \"bbox\":[[0.66,0.31],...,[0.72,0.32]],            \"text\": \"50.000\"        }    ]}structured information(a)(b)(c)(d)\n",
            "fig. 1. the schema of the conventional document information extraction\n",
            "(ie) pipeline. (a) the goal is to extract the structured information from a given semi-\n",
            "structured document image. in the pipeline, (b) text detection is conducted to obtain\n",
            "text locations and (c) each box is passed to the recognizer to comprehend characters.\n",
            "(d) finally, the recognized texts and its locations are passed to the following module\n",
            "to be processed for the desired structured form of the information\n",
            "imageocrdownstream modeloutputas-is (ocr + bert, layout lm, …)\n",
            "imagee2e modeloutputdonut 🍩\n",
            "(proposed)1.20.9time (sec/img)9182accuracy (%)0.8\n",
            "~0.6 sec143179+amemory (m)\n",
            "(a) pipeline overview. (b) system benchmarks.\n",
            "fig. 2. the pipeline overview and benchmarks. the proposed end-to-end model,\n",
            "donut , outperforms the recent ocr-dependent vdu models in memory, time cost\n",
            "and accuracy. performances on visual document ie [45] are shown in (b). more results\n",
            "on various vdu tasks are available at section 3 showing the same trend\n",
            "current vdu methods [22,24,65,64,18] solve the task in a two-stage manner:\n",
            "1) reading the texts in the document image; 2) holistic understanding of the doc-\n",
            "ument. they usually rely on deep-learning-based optical character recognition\n",
            "(ocr) [4,3] for the text reading task and focus on modeling the understanding\n",
            "part. for example, as shown in figure 1, a conventional pipeline for extracting\n",
            "structured information from documents (a.k.a. document parsing) consists of\n",
            "three separate modules for text detection, text recognition, and parsing [22,24].\n",
            "however, the ocr-dependent approach has critical problems. first of all, us-\n",
            "ing ocr as a pre-processing method is expensive. we can utilize pre-trained off-\n",
            "the-shelf ocr engines; however, the computational cost for inference would be\n",
            "expensive for high-quality ocr results. moreover, the off-the-shelf ocr meth-\n",
            "ods rarely have flexibility dealing with different languages or domain changes,\n",
            "which may lead to poor generalization ability. if we train an ocr model, it also\n",
            "requires extensive training costs and large-scale datasets [4,3,39,46]. another\n",
            "problem is, ocr errors would propagate to the vdu system and negatively\n",
            "influence subsequent processes [54,23]. this problem becomes more severe inocr-free document understanding transformer 3\n",
            "languages with complex character sets, such as korean or chinese, where the\n",
            "quality of ocr is relatively low [50]. to deal with this, post-ocr correction\n",
            "module [51,50,10] is usually adopted. however, it is not a practical solution for\n",
            "real application environments since it increases the entire system size and main-\n",
            "tenance cost.\n",
            "we go beyond the traditional framework by modeling a direct mapping from\n",
            "a raw input image to the desired output without ocr. we introduce a new\n",
            "ocr-free vdu model to address the problems induced by the ocr-dependency.\n",
            "our model is based on transformer-only architecture, referred to as document\n",
            "understanding transformer ( donut ), following the huge success in vision and\n",
            "language [8,9,29]. we present a minimal baseline including a simple architecture\n",
            "and pre-training method. despite its simplicity, donut shows comparable or\n",
            "better overall performance than previous methods as shown in figure 2.\n",
            "we take pre-train-and-fine-tune scheme [8,65] on donut training. in the\n",
            "pre-training phase, donut learns how to read the texts by predicting the next\n",
            "words by conditioning jointly on the image and previous text contexts. donut\n",
            "is pre-trained with document images and their text annotations. since our pre-\n",
            "training objective is simple ( i.e., reading the texts), we can realize domain and\n",
            "language flexibility straightforwardly pre-training with synthetic data. during\n",
            "fine-tuning stage, donut learns how to understand the whole document accord-\n",
            "ing to the downstream task. we demonstrate donut has a strong understanding\n",
            "ability through extensive evaluation on various vdu tasks and datasets. the\n",
            "experiments show a simple ocr-free vdu model can achieve state-of-the-art\n",
            "performance in terms of both speed and accuracy.\n",
            "the contributions are summarized as follows:\n",
            "1. we propose a novel ocr-free approach for vdu. to the best of our knowl-\n",
            "edge, this is the first method based on an ocr-free transformer trained in\n",
            "end-to-end manner.\n",
            "2. we introduce a simple pre-training scheme that enables the utilization of\n",
            "synthetic data. by using our generator synthdog, we show donut can\n",
            "easily be extended to a multi-lingual setting, which is not applicable for the\n",
            "conventional approaches that need to retrain an off-the-shelf ocr engine.\n",
            "3. we conduct extensive experiments and analyses on both public benchmarks\n",
            "and private industrial datasets, showing that the proposed method achieves\n",
            "not only state-of-the-art performances on benchmarks but also has many\n",
            "practical advantages (e.g., cost-effective ) in real-world applications.\n",
            "4. the codebase, pre-trained model, and synthetic data are available at github.1\n",
            "2 method\n",
            "2.1 preliminary: background\n",
            "there have been various visual document understanding (vdu) methods to un-\n",
            "derstand and extract essential information from the semi-structured documents\n",
            "such as receipts [20,25,18], invoices [49], and form documents [14,6,43].\n",
            "1https://github.com/clovaai/donut .4 g. kim et al.\n",
            "<vqa><question>what is the price of choco mochi?</question><answer>converted jsontransformer encoder\n",
            "input image and prompt\n",
            "transformer decoderdonut 🍩\n",
            "<classification><parsing><class>receipt</class></classification>14,000</answer></vqa><item><name>3002-kyoto choco mochi</name>・・・ </parsing>{ \"items\": [{\"name\": \"3002-kyoto choco mochi\",      \"count\": 2,      \"unitprice\": 14000, …}], … }output sequence{ \"class\":\"receipt\" }{ \"question\": \"what is the price of choco mochi?\",   \"answer\": \"14,000\" }\n",
            "fig. 3. the pipeline of donut. the encoder maps a given document image into\n",
            "embeddings. with the encoded embeddings, the decoder generates a sequence of tokens\n",
            "that can be converted into a target type of information in a structured form\n",
            "earlier vdu attempts have been done with ocr-independent visual back-\n",
            "bones [27,1,15,12,31], but the performances are limited. later, with the remark-\n",
            "able advances of ocr [4,3] and bert [8], various ocr-dependent vdu models\n",
            "have been proposed by combining them [22,24,23]. more recently, in order to get\n",
            "a more general vdu, most state-of-the-arts [64,18] use both powerful ocr en-\n",
            "gines and large-scale real document image data (e.g., iit-cdip [32]) for a model\n",
            "pre-training. although they showed remarkable advances in recent years, extra\n",
            "effort is required to ensure the performance of an entire vdu model by using\n",
            "the off-the-shelf ocr engine.\n",
            "2.2 document understanding transformer\n",
            "donut is an end-to-end (i.e., self-contained) vdu model for general understand-\n",
            "ing of document images. the architecture of donut is quite simple, which con-\n",
            "sists of a transformer [58,9]-based visual encoder and textual decoder modules.\n",
            "note that donut does not rely on any modules related to ocr functionality\n",
            "but uses a visual encoder for extracting features from a given document im-\n",
            "age. the following textual decoder maps the derived features into a sequence\n",
            "of subword tokens to construct a desired structured format (e.g., json). each\n",
            "model component is transformer-based, and thus the model is trained easily in\n",
            "an end-to-end manner. the overall process of donut is illustrated in figure 3.\n",
            "encoder. the visual encoder converts the input document image x∈rh×w×c\n",
            "into a set of embeddings {zi|zi∈rd,1≤i≤n}, where nis feature map size or the\n",
            "number of image patches and dis the dimension of the latent vectors of the\n",
            "encoder. note that cnn-based models [17] or transformer-based models [9,40]\n",
            "can be used as the encoder network. in this study, we use swin transformer [40]\n",
            "because it shows the best performance in our preliminary study in document\n",
            "parsing. swin transformer first splits the input image xinto non-overlapping\n",
            "patches. swin transformer blocks, consist of a shifted window-based multi-head\n",
            "self-attention module and a two-layer mlp, are applied to the patches. then,\n",
            "patch merging layers are applied to the patch tokens at each stage. the output\n",
            "of the final swin transformer block {z}is fed into the following textual decoder.ocr-free document understanding transformer 5\n",
            "decoder. given the {z}, the textual decoder generates a token sequence ( yi)m\n",
            "i=1,\n",
            "where yi∈rvis an one-hot vector for the i-th token, vis the size of token vo-\n",
            "cabulary, and mis a hyperparameter, respectively. we use bart [33] as the\n",
            "decoder architecture. specifically, we initialize the decoder model weights with\n",
            "those from the publicly available2pre-trained multi-lingual bart model[38].\n",
            "model input. following the original transformer [58], we use a teacher-forcing\n",
            "scheme [62], which is a model training strategy that uses the ground truth as\n",
            "input instead of model output from a previous time step. in the test phase,\n",
            "inspired by gpt-3 [5], the model generates a token sequence given a prompt.\n",
            "we add new special tokens for the prompt for each downstream task in our\n",
            "experiments. the prompts that we use for our applications are shown with the\n",
            "desired output sequences in figure 3. illustrative explanations for the teacher-\n",
            "forcing strategy and the decoder output format are available in appendix a.4.\n",
            "output conversion. the output token sequence is converted to a desired\n",
            "structured format. we adopt a json format due to its high representation\n",
            "capacity. as shown in figure 3, a token sequence is one-to-one invertible to a\n",
            "json data. we simply add two special tokens [start ∗]and [end ∗], where ∗\n",
            "indicates each field to extract. if the output token sequence is wrongly structured,\n",
            "we simply treat the field is lost. for example, if there is only [start name] exists\n",
            "but no [end name] , we assume the model fails to extract “name” field. this\n",
            "algorithm can easily be implemented with simple regular expressions [11].\n",
            "2.3 pre-training\n",
            "task. the model is trained to read all texts in the image in reading order (from\n",
            "top-left to bottom-right, basically). the objective is to minimize cross-entropy\n",
            "loss of next token prediction by jointly conditioning on the image and previous\n",
            "contexts. this task can be interpreted as a pseudo-ocr task. the model is\n",
            "trained as a visual language model over the visual corpora, i.e., document images.\n",
            "visual corpora. we use iit-cdip [32], which is a set of 11m scanned english\n",
            "document images. a commercial clova ocr api is applied to get the pseudo\n",
            "text labels. as aforementioned, however, this kind of dataset is not always avail-\n",
            "able, especially for languages other than english. to alleviate the dependencies,\n",
            "we build a scalable synth eticdocument generator , referred to as synthdog .\n",
            "using the synthdog and chinese, japanese, korean and english wikipedia, we\n",
            "generated 0.5m samples per language.\n",
            "synthetic document generator. the pipeline of image rendering basically\n",
            "follows yim et al. [67]. as shown in figure 4, the generated sample consists of\n",
            "2https://huggingface.co/hyunwoongko/asian-bart-ecjk .6 g. kim et al.\n",
            "fig. 4. generated english, chinese, japanese, and korean samples with\n",
            "synthdog. heuristic random patterns are applied to mimic the real documents\n",
            "several components; background, document, text, and layout. background image\n",
            "is sampled from imagenet [7], and a texture of document is sampled from the\n",
            "collected paper photos. words and phrases are sampled from wikipedia. layout\n",
            "is generated by a simple rule-based algorithm that randomly stacks grids. in\n",
            "addition, several image rendering techniques [13,41,67] are applied to mimic\n",
            "real documents. the generated examples are shown in figure 4. more details of\n",
            "synthdog are available in the code1and appendix a.2.\n",
            "2.4 fine-tuning\n",
            "after the model learns how to read , in the application stage (i.e., fine-tuning), we\n",
            "teach the model how to understand the document image. as shown in figure 3,\n",
            "we interpret all downstream tasks as a json prediction problem.\n",
            "the decoder is trained to generate a token sequence that can be converted\n",
            "into a json that represents the desired output information. for example, in the\n",
            "document classification task, the decoder is trained to generate a token sequence\n",
            "[start class][memo][end class] which is 1-to-1 invertible to a json {“class”:\n",
            "“memo” }. we introduce some special tokens (e.g., [memo] is used for representing\n",
            "the class “memo”), if such replacement is available in the target task.\n",
            "3 experiments and analyses\n",
            "in this section, we present donut fine-tuning results on three vdu applications\n",
            "on six different datasets including both public benchmarks and private industrial\n",
            "service datasets. the samples are shown in figure 5.\n",
            "3.1 downstream tasks and datasets\n",
            "document classification. to see whether the model can distinguish across\n",
            "different types of documents, we test a classification task. unlike other models\n",
            "that predict the class label via a softmax on the encoded embedding, donut\n",
            "generate a json that contains class information to maintain the uniformity of\n",
            "the task-solving method. we report overall classification accuracy on a test set.\n",
            "rvl-cdip. the rvl-cdip dataset [16] consists of 400k images in 16 classes,\n",
            "with 25k images per class. the classes include letter, memo, email, and so on.\n",
            "there are 320k training, 40k validation, and 40k test images.ocr-free document understanding transformer 7\n",
            "form\n",
            "handwritten(a)(b)\n",
            "(c)\n",
            "q: what is the extension number as per the voucher? a: (910) 741-0673\n",
            "fig. 5. samples of the downstream datasets. (a) document classification. (b)\n",
            "document information extraction. (c) document visual question answering\n",
            "document information extraction. to see the model fully understands\n",
            "the complex layouts and contexts in documents, we test document information\n",
            "extraction (ie) tasks on various real document images including both public\n",
            "benchmarks and real industrial datasets. in this task, the model aims to map\n",
            "each document to a structured form of information that is consistent with the\n",
            "target ontology or database schema. see figure 1 for an illustrative example. the\n",
            "model should not only read the characters well, but also understand the layouts\n",
            "and semantics to infer the groups and nested hierarchies among the texts.\n",
            "we evaluate the models with two metrics; field-level f1 score [22,65,18] and\n",
            "tree edit distance (ted) based accuracy [68,70,23]. the f1 checks whether the\n",
            "extracted field information is in the ground truth. even if a single character is\n",
            "missed, the score assumes the field extraction is failed. although f1 is simple\n",
            "and easy to understand, there are some limitations. first, it does not take into\n",
            "account partial overlaps. second, it can not measure the predicted structure (e.g.,\n",
            "groups and nested hierarchy). to assess overall accuracy, we also use another\n",
            "metric based on ted [68], that can be used for any documents represented as\n",
            "trees. it is calculated as, max(0 ,1−ted(pr ,gt)/ted( ϕ,gt)), where gt, pr, and ϕ\n",
            "stands for ground truth, predicted, and empty trees respectively. similar metrics\n",
            "are used in recent works on document ie [70,23]\n",
            "we use two public benchmark datasets as well as two private industrial\n",
            "datasets which are from our active real-world service products. each dataset\n",
            "is explained in the followings.\n",
            "cord. the consolidated receipt dataset (cord)3[45] is a public benchmark\n",
            "that consists of 0.8k train, 0.1k valid, 0.1k test receipt images. the letters\n",
            "of receipts is in latin alphabet. the number of unique fields is 30 containing\n",
            "menu name, count, total price, and so on. there are complex structures (i.e.,\n",
            "3https://huggingface.co/datasets/naver-clova-ix/cord-v1 .8 g. kim et al.\n",
            "nested groups and hierarchies such as items>item> {name, count, price }) in the\n",
            "information. see figure 1 for more details.\n",
            "ticket. this is a public benchmark dataset [12] that consists of 1.5k train and\n",
            "0.4k test chinese train ticket images. we split 10% of the train set as a validation\n",
            "set. there are 8 fields which are ticket number, starting station, train number,\n",
            "and so on. the structure of information is simple and all keys are guaranteed to\n",
            "appear only once and the location of each field is fixed.\n",
            "business card (in-service data). this dataset is from our active products that\n",
            "are currently deployed. the dataset consists of 20k train, 0.3k valid, 0.3k test\n",
            "japanese business cards. the number of fields is 11, including name, company,\n",
            "address, and so on. the structure of information is similar to the ticket dataset.\n",
            "receipt (in-service data). this dataset is also from one of our real products.\n",
            "the dataset consists of 40k train, 1k valid, 1k test korean receipt images.\n",
            "the number of unique field is 81, which includes store information, payment\n",
            "information, price information, and so on. each sample has complex structures\n",
            "compared to the aforementioned datasets. due to industrial policies, not all\n",
            "samples can publicly be available. some real-like high-quality samples are shown\n",
            "in figure 5 and in the supplementary material.\n",
            "document visual question answering. to validate the further capacity of\n",
            "the model, we conduct a document visual question answering task (docvqa). in\n",
            "this task, a document image and question pair is given and the model predicts the\n",
            "answer for the question by capturing both visual and textual information within\n",
            "the image. we make the decoder generate the answer by setting the question as\n",
            "a starting prompt to keep the uniformity of the method (see figure 3).\n",
            "docvqa. the dataset is from document visual question answering competi-\n",
            "tion4and consists of 50k questions defined on more than 12k documents [44].\n",
            "there are 40k train, 5k valid, and 5k test questions. the evaluation metric is\n",
            "anls (average normalized levenshtein similarity) which is an edit-distance-\n",
            "based metric. the score on the test set is measured via the evaluation site.\n",
            "3.2 setups\n",
            "we use swin-b [40] as a visual encoder of donut with slight modification.\n",
            "we set the layer numbers and window size as {2,2,14,2}and 10. in further\n",
            "consideration of the speed-accuracy trade-off, we use the first four layers of bart\n",
            "as a decoder. as explained in section 2.3, we train the multi-lingual donut using\n",
            "the 2m synthetic and 11m iit-cdip scanned document images. we pre-train the\n",
            "model for 200k steps with 64 a100 gpus and a mini-batch size of 196. we use\n",
            "adam [30] optimizer, the learning rate is scheduled and the initial rate is selected\n",
            "4https://rrc.cvc.uab.es/?ch=17 .ocr-free document understanding transformer 9\n",
            "table 1. classification results on the rvl-cdip dataset. donut achieves\n",
            "state-of-the-are performance with reasonable speed and model size efficiency. donut\n",
            "is a general purpose backbone but does not rely on ocr while other recent backbones\n",
            "(e.g., layoutlm) do.†# parameters for ocr should be considered for non-e2e models\n",
            "ocr #params time (ms) accuracy (%)\n",
            "bert ✓110m + α†1392 89.81\n",
            "roberta ✓125m + α†1392 90.06\n",
            "layoutlm ✓113m + α†1396 91.78\n",
            "layoutlm (w/ image) ✓160m + α†1426 94.42\n",
            "layoutlmv2 ✓200m + α†1489 95.25\n",
            "donut (proposed) 143m 752 95.30\n",
            "from 1e-5 to 1e-4. the input resolution is set to 2560 ×1920 and a max length\n",
            "in the decoder is set to 1536. all fine-tuning results are achieved by starting\n",
            "from the pre-trained multi-lingual model. some hyperparameters are adjusted\n",
            "at fine-tuning and in ablation studies. we use 960 ×1280 for train tickets and\n",
            "business card parsing tasks. we fine-tune the model while monitoring the edit\n",
            "distance over token sequences. the speed of donut is measured on a p40 gpu,\n",
            "which is much slower than a100. for the ocr based baselines, states-of-the-art\n",
            "ocr engines are used, including ms ocr api used in [64] and clova ocr\n",
            "api5used in [24,23]. an analysis on ocr engines is available in section 3.4.\n",
            "more details of ocr and training setups are available in appendix a.1 and a.5.\n",
            "3.3 experimental results\n",
            "document classification. the results are shown in table 1. without re-\n",
            "lying on any other resource (e.g., off-the-shelf ocr engine), donut shows a\n",
            "state-of-the-art performance among the general-purpose vdu models such as\n",
            "layoutlm [65] and layoutlmv2 [64]. in particular, donut surpasses the lay-\n",
            "outlmv2 accuracy reported in [64], while using fewer parameters with the 2x\n",
            "faster speed. note that the ocr-based models must consider additional model\n",
            "parameters and speed for the entire ocr framework, which is not small in gen-\n",
            "eral. for example, a recent advanced ocr-based model [4,3] requires more than\n",
            "80m parameters. also, training and maintaining the ocr-based systems are\n",
            "costly [23], leading to needs for the donut-like end-to-end approach.\n",
            "document information extraction. table 2 shows the results on the four\n",
            "different document ie tasks. the first group uses a conventional bio-tagging-\n",
            "based ie approach [22]. we follows the conventions in ie [65,18]. ocr extracts\n",
            "texts and bounding boxes from the image, and then the serialization module sorts\n",
            "all texts with geometry information within the bounding box. the bio-tagging-\n",
            "based named entity recognition task performs token-level tag classification upon\n",
            "5https://clova.ai/ocr .10 g. kim et al.\n",
            "table 2. performances on various document ie tasks. the field-level f1 scores\n",
            "and tree-edit-distance-based accuracies are reported. donut shows the best accuracies\n",
            "for all domains with significantly faster inference speed.†parameters for vocabulary\n",
            "are omitted for fair comparisons among multi-lingual models.‡# parameters for ocr\n",
            "should be considered.∗official multi-lingual extension models are used\n",
            "cord [45] ticket [12] business card receipt\n",
            "ocr #params time (s) f1 acc. time (s) f1 acc. time (s) f1 acc. time (s) f1 acc.\n",
            "bert∗[22] ✓86†\n",
            "m+α‡1.6 73.0 65.5 1.7 74.3 82.4 1.5 40.8 72.1 2.5 70.3 54.1\n",
            "bros [18] ✓86†\n",
            "m+α‡1.7 74.7 70.0\n",
            "layoutlm [65] ✓89†\n",
            "m+α‡1.7 78.4 81.3\n",
            "layoutlmv2∗[64,66] ✓179†\n",
            "m+α‡1.7 78.9 82.4 1.8 87.2 90.1 1.6 52.2 83.0 2.6 72.9 78.0\n",
            "donut 143†\n",
            "m 1.2 84.1 90.9 0.6 94.1 98.7 1.4 57.8 84.4 1.9 78.6 88.6\n",
            "spade∗[25] ✓93†\n",
            "m+α‡4.0 74.0 75.8 4.5 14.9 29.4 4.3 32.3 51.3 7.3 64.1 53.2\n",
            "wyvern∗[21] ✓106†\n",
            "m+α‡1.2 43.3 46.9 1.5 41.8 54.8 1.7 29.9 51.5 3.4 71.5 82.9\n",
            "the ordered texts to generate a structured form. we test three general-purpose\n",
            "vdu backbones, bert [8], bros [18], layoutlm [65], and layoutlmv2 [64,66].\n",
            "we also test two recently proposed ie models, spade [24] and wyvern [23].\n",
            "spade is a graph-based ie method that predicts relations between bounding\n",
            "boxes. wyvern is an transformer encoder-decoder model that directly gen-\n",
            "erates entities with structure given ocr outputs. wyvern is different from\n",
            "donut in that it takes the ocr output as its inputs.\n",
            "for all domains, including public and private in-service datasets, donut shows\n",
            "the best scores among the comparing models. by measuring both f1 and ted-\n",
            "based accuracy, we observe not only donut can extract key information but\n",
            "also predict complex structures among the field information. we observe that\n",
            "a large input resolution gives robust accuracies but makes the model slower.\n",
            "for example, the performance on the cord with 1280 ×960 was 0.7 sec./image\n",
            "and 91.1 accuracy. but, the large resolution showed better performances on the\n",
            "low-resource situation. the detailed analyses are in section 3.4. unlike other\n",
            "baselines, donut shows stable performance regardless of the size of datasets and\n",
            "complexity of the tasks (see figure 5). this is a significant impact as the target\n",
            "tasks are already actively used in industries.\n",
            "document visual question answering. table 3 shows the results on the\n",
            "docvqa dataset. the first group is the general-purposed vdu backbones whose\n",
            "scores are from the layoutlmv2 paper [64]. we measure the running time with\n",
            "ms ocr api used in [64]. the model in the third group is a docvqa-specific-\n",
            "purposed fine-tuning model of layoutlmv2, whose inference results are available\n",
            "in the official leader-board.6\n",
            "as can be seen, donut achieves competitive scores with the baselines that\n",
            "are dependent on external ocr engines. especially, donut shows that it is robust\n",
            "to the handwritten documents, which is known to be challenging to process. in\n",
            "the conventional approach, adding a post-processing module that corrects ocr\n",
            "6https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1 .ocr-free document understanding transformer 11\n",
            "table 3. average normalized levenshtein similarity (anls) scores on\n",
            "docvqa. donut shows a promising result without ocr.∗donut shows a high\n",
            "anls score on the handwritten documents which are known to be challenging due to\n",
            "the difficulty of handwriting ocr (see figure 6).†token embeddings for english is\n",
            "counted for a fair comparison.‡# parameters for ocr should be considered\n",
            "fine-tuning set ocr #params†time (ms)anls\n",
            "test setanls∗\n",
            "handwritten\n",
            "bert [64] train set ✓110m + α‡1517 63.5 n/a\n",
            "layoutlm[65] train set ✓113m + α‡1519 69.8 n/a\n",
            "layoutlmv2[64] train set ✓200m + α‡1610 78.1 n/a\n",
            "donut train set 176m 782 67.5 72.1\n",
            "layoutlmv2-large-qg[64] train + dev + qg ✓390m + α‡1698 86.7 67.3\n",
            "q: what is the name of the passenger? \n",
            "answer: dr. william j. darby \n",
            "donut: dr. william j. darby \n",
            "layoutlmv2-large-qg: dr. william j. jarry q: what is the publication no.? \n",
            "answer: 540\n",
            "donut: 943  (another number in the image is extracted) \n",
            "layoutlmv2-large-qg: 540\n",
            "q: what is the phone number given? \n",
            "answer: 336-723-6100 \n",
            "donut: 336-723-6100 \n",
            "layoutlmv2-large-qg: 336-723- 4100 \n",
            "fig. 6. examples of donut and layoutlmv2 outputs on docvqa. the ocr-\n",
            "errors make a performance upper-bound of the ocr-dependent baselines, e.g., lay-\n",
            "outlmv2 (left and middle examples). due to the input resolution constraint of the\n",
            "end-to-end pipeline, donut miss some tiny texts in large-scale images (right example)\n",
            "but this could be mitigated by scaling the input image size (see section 3.4)\n",
            "errors is an option to strengthen the pipeline [51,50,10] or adopting an encoder-\n",
            "decoder architecture on the ocr outputs can mitigate the problems of ocr\n",
            "errors [23]. however, this kind of approaches tend to increase the entire system\n",
            "size and maintenance cost. donut shows a completely different direction. some\n",
            "inference results are shown in figure 6. the samples show the current strengths\n",
            "of donut as well as the left challenges in the donut-like end-to-end approach.\n",
            "further analysis and ablation is available in section 3.4.\n",
            "3.4 further studies\n",
            "in this section, we study several elements of understanding donut. we show some\n",
            "striking characteristics of donut through the experiments and visualization.\n",
            "on pre-training strategy. we test several pre-training tasks for vdus. fig-\n",
            "ure 7(a) shows that the donut pre-training task (i.e., text reading) is the most12 g. kim et al.\n",
            "no pretraining classification captioningread (synthdog)read (cdip) read (both) resnet-152 effnetv2vit-bswin-b 18 swin-b 14 640x640 960x960 1280x960 2560×1920\n",
            " 30507090accuracy\n",
            "020406080docvqa score(a) pretrain strategy                                          (b) backbone                                         (c) resolution\n",
            "cord (accuracy)\n",
            "docvqa score (anls)\n",
            "fig. 7. analysis on (a) pre-training strategies, (b) image backbones, and (c)\n",
            "input resolutions. performances on cord [45] and docvqa [44] are shown\n",
            "simple yet effective approach. other tasks that impose a general knowledge of\n",
            "images and texts on models, e.g., image captioning, show little gains in the\n",
            "fine-tuning tasks. for the text reading tasks, we verify three options, synthdog\n",
            "only, iit-cdip only, and both. note that synthetic images were enough for the\n",
            "document ie task in our analysis. however, in the docvqa task, it was impor-\n",
            "tant to see the real images. this is probably because the image distributions of\n",
            "iit-cdip and docvqa are similar [44].\n",
            "on encoder backbone. here, we study popular image classification back-\n",
            "bones that show superior performance in traditional vision tasks to measure\n",
            "their performance in vdu tasks. the figure 7(b) shows the comparison results.\n",
            "we use all the backbones pre-trained on imagenet [7]. efficientnetv2 [55] and\n",
            "swin transformer [40] outperform others on both datasets. we argue that this is\n",
            "due to the high expressiveness of the backbones, which were shown by the strik-\n",
            "ing scores on several downstream tasks as well. we choose swin transformer\n",
            "due to the high scalability of the transformer-based architecture and higher\n",
            "performance over the efficientnetv2’s.\n",
            "on input resolution. the figure 7(c) shows the performance of donut grows\n",
            "rapidly as we set a larger input size. this gets clearer in the docvqa where the\n",
            "images are larger with many tiny texts. but, increasing the size for a precise\n",
            "result incurs bigger computational costs. using an efficient attention mecha-\n",
            "nism [60] may avoid the matter in architectural design, but we use the original\n",
            "transformer [58] as we aim to present a simpler architecture in this work.\n",
            "on text localization. to see how the model behaves, we visualize the corss\n",
            "attention maps of the decoder given an unseen document image. as can be seen\n",
            "in figure 8, the model shows meaningful results that can be used as an auxiliary\n",
            "indicator. the model attends to a desired location in the given image.\n",
            "on ocr system. we test four widely-used public ocr engines (see fig-\n",
            "ure 9). the results show that the performances (i.e., speed and accuracy) of the\n",
            "conventional ocr-based methods heavily rely on the off-the-shelf ocr engine.\n",
            "more details of the ocr engines are available in appendix a.1.ocr-free document understanding transformer 13\n",
            "k\n",
            "chocomochiyoto3002-\n",
            "fig. 8. visualization of cross-attention maps in the decoder and its applica-\n",
            "tion to text localization. donut is trained without any supervision for the localiza-\n",
            "tion. the donut decoder attends proper text regions to process the image\n",
            "layoutlmv2 bert donut405060708090 accuracy\n",
            "1280   2560easyocr\n",
            "paddleocr\n",
            "msazure\n",
            "clovaocr\n",
            "layoutlmv2 bert donut0.51.01.5 time(s/image)\n",
            "1280   2560easyocr\n",
            "paddleocr\n",
            "msazure\n",
            "clovaocr\n",
            "80 160 400 800\n",
            "number of samples5060708090 accuracy\n",
            "donut, 2560\n",
            "donut, 1280layoutlmv2\n",
            "bert\n",
            "fig. 9. comparison of bert, layoutlmv2 and donut on cord. the perfor-\n",
            "mances (i.e., speed and accuracy) of the ocr-based models extremely varies depending\n",
            "on what ocr engine is used (left and center). donut shows robust performances even\n",
            "in a low resourced situation showing the higher score only with 80 samples (right)\n",
            "on low resourced situation. we evaluate the models by limiting the size\n",
            "of training set of cord [45]. the performance curves are shown in the right\n",
            "figure 9. donut shows a robust performances. we also observe that a larger\n",
            "input resolution, 2560 ×1920, shows more robust scores on the extremely low-\n",
            "resourced situation, e.g., 80 samples. as can be seen, donut outperformed the\n",
            "layoutlmv2 accuracy only with 10% of the data, which is only 80 samples.\n",
            "4 related work\n",
            "4.1 optical character recognition\n",
            "recent trends of ocr study are to utilize deep learning models in its two sub-\n",
            "steps: 1) text areas are predicted by a detector; 2) a text recognizer then rec-\n",
            "ognizes all characters in the cropped image instances. both are trained with a\n",
            "large-scale datasets including the synthetic images [26,13] and real images [28,47].\n",
            "early detection methods used cnns to predict local segments and apply\n",
            "heuristics to merge them [19,69]. later, region proposal and bounding box regres-\n",
            "sion based methods were proposed [36]. recently, focusing on the homogeneity\n",
            "and locality of texts, component-level approaches were proposed [56,4].\n",
            "many modern text recognizer share a similar approach [37,53,52,59] that can\n",
            "be interpreted into a combination of several common deep modules [3]. given the\n",
            "cropped text instance image, most recent text recognition models apply cnns\n",
            "to encode the image into a feature space. a decoder is then applied to extract\n",
            "characters from the features.14 g. kim et al.\n",
            "4.2 visual document understanding\n",
            "classification of the document type is a core step towards automated document\n",
            "processing. early methods treated the problem as a general image classification,\n",
            "so various cnns were tested [27,1,15]. recently, with bert [8], the methods\n",
            "based on a combination of cv and nlp were widely proposed [65,34]. as a\n",
            "common approach, most methods rely on an ocr engine to extract texts; then\n",
            "the ocr-ed texts are serialized into a token sequence; finally they are fed into\n",
            "a language model (e.g., bert) with some visual features if available. although\n",
            "the idea is simple, the methods showed remarkable performance improvements\n",
            "and became a main trend in recent years [64,35,2].\n",
            "document ie covers a wide range of real applications [22,42], for example,\n",
            "given a bunch of raw receipt images, a document parser can automate a major\n",
            "part of receipt digitization, which has been required numerous human-labors\n",
            "in the traditional pipeline. most recent models [25,23] take the output of ocr\n",
            "as their input. the ocr results are then converted to the final parse through\n",
            "several processes, which are often complex. despite the needs in the industry,\n",
            "only a few works have been attempted on end-to-end parsing. recently, some\n",
            "works are proposed to simplify the complex parsing processes [25,23]. but they\n",
            "still rely on a separate ocr to extract text information.\n",
            "visual qa on documents seeks to answer questions asked on document im-\n",
            "ages. this task requires reasoning over visual elements of the image and general\n",
            "knowledge to infer the correct answer [44]. currently, most state-of-the-arts fol-\n",
            "low a simple pipeline consisting of applying ocr followed by bert-like trans-\n",
            "formers [65,64]. however, the methods work in an extractive manner by their\n",
            "nature. hence, there are some concerns for the question whose answer does not\n",
            "appear in the given image [57]. to tackle the concerns, generation-based methods\n",
            "have also been proposed [48].\n",
            "5 conclusions\n",
            "in this work, we propose a novel end-to-end framework for visual document un-\n",
            "derstanding. the proposed method, donut , directly maps an input document\n",
            "image into a desired structured output. unlike conventional methods, donut\n",
            "does not depend on ocr and can easily be trained in an end-to-end fashion. we\n",
            "also propose a synthetic document image generator, synthdog, to alleviate the\n",
            "dependency on large-scale real document images and we show that donut can\n",
            "be easily extended to a multi-lingual setting. we gradually trained the model\n",
            "from how to read tohow to understand through the proposed training pipeline.\n",
            "our extensive experiments and analysis on both external public benchmarks\n",
            "and private internal service datasets show higher performance and better cost-\n",
            "effectiveness of the proposed method. this is a significant impact as the target\n",
            "tasks are already practically used in industries. enhancing the pre-training ob-\n",
            "jective could be a future work direction. we believe our work can easily be\n",
            "extended to other domains/tasks regarding document understanding.ocr-free document understanding transformer 15\n",
            "references\n",
            "1. afzal, m.z., capobianco, s., malik, m.i., marinai, s., breuel, t.m.,\n",
            "dengel, a., liwicki, m.: deepdocclassifier: document classification with\n",
            "deep convolutional neural network. in: 2015 13th international conference\n",
            "on document analysis and recognition (icdar). pp. 1111–1115 (2015).\n",
            "https://doi.org/10.1109/icdar.2015.7333933 1, 4, 14\n",
            "2. appalaraju, s., jasani, b., kota, b.u., xie, y., manmatha, r.: docformer: end-to-\n",
            "end transformer for document understanding. in: proceedings of the ieee/cvf\n",
            "international conference on computer vision (iccv). pp. 993–1003 (october\n",
            "2021) 14\n",
            "3. baek, j., kim, g., lee, j., park, s., han, d., yun, s., oh, s.j., lee, h.: what is\n",
            "wrong with scene text recognition model comparisons? dataset and model analysis.\n",
            "in: proceedings of the ieee/cvf international conference on computer vision\n",
            "(iccv) (october 2019) 2, 4, 9, 13, 22\n",
            "4. baek, y., lee, b., han, d., yun, s., lee, h.: character region awareness for text\n",
            "detection. in: 2019 ieee/cvf conference on computer vision and pattern recog-\n",
            "nition (cvpr). pp. 9357–9366 (2019). https://doi.org/10.1109/cvpr.2019.00959\n",
            "2, 4, 9, 13, 22\n",
            "5. brown, t., mann, b., ryder, n., subbiah, m., kaplan, j.d., dhariwal, p., nee-\n",
            "lakantan, a., shyam, p., sastry, g., askell, a., agarwal, s., herbert-voss, a.,\n",
            "krueger, g., henighan, t., child, r., ramesh, a., ziegler, d., wu, j., winter, c.,\n",
            "hesse, c., chen, m., sigler, e., litwin, m., gray, s., chess, b., clark, j., berner,\n",
            "c., mccandlish, s., radford, a., sutskever, i., amodei, d.: language models are\n",
            "few-shot learners. in: larochelle, h., ranzato, m., hadsell, r., balcan, m.f., lin,\n",
            "h. (eds.) advances in neural information processing systems. vol. 33, pp. 1877–\n",
            "1901. curran associates, inc. (2020), https://proceedings.neurips.cc/paper/\n",
            "2020/file/1457c0d6bfcb4967418bfb8ac142f64a-paper.pdf 5\n",
            "6. davis, b., morse, b., cohen, s., price, b., tensmeyer, c.: deep visual template-free\n",
            "form parsing. in: 2019 international conference on document analysis and recog-\n",
            "nition (icdar). pp. 134–141 (2019). https://doi.org/10.1109/icdar.2019.00030\n",
            "3\n",
            "7. deng, j., dong, w., socher, r., li, l.j., li, k., fei-fei, l.: imagenet: a large-\n",
            "scale hierarchical image database. in: 2009 ieee conference on computer vision\n",
            "and pattern recognition. pp. 248–255. ieee (2009) 6, 12, 23\n",
            "8. devlin, j., chang, m.w., lee, k., toutanova, k.: bert: pre-training of deep\n",
            "bidirectional transformers for language understanding. in: proceedings of the 2019\n",
            "conference of the north american chapter of the association for computational\n",
            "linguistics: human language technologies, volume 1 (long and short papers).\n",
            "pp. 4171–4186. association for computational linguistics, minneapolis, minnesota\n",
            "(jun 2019). https://doi.org/10.18653/v1/n19-1423, https://aclanthology.org/\n",
            "n19-1423 3, 4, 10, 14, 27, 29\n",
            "9. dosovitskiy, a., beyer, l., kolesnikov, a., weissenborn, d., zhai, x., unterthiner,\n",
            "t., dehghani, m., minderer, m., heigold, g., gelly, s., uszkoreit, j., houlsby,\n",
            "n.: an image is worth 16x16 words: transformers for image recognition at scale.\n",
            "in: 9th international conference on learning representations, iclr 2021, virtual\n",
            "event, austria, may 3-7, 2021. openreview.net (2021), https://openreview.net/\n",
            "forum?id=yicbfdntty 3, 4\n",
            "10. duong, q., h¨ am¨ al¨ ainen, m., hengchen, s.: an unsupervised method for ocr\n",
            "post-correction and spelling normalisation for finnish. in: proceedings of the16 g. kim et al.\n",
            "23rd nordic conference on computational linguistics (nodalida). pp. 240–248.\n",
            "link¨ oping university electronic press, sweden, reykjavik, iceland (online) (may\n",
            "31–2 jun 2021), https://aclanthology.org/2021.nodalida-main.24 3, 11\n",
            "11. friedl, j.e.f.: mastering regular expressions. o’reilly, beijing, 3\n",
            "edn. (2006), https://www.safaribooksonline.com/library/view/\n",
            "mastering-regular-expressions/0596528124/ 5\n",
            "12. guo, h., qin, x., liu, j., han, j., liu, j., ding, e.: eaten: entity-aware at-\n",
            "tention for single shot visual text extraction. in: 2019 international confer-\n",
            "ence on document analysis and recognition (icdar). pp. 254–259 (2019).\n",
            "https://doi.org/10.1109/icdar.2019.00049 4, 8, 10, 22, 24, 25, 26\n",
            "13. gupta, a., vedaldi, a., zisserman, a.: synthetic data for text localisation in nat-\n",
            "ural images. in: proceedings of the ieee conference on computer vision and\n",
            "pattern recognition (cvpr) (june 2016) 6, 13\n",
            "14. hammami, m., h´ eroux, p., adam, s., d’andecy, v.p.: one-shot field spotting\n",
            "on colored forms using subgraph isomorphism. in: 2015 13th international con-\n",
            "ference on document analysis and recognition (icdar). pp. 586–590 (2015).\n",
            "https://doi.org/10.1109/icdar.2015.7333829 3\n",
            "15. harley, a.w., ufkes, a., derpanis, k.g.: evaluation of deep convolutional nets\n",
            "for document image classification and retrieval. in: 2015 13th international con-\n",
            "ference on document analysis and recognition (icdar). pp. 991–995 (2015).\n",
            "https://doi.org/10.1109/icdar.2015.7333910 4, 14\n",
            "16. harley, a.w., ufkes, a., derpanis, k.g.: evaluation of deep convolutional nets\n",
            "for document image classification and retrieval. in: 2015 13th international con-\n",
            "ference on document analysis and recognition (icdar). pp. 991–995 (2015).\n",
            "https://doi.org/10.1109/icdar.2015.7333910 6\n",
            "17. he, k., zhang, x., ren, s., sun, j.: deep residual learning for image recognition.\n",
            "in: 2016 ieee conference on computer vision and pattern recognition (cvpr).\n",
            "pp. 770–778 (2016). https://doi.org/10.1109/cvpr.2016.90 4\n",
            "18. hong, t., kim, d., ji, m., hwang, w., nam, d., park, s.: bros: a pre-trained\n",
            "language model focusing on text and layout for better key information extrac-\n",
            "tion from documents. proceedings of the aaai conference on artificial intelli-\n",
            "gence 36(10), 10767–10775 (jun 2022). https://doi.org/10.1609/aaai.v36i10.21322,\n",
            "https://ojs.aaai.org/index.php/aaai/article/view/21322 2, 3, 4, 7, 9, 10, 22,\n",
            "27\n",
            "19. huang, w., qiao, y., tang, x.: robust scene text detection with convolution\n",
            "neural network induced mser trees. in: fleet, d., pajdla, t., schiele, b., tuytelaars,\n",
            "t. (eds.) computer vision – eccv 2014. pp. 497–511. springer international\n",
            "publishing, cham (2014) 13\n",
            "20. huang, z., chen, k., he, j., bai, x., karatzas, d., lu, s., jawahar, c.v.: ic-\n",
            "dar2019 competition on scanned receipt ocr and information extraction. in: 2019\n",
            "international conference on document analysis and recognition (icdar). pp.\n",
            "1516–1520 (2019). https://doi.org/10.1109/icdar.2019.00244 3\n",
            "21. hwang, a., frey, w.r., mckeown, k.: towards augmenting lexical resources for\n",
            "slang and african american english. in: proceedings of the 7th workshop on\n",
            "nlp for similar languages, varieties and dialects. pp. 160–172. international\n",
            "committee on computational linguistics (iccl), barcelona, spain (online) (dec\n",
            "2020), https://aclanthology.org/2020.vardial-1.15 10\n",
            "22. hwang, w., kim, s., yim, j., seo, m., park, s., park, s., lee, j., lee, b., lee, h.:\n",
            "post-ocr parsing: building simple and robust parser via bio tagging. in: workshop\n",
            "on document intelligence at neurips 2019 (2019) 1, 2, 4, 7, 9, 10, 14, 28, 29ocr-free document understanding transformer 17\n",
            "23. hwang, w., lee, h., yim, j., kim, g., seo, m.: cost-effective end-to-end infor-\n",
            "mation extraction for semi-structured document images. in: proceedings of the\n",
            "2021 conference on empirical methods in natural language processing. pp. 3375–\n",
            "3383. association for computational linguistics, online and punta cana, do-\n",
            "minican republic (nov 2021). https://doi.org/10.18653/v1/2021.emnlp-main.271,\n",
            "https://aclanthology.org/2021.emnlp-main.271 2, 4, 7, 9, 10, 11, 14, 27\n",
            "24. hwang, w., yim, j., park, s., yang, s., seo, m.: spatial depen-\n",
            "dency parsing for semi-structured document information extraction. in:\n",
            "findings of the association for computational linguistics: acl-ijcnlp\n",
            "2021. pp. 330–343. association for computational linguistics, online (aug\n",
            "2021). https://doi.org/10.18653/v1/2021.findings-acl.28, https://aclanthology.\n",
            "org/2021.findings-acl.28 2, 4, 9, 10\n",
            "25. hwang, w., yim, j., park, s., yang, s., seo, m.: spatial depen-\n",
            "dency parsing for semi-structured document information extraction. in:\n",
            "findings of the association for computational linguistics: acl-ijcnlp\n",
            "2021. pp. 330–343. association for computational linguistics, online (aug\n",
            "2021). https://doi.org/10.18653/v1/2021.findings-acl.28, https://aclanthology.\n",
            "org/2021.findings-acl.28 3, 10, 14, 27\n",
            "26. jaderberg, m., simonyan, k., vedaldi, a., zisserman, a.: synthetic data and ar-\n",
            "tificial neural networks for natural scene text recognition. in: workshop on deep\n",
            "learning, nips (2014) 13\n",
            "27. kang, l., kumar, j., ye, p., li, y., doermann, d.s.: convolutional neural networks\n",
            "for document image classification. 2014 22nd international conference on pattern\n",
            "recognition pp. 3168–3172 (2014) 1, 4, 14\n",
            "28. karatzas, d., gomez-bigorda, l., nicolaou, a., ghosh, s., bagdanov, a., iwamura,\n",
            "m., matas, j., neumann, l., chandrasekhar, v.r., lu, s., shafait, f., uchida,\n",
            "s., valveny, e.: icdar 2015 competition on robust reading. in: 2015 13th interna-\n",
            "tional conference on document analysis and recognition (icdar). pp. 1156–1160\n",
            "(2015). https://doi.org/10.1109/icdar.2015.7333942 13\n",
            "29. kim, w., son, b., kim, i.: vilt: vision-and-language transformer without con-\n",
            "volution or region supervision. in: meila, m., zhang, t. (eds.) proceedings of\n",
            "the 38th international conference on machine learning. proceedings of ma-\n",
            "chine learning research, vol. 139, pp. 5583–5594. pmlr (18–24 jul 2021),\n",
            "http://proceedings.mlr.press/v139/kim21k.html 3\n",
            "30. kingma, d.p., ba, j.: adam: a method for stochastic optimization. in: bengio,\n",
            "y., lecun, y. (eds.) 3rd international conference on learning representations,\n",
            "iclr 2015, san diego, ca, usa, may 7-9, 2015, conference track proceedings\n",
            "(2015), http://arxiv.org/abs/1412.6980 8, 25\n",
            "31. klaiman, s., lehne, m.: docreader: bounding-box free training of a document\n",
            "information extraction model. in: document analysis and recognition – ic-\n",
            "dar 2021: 16th international conference, lausanne, switzerland, september\n",
            "5–10, 2021, proceedings, part i. p. 451–465. springer-verlag, berlin, heidel-\n",
            "berg (2021). https://doi.org/10.1007/978-3-030-86549-8 29,https://doi.org/10.\n",
            "1007/978-3-030-86549-8_29 4\n",
            "32. lewis, d., agam, g., argamon, s., frieder, o., grossman, d., heard, j.: building\n",
            "a test collection for complex document information processing. in: proceedings of\n",
            "the 29th annual international acm sigir conference on research and develop-\n",
            "ment in information retrieval. p. 665–666. sigir ’06, association for computing\n",
            "machinery, new york, ny, usa (2006). https://doi.org/10.1145/1148170.1148307,\n",
            "https://doi.org/10.1145/1148170.1148307 4, 518 g. kim et al.\n",
            "33. lewis, m., liu, y., goyal, n., ghazvininejad, m., mohamed, a., levy, o., stoy-\n",
            "anov, v., zettlemoyer, l.: bart: denoising sequence-to-sequence pre-training\n",
            "for natural language generation, translation, and comprehension. in: proceed-\n",
            "ings of the 58th annual meeting of the association for computational lin-\n",
            "guistics. pp. 7871–7880. association for computational linguistics, online (jul\n",
            "2020). https://doi.org/10.18653/v1/2020.acl-main.703, https://aclanthology.\n",
            "org/2020.acl-main.703 5\n",
            "34. li, c., bi, b., yan, m., wang, w., huang, s., huang, f., si, l.: struc-\n",
            "turallm: structural pre-training for form understanding. in: proceedings of the\n",
            "59th annual meeting of the association for computational linguistics and\n",
            "the 11th international joint conference on natural language processing (vol-\n",
            "ume 1: long papers). pp. 6309–6318. association for computational linguis-\n",
            "tics, online (aug 2021). https://doi.org/10.18653/v1/2021.acl-long.493, https:\n",
            "//aclanthology.org/2021.acl-long.493 14\n",
            "35. li, p., gu, j., kuen, j., morariu, v.i., zhao, h., jain, r., manjunatha, v., liu,\n",
            "h.: selfdoc: self-supervised document representation learning. in: 2021 ieee/cvf\n",
            "conference on computer vision and pattern recognition (cvpr). pp. 5648–5656\n",
            "(2021). https://doi.org/10.1109/cvpr46437.2021.00560 14\n",
            "36. liao, m., shi, b., bai, x., wang, x., liu, w.: textboxes: a fast text detector with\n",
            "a single deep neural network. proceedings of the aaai conference on artificial\n",
            "intelligence 31(1) (feb 2017). https://doi.org/10.1609/aaai.v31i1.11196, https:\n",
            "//ojs.aaai.org/index.php/aaai/article/view/11196 13\n",
            "37. liu, w., chen, c., wong, k.y.k., su, z., han, j.: star-net: a spatial attention\n",
            "residue network for scene text recognition. in: richard c. wilson, e.r.h., smith,\n",
            "w.a.p. (eds.) proceedings of the british machine vision conference (bmvc).\n",
            "pp. 43.1–43.13. bmva press (september 2016). https://doi.org/10.5244/c.30.43,\n",
            "https://dx.doi.org/10.5244/c.30.43 13\n",
            "38. liu, y., gu, j., goyal, n., li, x., edunov, s., ghazvininejad, m., lewis, m.,\n",
            "zettlemoyer, l.: multilingual denoising pre-training for neural machine translation.\n",
            "transactions of the association for computational linguistics 8, 726–742 (2020),\n",
            "https://aclanthology.org/2020.tacl-1.47 5\n",
            "39. liu, y., chen, h., shen, c., he, t., jin, l., wang, l.: abcnet: real-time scene text\n",
            "spotting with adaptive bezier-curve network. in: proceedings of the ieee/cvf\n",
            "conference on computer vision and pattern recognition (cvpr) (june 2020) 2\n",
            "40. liu, z., lin, y., cao, y., hu, h., wei, y., zhang, z., lin, s., guo, b.: swin trans-\n",
            "former: hierarchical vision transformer using shifted windows. in: proceedings of\n",
            "the ieee/cvf international conference on computer vision (iccv). pp. 10012–\n",
            "10022 (october 2021) 4, 8, 12\n",
            "41. long, s., yao, c.: unrealtext: synthesizing realistic scene text images from the\n",
            "unreal world. arxiv preprint arxiv:2003.10608 (2020) 6\n",
            "42. majumder, b.p., potti, n., tata, s., wendt, j.b., zhao, q., najork, m.: rep-\n",
            "resentation learning for information extraction from form-like documents. in:\n",
            "proceedings of the 58th annual meeting of the association for computational\n",
            "linguistics. pp. 6495–6504. association for computational linguistics, online\n",
            "(jul 2020). https://doi.org/10.18653/v1/2020.acl-main.580, https://www.aclweb.\n",
            "org/anthology/2020.acl-main.580 1, 14\n",
            "43. majumder, b.p., potti, n., tata, s., wendt, j.b., zhao, q., najork, m.: repre-\n",
            "sentation learning for information extraction from form-like documents. in: pro-\n",
            "ceedings of the 58th annual meeting of the association for computational lin-\n",
            "guistics. pp. 6495–6504. association for computational linguistics, online (julocr-free document understanding transformer 19\n",
            "2020). https://doi.org/10.18653/v1/2020.acl-main.580, https://aclanthology.\n",
            "org/2020.acl-main.580 3\n",
            "44. mathew, m., karatzas, d., jawahar, c.: docvqa: a dataset for vqa on document\n",
            "images. in: proceedings of the ieee/cvf winter conference on applications of\n",
            "computer vision. pp. 2200–2209 (2021) 1, 8, 12, 14\n",
            "45. park, s., shin, s., lee, b., lee, j., surh, j., seo, m., lee, h.: cord: a consolidated\n",
            "receipt dataset for post-ocr parsing. in: workshop on document intelligence at\n",
            "neurips 2019 (2019) 2, 7, 10, 12, 13, 22, 24, 26\n",
            "46. peng, d., wang, x., liu, y., zhang, j., huang, m., lai, s., zhu, s., li, j., lin,\n",
            "d., shen, c., jin, l.: spts: single-point text spotting. corr abs/2112.07917\n",
            "(2021), https://arxiv.org/abs/2112.07917 2\n",
            "47. phan, t.q., shivakumara, p., tian, s., tan, c.l.: recognizing text with per-\n",
            "spective distortion in natural scenes. in: proceedings of the ieee international\n",
            "conference on computer vision (iccv) (december 2013) 13\n",
            "48. powalski, r., borchmann,  l., jurkiewicz, d., dwojak, t., pietruszka, m., pa lka,\n",
            "g.: going full-tilt boogie on document understanding with text-image-layout trans-\n",
            "former. in: llad´ os, j., lopresti, d., uchida, s. (eds.) document analysis and\n",
            "recognition – icdar 2021. pp. 732–747. springer international publishing, cham\n",
            "(2021) 14\n",
            "49. riba, p., dutta, a., goldmann, l., forn´ es, a., ramos, o., llad´ os, j.: table de-\n",
            "tection in invoice documents by graph neural networks. in: 2019 international\n",
            "conference on document analysis and recognition (icdar). pp. 122–127 (2019).\n",
            "https://doi.org/10.1109/icdar.2019.00028 3\n",
            "50. rijhwani, s., anastasopoulos, a., neubig, g.: ocr post correction for\n",
            "endangered language texts. in: proceedings of the 2020 conference\n",
            "on empirical methods in natural language processing (emnlp). pp.\n",
            "5931–5942. association for computational linguistics, online (nov 2020).\n",
            "https://doi.org/10.18653/v1/2020.emnlp-main.478, https://aclanthology.org/\n",
            "2020.emnlp-main.478 3, 11\n",
            "51. schaefer, r., neudecker, c.: a two-step approach for automatic ocr post-\n",
            "correction. in: proceedings of the the 4th joint sighum workshop on com-\n",
            "putational linguistics for cultural heritage, social sciences, humanities and lit-\n",
            "erature. pp. 52–57. international committee on computational linguistics, online\n",
            "(dec 2020), https://aclanthology.org/2020.latechclfl-1.6 3, 11\n",
            "52. shi, b., bai, x., yao, c.: an end-to-end trainable neural network for image-based\n",
            "sequence recognition and its application to scene text recognition. ieee transac-\n",
            "tions on pattern analysis and machine intelligence 39, 2298–2304 (2017) 13\n",
            "53. shi, b., wang, x., lyu, p., yao, c., bai, x.: robust scene text recog-\n",
            "nition with automatic rectification. in: 2016 ieee conference on com-\n",
            "puter vision and pattern recognition (cvpr). pp. 4168–4176 (2016).\n",
            "https://doi.org/10.1109/cvpr.2016.452 13\n",
            "54. taghva, k., beckley, r., coombs, j.: the effects of ocr error on the extraction of\n",
            "private information. in: bunke, h., spitz, a.l. (eds.) document analysis systems\n",
            "vii. pp. 348–357. springer berlin heidelberg, berlin, heidelberg (2006) 2\n",
            "55. tan, m., le, q.: efficientnetv2: smaller models and faster training. in: meila, m.,\n",
            "zhang, t. (eds.) proceedings of the 38th international conference on machine\n",
            "learning. proceedings of machine learning research, vol. 139, pp. 10096–10106.\n",
            "pmlr (18–24 jul 2021), https://proceedings.mlr.press/v139/tan21a.html 12\n",
            "56. tian, z., huang, w., he, t., he, p., qiao, y.: detecting text in natural image with\n",
            "connectionist text proposal network. in: leibe, b., matas, j., sebe, n., welling,20 g. kim et al.\n",
            "m. (eds.) computer vision – eccv 2016. pp. 56–72. springer international pub-\n",
            "lishing, cham (2016) 13\n",
            "57. tito, r., mathew, m., jawahar, c.v., valveny, e., karatzas, d.: icdar 2021 compe-\n",
            "tition on document visual question answering. in: llad´ os, j., lopresti, d., uchida,\n",
            "s. (eds.) document analysis and recognition – icdar 2021. pp. 635–649. springer\n",
            "international publishing, cham (2021) 1, 14\n",
            "58. vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l., gomez, a.n.,\n",
            "kaiser, l.u., polosukhin, i.: attention is all you need. in: guyon, i., luxburg,\n",
            "u.v., bengio, s., wallach, h., fergus, r., vishwanathan, s., garnett, r.\n",
            "(eds.) advances in neural information processing systems. vol. 30. curran\n",
            "associates, inc. (2017), https://proceedings.neurips.cc/paper/2017/file/\n",
            "3f5ee243547dee91fbd053c1c4a845aa-paper.pdf 4, 5, 12, 24, 27\n",
            "59. wang, j., hu, x.: gated recurrent convolution neural network for ocr. in: guyon,\n",
            "i., luxburg, u.v., bengio, s., wallach, h., fergus, r., vishwanathan, s., gar-\n",
            "nett, r. (eds.) advances in neural information processing systems. vol. 30.\n",
            "curran associates, inc. (2017), https://proceedings.neurips.cc/paper/2017/\n",
            "file/c24cd76e1ce41366a4bbe8a49b02a028-paper.pdf 13\n",
            "60. wang, s., li, b., khabsa, m., fang, h., ma, h.: linformer: self-attention with\n",
            "linear complexity. arxiv preprint arxiv:2006.04768 (2020) 12, 27\n",
            "61. wightman, r.: pytorch image models. https://github.com/rwightman/\n",
            "pytorch-image-models (2019). https://doi.org/10.5281/zenodo.4414861 25\n",
            "62. williams, r.j., zipser, d.: a learning algorithm for continually running fully re-\n",
            "current neural networks. neural computation 1(2), 270–280 (1989) 5\n",
            "63. wolf, t., debut, l., sanh, v., chaumond, j., delangue, c., moi, a., cistac,\n",
            "p., rault, t., louf, r., funtowicz, m., davison, j., shleifer, s., von platen,\n",
            "p., ma, c., jernite, y., plu, j., xu, c., le scao, t., gugger, s., drame, m.,\n",
            "lhoest, q., rush, a.: transformers: state-of-the-art natural language processing.\n",
            "in: proceedings of the 2020 conference on empirical methods in natural language\n",
            "processing: system demonstrations. pp. 38–45. association for computational\n",
            "linguistics, online (oct 2020). https://doi.org/10.18653/v1/2020.emnlp-demos.6,\n",
            "https://aclanthology.org/2020.emnlp-demos.6 25\n",
            "64. xu, y., xu, y., lv, t., cui, l., wei, f., wang, g., lu, y., florencio,\n",
            "d., zhang, c., che, w., zhang, m., zhou, l.: layoutlmv2: multi-modal\n",
            "pre-training for visually-rich document understanding. in: proceedings of the\n",
            "59th annual meeting of the association for computational linguistics and\n",
            "the 11th international joint conference on natural language processing (vol-\n",
            "ume 1: long papers). pp. 2579–2591. association for computational linguis-\n",
            "tics, online (aug 2021). https://doi.org/10.18653/v1/2021.acl-long.201, https:\n",
            "//aclanthology.org/2021.acl-long.201 2, 4, 9, 10, 11, 14, 22, 27, 28\n",
            "65. xu, y., li, m., cui, l., huang, s., wei, f., zhou, m.: layoutlm: pre-training of\n",
            "text and layout for document image understanding. in: proceedings of the 26th\n",
            "acm sigkdd international conference on knowledge discovery & data min-\n",
            "ing. p. 1192–1200. kdd ’20, association for computing machinery, new york,\n",
            "ny, usa (2020). https://doi.org/10.1145/3394486.3403172, https://doi.org/\n",
            "10.1145/3394486.3403172 2, 3, 7, 9, 10, 11, 14, 22, 28\n",
            "66. xu, y., lv, t., cui, l., wang, g., lu, y., florencio, d., zhang, c., wei, f.:\n",
            "layoutxlm: multimodal pre-training for multilingual visually-rich document un-\n",
            "derstanding. arxiv preprint arxiv:2104.08836 (2021) 10, 27\n",
            "67. yim, m., kim, y., cho, h.c., park, s.: synthtiger: synthetic text image generator\n",
            "towards better text recognition models. in: llad´ os, j., lopresti, d., uchida, s.ocr-free document understanding transformer 21\n",
            "(eds.) document analysis and recognition – icdar 2021. pp. 109–124. springer\n",
            "international publishing, cham (2021) 5, 6, 23\n",
            "68. zhang, k., shasha, d.: simple fast algorithms for the editing distance be-\n",
            "tween trees and related problems. siam j. comput. 18, 1245–1262 (12 1989).\n",
            "https://doi.org/10.1137/0218082 7\n",
            "69. zhang, z., zhang, c., shen, w., yao, c., liu, w., bai, x.: multi-oriented\n",
            "text detection with fully convolutional networks. in: 2016 ieee conference\n",
            "on computer vision and pattern recognition (cvpr). pp. 4159–4167 (2016).\n",
            "https://doi.org/10.1109/cvpr.2016.451 13\n",
            "70. zhong, x., shafieibavani, e., jimeno yepes, a.: image-based table recognition:\n",
            "data, model, and evaluation. in: vedaldi, a., bischof, h., brox, t., frahm, j.m.\n",
            "(eds.) computer vision – eccv 2020. pp. 564–580. springer international pub-\n",
            "lishing, cham (2020) 722 g. kim et al.\n",
            "a appendix\n",
            "a.1 details of ocr engines (ms, clova, easy, paddle)\n",
            "current state-of-the-art visual document understanding (vdu) backbones, such\n",
            "as bros [18], layoutlm [65] and layoutlmv2 [64], are dependent on off-the-\n",
            "shelf ocr engines. these backbones take the output of ocr as their (one of)\n",
            "input features. for the ocr-dependent methods, in our experiments, we use\n",
            "state-of-the-art ocr engines that are publicly available, including 2 ocr api\n",
            "products (i.e., ms ocr7and clova ocr8) and 2 open-source ocr models\n",
            "(i.e., easy ocr9and paddle ocr10). in the main paper, paddle ocr is used\n",
            "for the chinese train ticket dataset [12] and clova ocr is used for the rest\n",
            "datasets in the document information extraction (ie) tasks. ms ocr is used\n",
            "to measure the running time of the layoutlm family in document classification\n",
            "and visual question answering (vqa) tasks, following the previous work of xu\n",
            "et al. [64]. each ocr engine is explained in the following.\n",
            "ms ocr ms ocr7is the latest ocr api product from microsoft and used in\n",
            "several recent vdu methods, e.g., layoutlmv2 [64]. this engine supports 164\n",
            "languages for printed text and 9 languages for handwritten text (until 2022/03).\n",
            "clova ocr clova ocr8is an api product from naver clova and\n",
            "is specialized in document ie tasks. this engine supports english, japanese and\n",
            "korean (until 2022/03). in the ablation experiments on the cord dataset [45]\n",
            "(figure 9 in the main paper), the clova ocr achieved the best accuracy.\n",
            "easy ocr easy ocr9is a ready-to-use ocr engine that is publicly available\n",
            "at github. this engine supports more than 80 languages (until 2022/03). un-\n",
            "like the aforementioned two ocr products (i.e., ms ocr and clova ocr),\n",
            "this engine is publicly opened and downloadable.9the entire model architec-\n",
            "ture is based on the modern deep-learning-based ocr modules [4,3] with some\n",
            "modifications to make the model lighter and faster. the total number of model\n",
            "parameters is 27m which is small compared to the state-of-the-art models [4,3].\n",
            "paddle ocr paddle ocr10is an open-source ocr engine available at github.\n",
            "we used a lightweight (i.e., mobile) version of the model which is specially de-\n",
            "signed for a fast and light ocr of english and chinese texts. the model is served\n",
            "on a cpu environment and the size of the model is extremely small, which is\n",
            "approximately 10m.\n",
            "7https://docs.microsoft.com/en-us/azure/cognitive-services/\n",
            "computer-vision/overview-ocr .\n",
            "8https://clova.ai/ocr/en .\n",
            "9https://github.com/jaidedai/easyocr .\n",
            "10https://github.com/paddlepaddle/paddleocr .ocr-free document understanding transformer 23\n",
            "fig. a. examples of synthdog. english, chinese, japanese and korean samples\n",
            "are shown (from top to bottom). although the idea is simple, these synthetic samples\n",
            "play an important role in the pre-training of donut. please, see figure 7 in the main\n",
            "paper for details\n",
            "a.2 details of synthetic document generator (synthdog)\n",
            "in this section, we explain the components of the proposed synthetic document\n",
            "generator (synthdog) in detail. the entire pipeline basically follows yim et\n",
            "al. [67]. our source code is available at https://github.com/clovaai/donut .\n",
            "more samples are shown in figure a.\n",
            "background background images are sampled from imagenet [7]. gaussian blur\n",
            "is randomly applied to the background image to represent out-of-focus effects.\n",
            "document paper textures are sampled from the photos that we collected. the\n",
            "texture is applied to an white background. in order to make the texture realistic,\n",
            "random elastic distortion and gaussian noise are applied. to represent various\n",
            "view angles in photographs, a random perspective transformation is applied to\n",
            "the image.24 g. kim et al.\n",
            "text layout and pattern to mimic the layouts in real-world documents, a\n",
            "heuristic rule-based pattern generator is applied to the document image region\n",
            "to generate text regions. the main idea is to set multiple squared regions to rep-\n",
            "resent text paragraphs. each squared text region is then interpreted as multiple\n",
            "lines of text. the size of texts and text region margins are chosen randomly.\n",
            "text content and style we prepare the multi-lingual text corpora from\n",
            "wikipedia.11we use noto fonts12since it supports various languages. synthdog\n",
            "samples texts and fonts from these resources and the sampled texts are rendered\n",
            "in the regions that are generated by the layout pattern generator. the text colors\n",
            "are randomly assigned.\n",
            "post-processing finally, some post-processing techniques are applied to the\n",
            "output image. in this process, the color, brightness, and contrast of the image\n",
            "are adjusted. in addition, shadow effect, motion blur, gaussian blur, and jpeg\n",
            "compression are applied to the image.\n",
            "a.3 details of document information extraction\n",
            "information extraction (ie) on documents is an arduous task since it requires (a)\n",
            "reading texts, (b) understanding the meaning of the texts, and (c) predicting the\n",
            "relations and structures among the extracted information. some previous works\n",
            "have only focused on extracting several pre-defined key information [12]. in that\n",
            "case, only (a) and (b) are required for ie models. we go beyond the previous\n",
            "works by considering (c) also. although the task is complex, its interface (i.e., the\n",
            "format of input and output) is simple. in this section, for explanation purposes,\n",
            "we show some sample images (which are the raw input of the ie pipeline) with\n",
            "the output of donut.\n",
            "in the main paper, we test four datasets including two public benchmarks\n",
            "(i.e., cord [45] and ticket [12]) and two private industrial datasets (i.e., busi-\n",
            "ness card andreceipt ). figure b shows examples of ticket with the outputs\n",
            "of donut. figure c shows examples of cord with the outputs of donut. due\n",
            "to strict industrial policies on the private industrial datasets, we instead show\n",
            "some real-like high-quality samples of business card andreceipt in figure d.\n",
            "a.4 details of model training scheme and output format\n",
            "in the model architecture and training objective, we basically followed the orig-\n",
            "inal transformer [58], which uses a transformer encoder-decoder architecture\n",
            "and a teacher-forcing training scheme. the teacher-forcing scheme is a model\n",
            "training strategy that uses the ground truth as input instead of model output\n",
            "from a previous time step. figure e shows a details of the model training scheme\n",
            "and decoder output format.\n",
            "11https://dumps.wikimedia.org .\n",
            "12https://fonts.google.com/noto .ocr-free document understanding transformer 25\n",
            "(a) input image(b) prediction(c) ground truth\n",
            "ticket\n",
            "{    \"date\":\"2017年11月15日\",    \"destination_station\": \"福田站\",    \"name\": \"珂\",    \"seat_category\": \"二等座\",    \"starting_station\": \"广州南站\",    \"ticket_num\": \"c068987\",    \"ticket_rates\": ¥82.0元\",    \"train_num\": \"g79”}{    \"date\": \"2017年12月05日\",    \"destination_station\": \"广州东站\",    \"name\": \"延辉\",    \"seat_category\": \"一等座\",    \"starting_station\": \"深圳站\",    \"ticket_num\": \"e019154\",    \"ticket_rates\": \"¥99.5元\",    \"train_num\": \"c7128\"}\n",
            "{    \"date\": \"2018年02月13日\",    \"destination_station\": \"扎兰平站\",    \"name\": \"海鹏\",    \"seat_category\": \"新空调硬卧\",    \"starting_station\": \"北京站\",    \"ticket_num\": ”j033534\",    \"ticket_rates\": \"¥367.0元\",    \"train_num\": \"k1301”}{    \"date\": \"2018年02月13日\",    \"destination_station\": \"扎兰屯站\",    \"name\": \"海鹏\",    \"seat_category\": \"新空调硬卧\",    \"starting_station\": \"北京站\",    \"ticket_num\": ”j033534\",    \"ticket_rates\": \"¥367.0元\",    \"train_num\": \"k1301”}ted acc. 97.7ted acc. 97.5{    \"date\": \"2017年12月05日\",    \"destination_station\": \"广州东站\",    \"name\": \"延褥\",    \"seat_category\": \"一等座\",    \"starting_station\": \"深圳站\",    \"ticket_num\": \"e019154\",    \"ticket_rates\": \"¥99.5元\",    \"train_num\": \"c7128\"}ted acc. 100{    \"date\":\"2017年11月15日\",    \"destination_station\": \"福田站\",    \"name\": \"珂\",    \"seat_category\": \"二等座\",    \"starting_station\": \"广州南站\",    \"ticket_num\": \"c068987\",    \"ticket_rates\": ¥82.0元\",    \"train_num\": \"g79”}\n",
            "fig. b. examples of ticket [12] with donut predictions. there is no hierarchy in\n",
            "the structure of information (i.e., depth = 1) and the location of each key information\n",
            "is almost fixed. failed predictions are marked and bolded (red)\n",
            "a.5 implementation and training hyperparameters\n",
            "the codebase and settings are available at github.13we implement the en-\n",
            "tire model pipeline with huggingface’s transformers14[63] and an open-source\n",
            "library timm (pytorch image models)15[61].\n",
            "for all model training, we use a half-precision (fp16) training. we train donut\n",
            "using adam optimizer [30] by decreasing the learning rate as the training pro-\n",
            "gresses. the initial learning rate of pre-training is set to 1e-4 and that of fine-\n",
            "tuning is selected from 1e-5 to 1e-4. we pre-train the model for 200k steps with\n",
            "64 nvidia a100 gpus and a mini-batch size of 196, which takes about 2-3\n",
            "gpu days. we also apply a gradient clipping technique where a maximum gra-\n",
            "dient norm is selected from 0.05 to 1.0. the input resolution of donut is set\n",
            "to 2560 ×1920 at the pre-training phase. in downstream tasks, the input reso-\n",
            "lutions are controlled. in some downstream document ie experiments, such as,\n",
            "13https://github.com/clovaai/donut .\n",
            "14https://github.com/huggingface/transformers .\n",
            "15https://github.com/rwightman/pytorch-image-models .26 g. kim et al.\n",
            "{    \"menu\": [        {            \"cnt\": [\"1\"],            \"nm\": [\"cashew nuts chkn\"],            \"price\": [\"64,500\"]        },          …        {            \"cnt\": [\"4\"],            \"nm\": [\"steamed rice\"],            \"price\": [\"47,600\"]        }    ],    \"sub_total\": [        {            \"service_price\": [\"17,908\"],            \"subtotal_price\": [\"325,600\"],            \"tax_price\": [\"34,351\"]        }    ],    \"total\": [        {            \"total_price\": [\"377,859\"]        } ]}ted acc. 100\n",
            "(a) input image(b) prediction(c) ground truth\n",
            "cord{    \"menu\": [        {            \"cnt\": [\"2\"],            \"nm\": [\"twist donut\"],            \"price\": [\"18,000\"]        },         …        {            \"cnt\": [\"1\"],            \"nm\": [\"frankfrut sausage roll\"],            \"price\": [\"12,000\"]        }    ],    \"total\": [        {            \"cashprice\": [\"104.000\"],            \"changeprice\": [\"56.000\"],            \"total_price\": [\"54.000\"]        } ]}ted acc. 99.0{    \"menu\": [        {            \"cnt\": [\"2\"],            \"nm\": [\"twist donut\"],            \"price\": [\"18,000\"]        },         …        {            \"cnt\": [\"1\"],            \"nm\": [\"frankfrut sausage roll\"],            \"price\": [\"12,000\"]        }    ],    \"total\": [        {            \"cashprice\": [\"104.000\"],            \"changeprice\": [\"50.000\"],            \"total_price\": [\"54.000\"]        } ]}{    \"menu\": [        {            \"nm\": [\"trad ky toast carte\"],            \"price\": [\"28.182\"]        }    ],    \"sub_total\": [        {            \"subtotal_price\": [\"28.182\"],            \"tax_price\": [\"2.818\"]        }    ],    \"total\": [        {            \"cashprice\": [\"31.000\"],            \"menuqty_cnt\": [\"1.00\"],            \"total_price\": [\"31.000\"]        } ]}{    \"menu\": [        {            \"nm\": [\"trad ky toast carte\"],            \"price\": [\"28.182\"]        }    ],    \"sub_total\": [        {            \"subtotal_price\": [\"28.182\"],            \"tax_price\": [\"2.818\"]        }    ],    \"total\": [        {            \"cashprice\": [\"31.000\"],            \"total_price\": [\"31.000\"]        } ]}\n",
            "ted acc. 89.6{    \"menu\": [        {            \"cnt\": [\"1\"],            \"nm\": [\"cashew nuts chkn\"],            \"price\": [\"64,500\"]        },          …        {            \"cnt\": [\"4\"],            \"nm\": [\"steamed rice\"],            \"price\": [\"47,600\"]        }    ],    \"sub_total\": [        {            \"service_price\": [\"17,908\"],            \"subtotal_price\": [\"325,600\"],            \"tax_price\": [\"34,351\"]        }    ],    \"total\": [        {            \"total_price\": [\"377,859\"]        } ]}\n",
            "fig. c. examples of cord [45] with donut predictions. there is a hierarchy\n",
            "in the structure of information (i.e., depth = 2). donut not only reads some important\n",
            "key information from the image, but also predicts the relationship among the extracted\n",
            "information (e.g., the name, price, and quantity of each menu item are grouped)\n",
            "cord [45], ticket [12] and business card , smaller size of input resolution,\n",
            "e.g., 1280 ×960, is tested. with the 1280 ×960 setting, the model training cost\n",
            "of donut was small. for example, the model fine-tuning on cord orticket\n",
            "took approximately 0.5 hours with one a100 gpu. however, when we set theocr-free document understanding transformer 27\n",
            "fig. d. examples of business card (top) and receipt (bottom). due to strict\n",
            "industrial policies on the private industrial datasets from our active products, real-like\n",
            "high-quality samples are shown instead\n",
            "2560×1920 setting for larger datasets, e.g., rvl-cdip ordocvqa , the cost\n",
            "increased rapidly. with 64 a100 gpus, docvqa requires one gpu day and\n",
            "rvl-cdip requires two gpu days approximately. this is not surprising in that\n",
            "increasing the input size for a precise result incurs higher computational costs\n",
            "in general. using an efficient attention mechanism [60] may avoid the prob-\n",
            "lem in architectural design, but we use the original transformer [58] as we aim\n",
            "to present a simpler architecture in this work. our preliminary experiments in\n",
            "smaller resources are available in appendix a.6.\n",
            "for the implementation of document ie baselines, we use the transformers\n",
            "library for bert [8], bros [18], layoutlmv2 [64,66] and wyvern [23]. for\n",
            "the spade [25] baseline, the official implementation16is used. the models are\n",
            "trained using nvidia p40, v100, or a100 gpus. the major hyperparameters,\n",
            "such as initial learning rate and number of epochs, are adjusted by monitoring\n",
            "the scores on the validation set. the architectural details of the ocr-dependent\n",
            "vdu backbone baselines (e.g., layoutlm and layoutlmv2) are available in\n",
            "appendix a.7.\n",
            "a.6 preliminary experiments in smaller resources\n",
            "in our preliminary experiments, we pre-trained donut with smaller resources\n",
            "(denoted as donut proto), i.e., smaller data (synthdog 1.2m) and fewer gpus\n",
            "16https://github.com/clovaai/spade .28 g. kim et al.\n",
            "*urxqg\u00037uxwk\n",
            ",qsxw\u00037rnhqv'hfrghu\u00032xwsxw\u0003\u000b'lvwulexwlrqv\f'hfrghu«««7udlqlqj,qihuhqfh\n",
            "7rnhq\u0003fodvvlilfdwlrq\u0003dw\u0003hdfk\u0003vwhs\u0011\u001fsduvlqj!\u001flwhp!\u001fqdph!\u0016\u0013\u0013\u0015\u001flwhp!\u001fqdph!\u0016\u0013\u0013\u00150lqlpl]h\u0003&urvv\u0003(qwurs\\3uhglfwhg««\u001fsduvlqj!\u001flwhp!\u001fqdph!\n",
            "«\u001flwhp!\u001fqdph!\u001b\u0013\u0013\u0015'hfrghu\u001b\u0013\u0013\u0015\n",
            "3uhglfwhg\u0003wrnhq\u0003vhtxhqfh\u0003lv\u0003frqyhuwhg\u0003lqwr\u0003d\u0003-621\u0003irupdw\u0011^³,whp´\u0003\u001d\u0003^\u0003\u0003\u0003\u0003\u0003\u0003³qdph´\u0003\u001d\u0003³\u001b\u0013\u0013\u0015\u0010.\\rwr\u0003&krfr\u00030rfkl´\u000f\u0003\u0003\u0003\u0003\u0003\u0003³sulfh´\u0003\u001d\u0003³\u0014\u0017\u0011\u0013\u0013\u0013´\u000f\u0003\u0003\u0003\u0003\u0003\u0003³frxqw´\u0003\u001d\u0003³\u0015´\u0003``\u001fsduvlqj!\u001flwhp!\u001fqdph!\u001b\u0013\u0013\u0015\u0010.\\rwr\u0003&krfr\u00030rfkl\u001f\u0012qdph!\u001fsulfh!\u0014\u0017\u0011\u0013\u0013\u0013\u001f\u0012sulfh!\u001ffrxqw!\u0015\u001f\u0012frxqw!\u001f\u0012lwhp!\u001fhqg!\n",
            "fig. e. donut training scheme with teacher forcing and decoder output\n",
            "format examples. the model is trained to minimize cross-entropy loss of the token\n",
            "classifications simultaneously. at inference, the predicted token from the last step is\n",
            "fed to the next\n",
            "(8 v100 gpus for 5 days). the input size was 2048 ×1536. in this setting,\n",
            "donut proto also achieved comparable results on rvl-cdip andcord . the\n",
            "accuracy on rvl-cdip was 94.5 and cord was 85.4. after the preliminaries,\n",
            "we have scaled the model training with more data.\n",
            "a.7 details of ocr-dependent baseline models\n",
            "in this section, we provide a gentle introduction to the general-purpose vdu\n",
            "backbones, such as layoutlm [65] and layoutlmv2 [64]. to be specific, we\n",
            "explain how the conventional backbones perform downstream vdu tasks; docu-\n",
            "ment classification, ie, and vqa. common to all tasks, the output of the ocr\n",
            "engine is used as input features of the backbone. that is, the extracted texts\n",
            "are sorted and converted to a sequence of text tokens. the sequence is passed to\n",
            "the transformer encoder to get contextualized output vectors. the vectors are\n",
            "used to get the desired output. the difference in each task depends on a slight\n",
            "modification on the input sequence or on the utilization of the output vectors.\n",
            "document classification at the start of the input sequence, a special token\n",
            "[cls] is appended. the sequence is passed to the backbone to get the output\n",
            "vectors. with a linear mapping and softmax operation, the output vector of the\n",
            "special token [cls] is used to get a class-label prediction.\n",
            "document ie with a linear mapping and softmax operation, the output vector\n",
            "sequence is converted to a bio-tag sequence [22].ocr-free document understanding transformer 29\n",
            "ie on 1-depth structured documents when there is no hierarchical structure in\n",
            "the document (see figure b), the tag set is defined as {“bk”, “i k”, “o” |k∈\n",
            "pre-defined keys }. “bk” and “i k” are tags that represent the beginning (b) and\n",
            "the inside (i) token of the key krespectively. the “o” tag indicates that the\n",
            "token belongs to no key information.\n",
            "ie on n-depth structured documents when there are hierarchies in the structure\n",
            "(see figure c), the bio-tags are defined for each hierarchy level. in this section,\n",
            "we explain a case where the depth of structure is n= 2. the tag set is defined\n",
            "as{“bg.bk”, “b g.ik”, “i g.bk”, “i g.ik”, “o” |g∈pre-defined parent keys, k∈\n",
            "pre-defined child keys }. for instance, the figure c shows an example where a\n",
            "parent key is “menu” and related child keys are {“cnt”, “nm”, “price” }. “b g”\n",
            "represents that one group (i.e., a parent key such as “menu”) starts, and “i g”\n",
            "represents that the group is continuing. separately from the bi tags of the parent\n",
            "key (i.e., “b g” and “i g”), the bi tags of each child key (i.e., “b k” and “i k”) work\n",
            "the same as in the case of n= 1. this bio-tagging method is also known as\n",
            "group bio-tagging and the details are also available in hwang et al. [22].\n",
            "document vqa with a linear mapping and softmax operation, the output\n",
            "vector sequence is converted to a span-tag sequence. for the input token se-\n",
            "quence, the model finds the beginning and the end of the answer span. details\n",
            "can also be found in the section 4.2 of devlin et al. [8].\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tibQ9GFGOr6",
        "outputId": "9e923b73-cd39-4256-8c51-a4bbefa1be51"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (3.9.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i was facing error in google colab with sentence transformer so i proceed with autotokenizer for creating embeddings"
      ],
      "metadata": {
        "id": "HDcoiPlaIGDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "documents_to_embed = [all_processed_text] # Or a list of individual document texts\n",
        "\n",
        "# Tokenize the documents\n",
        "encoded_input = tokenizer(documents_to_embed, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Generate embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "\n",
        "document_embeddings = model_output.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "print(\"Embeddings generated using transformers. Shape:\", document_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "fa18cabe283949379613d0f1753ceb3f",
            "30853b283b9041748dbc490cf611380d",
            "8fcc1d7691244d3f8481cb9e99e21d9f",
            "a650f990e0e64b2a9117aeb348af38b8",
            "8bfca65edbf54dfeb3c7282441947835",
            "73d78321b21e459997f924fb5f1bfe8a",
            "65bec95b20ac45179c91b6357e8a2c15",
            "43047dada2b44bf7a5daa5d3d202007e",
            "9d1bc211218541d190415d0d8850abcf",
            "121acc13507e43299549b3aba197125c",
            "3a7e3c8c00e74a999b4e2618eb5df3fa",
            "e0fd08bf01e1499dad918edace7d6223",
            "4e7b623ba9994f9fb650912b1d775804",
            "6006e60708634159b8677d76841fe6f4",
            "7a1bae42de9f4ecc9ef99e18f546edb6",
            "723a5979deab41b7b066c23feba1f388",
            "00e0d4970ad2443cbfcf3e0a52e5bec4",
            "46762faf3e934249a35991c00bf42ba8",
            "48c4345060eb4ff7b03e0e1540b8be73",
            "4c5e07301a3d425ab00e5ba07a5098e5",
            "82e91205e9bb46f78a147d6b18b06770",
            "c5f239273e17497f8cc54cdb9551096a",
            "3143121c68994ad198bf59b50b3eb0a1",
            "f99eac364398434aa5ecfbf54d94613c",
            "9e76b745e60749298c7ad7cdd89fc234",
            "038bec5cbf874f158cf951c726f31fb7",
            "5b136bf84c274ec98733224f8b7c41cc",
            "aaeb681ce16847fdbd6001c202999251",
            "bf19688eeea04df6a53faf8153d8d25c",
            "1241d394ca044ec9978fcb4627bffac9",
            "c979e4990e764d2589f49f3bdc5eeb88",
            "9738c8e5743140aca2352a580309751d",
            "9ab54032c4304485afd7fbd03f5af771",
            "9ff40f2c3e1345c789898e773c6e9010",
            "3573d7a2e49142cd8fb4782ccd826f53",
            "79f7287fefe4425b84474b65f97dbdf1",
            "83f6f6eed9ce4cc49aac082b7ae0a16d",
            "85c6ddbdf6e54e1690dd4d98a714bc2f",
            "1fd0cfd908174446bd4aa207f4e32a88",
            "f4017dfb43674bc5b8352e7cabb03d27",
            "4322201a186a48188c1b68855df8ce07",
            "de9d42ea2b124a5a82a778d72ad7a2f9",
            "61cf7e5d304a403381206e23d49ef9c1",
            "d8f2742ff2b748d5ad5d9fe2a8675713",
            "a5df6298ebe14cf38838b7a191593a73",
            "2c98c4985cb547488dc1335cb7ef839f",
            "b526ba58bc304fd3a246668dfccb22e9",
            "0b2f6d8390fe48539ba3686784d43a9d",
            "93d4a83c49864adea34ac90124d7315c",
            "ed58bdc72cfe42819b6b529a9f8d8ae5",
            "3b0bb904feca4b359280d83b706d66f3",
            "baacf5df2a754e80b7f90629beae635f",
            "6986db88f4d94dae8003b819ca283381",
            "d7eb8a57fa0e44d7acdb0bc89c416925",
            "28610adce4774fe28ff8bb5e199a52b9"
          ]
        },
        "id": "kOsBwFsuHzg6",
        "outputId": "66694303-f699-4fb0-d72a-30a16451cc1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa18cabe283949379613d0f1753ceb3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0fd08bf01e1499dad918edace7d6223"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3143121c68994ad198bf59b50b3eb0a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ff40f2c3e1345c789898e773c6e9010"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5df6298ebe14cf38838b7a191593a73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings generated using transformers. Shape: (1, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb==0.4.14"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXn0d3MZGaG1",
        "outputId": "5bb532cd-9c07-443f-99b2-bc7fd8c12807"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb==0.4.14\n",
            "  Downloading chromadb-0.4.14-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (2.32.3)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (2.11.4)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.14)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb==0.4.14)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.14)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.4.14)\n",
            "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (4.13.2)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.14)\n",
            "  Downloading pulsar_client-3.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb==0.4.14)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb==0.4.14)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.4.14)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb==0.4.14)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.15.3)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (2.0.2)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb==0.4.14)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.14)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (2.9.0.post0)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.4.14)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (1.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pulsar-client>=3.1.0->chromadb==0.4.14) (2025.4.26)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.14) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.14) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.14) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.14) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.14) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.14) (2.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb==0.4.14) (0.31.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.14) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.14) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.14) (13.9.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.14) (0.16.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.14)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.14)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (6.0.2)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.14)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.14)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.14) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.14) (2025.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.14) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.14) (2.19.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.14) (4.9.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.14)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.14) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.14) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.14) (0.1.2)\n",
            "Downloading chromadb-0.4.14-py3-none-any.whl (448 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.1/448.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pulsar_client-3.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53801 sha256=75e9f8f2ba0b5a114abf0adabbd367111b0e0bae56b2c36ad1fa3cd68e67e2f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, uvloop, uvicorn, python-dotenv, pulsar-client, overrides, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, watchfiles, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chroma-hnswlib-0.7.3 chromadb-0.4.14 coloredlogs-15.0.1 fastapi-0.115.12 httptools-0.6.4 humanfriendly-10.0 onnxruntime-1.22.0 overrides-7.7.0 posthog-4.0.1 pulsar-client-3.7.0 pypika-0.48.9 python-dotenv-1.1.0 starlette-0.46.2 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "# Initialize a ChromaDB client\n",
        "# This will create a local ChromaDB instance\n",
        "client = chromadb.Client()\n",
        "\n",
        "# Create or get a collection\n",
        "collection_name = \"my_document_collection\"\n",
        "try:\n",
        "    collection = client.create_collection(name=collection_name)\n",
        "    print(f\"Collection '{collection_name}' created.\")\n",
        "except chromadb.db.base.UniqueConstraintError:\n",
        "    collection = client.get_collection(name=collection_name)\n",
        "    print(f\"Collection '{collection_name}' already exists. Getting existing collection.\")\n",
        "\n",
        "\n",
        "# For now, let's assume you have one embedding and one document string (the concatenated text)\n",
        "ids = [\"doc1\"]  # Unique ID for the document\n",
        "embeddings = document_embeddings.tolist()  # Convert numpy array to list\n",
        "documents = documents_to_embed  # The list of document strings\n",
        "\n",
        "# Add data to the collection\n",
        "collection.add(\n",
        "    embeddings=embeddings,\n",
        "    documents=documents,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(f\"Embeddings and documents added to collection '{collection_name}'.\")\n",
        "\n",
        "# You can now perform operations like searching the collection\n",
        "print(\"Number of items in collection:\", collection.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJA9cazhIqob",
        "outputId": "a0dd4ccc-0cb9-424d-898f-23fe9fe5a84f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collection 'my_document_collection' created.\n",
            "Embeddings and documents added to collection 'my_document_collection'.\n",
            "Number of items in collection: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you have the transformers library installed for embedding the query\n",
        "!pip install transformers==4.34.0 torch==2.1.0\n",
        "\n",
        "# Make sure you have chromadb installed\n",
        "!pip install chromadb==0.4.14\n",
        "\n",
        "# %%\n",
        "import chromadb\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "client = chromadb.Client()\n",
        "\n",
        "\n",
        "# Get the collection where you stored the embeddings\n",
        "collection_name = \"my_document_collection\"\n",
        "try:\n",
        "    collection = client.get_collection(name=collection_name)\n",
        "    print(f\"Successfully retrieved collection '{collection_name}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error retrieving collection '{collection_name}': {e}\")\n",
        "    print(\"Please ensure the collection exists and the client is initialized correctly.\")\n",
        "\n",
        "\n",
        "# Load the same model and tokenizer used for document embedding\n",
        "model_name = \"bert-base-uncased\" # Use the same model as for document embedding\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "# --- End of initialization ---\n",
        "\n",
        "\n",
        "# Function to generate embedding for a query\n",
        "def embed_query(query_text):\n",
        "    encoded_input = tokenizer(query_text, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    # Use the same method to get the query embedding as you used for documents\n",
        "    query_embedding = model_output.last_hidden_state[:, 0, :].numpy()\n",
        "    return query_embedding.tolist() # Return as a list\n",
        "\n",
        "\n",
        "# Accept user query\n",
        "user_query = input(\"Enter your query: \")\n",
        "\n",
        "# Generate embedding for the query\n",
        "query_embedding = embed_query(user_query)\n",
        "\n",
        "# Perform similarity search in ChromaDB\n",
        "# query_embeddings expects a list of embeddings, even if you have only one query\n",
        "results = collection.query(\n",
        "    query_embeddings=query_embedding,\n",
        "    n_results=1, # Retrieve the top 1 most relevant document\n",
        "    include=['documents', 'distances'] # Include the original document text and distance\n",
        ")\n",
        "\n",
        "# Display the most relevant document and its similarity score (distance)\n",
        "if results and results.get('documents') and results.get('distances'):\n",
        "    most_relevant_document = results['documents'][0][0]\n",
        "    similarity_distance = results['distances'][0][0]\n",
        "    print(\"\\nMost Relevant Document:\")\n",
        "    print(most_relevant_document)\n",
        "    print(f\"\\nSimilarity Distance (lower is more similar): {similarity_distance}\")\n",
        "else:\n",
        "    print(\"\\nNo relevant documents found.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE8oNigwJI4O",
        "outputId": "6ba14480-8c4a-476d-e565-b76d264d666a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.34.0\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m112.6/121.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.1.0\n",
            "  Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (2.32.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0)\n",
            "  Downloading tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0)\n",
            "  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.4.127)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/731.7 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:09\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n",
            "    unknown package:\n",
            "        Expected sha256 5ccb288774fdfb07a7e7025ffec286971c06d8d7b4fb162525334616d7629ff9\n",
            "             Got        69f0ffa0c599fe945b882ee66fa808e2954932ee492e7e4a1dacea376a942e60\n",
            "\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: chromadb==0.4.14 in /usr/local/lib/python3.11/dist-packages (0.4.14)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (2.32.3)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (2.11.4)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (0.34.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (4.13.2)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (3.7.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (1.22.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.15.3)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (2.0.2)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb==0.4.14) (0.46.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (2.9.0.post0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (1.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pulsar-client>=3.1.0->chromadb==0.4.14) (2025.4.26)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.14) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.14) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.14) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.14) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.14) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.14) (2.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb==0.4.14) (0.31.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.14) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.14) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.14) (13.9.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.14) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (1.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (6.0.2)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.14) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.14) (2025.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.14) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.14) (2.19.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.14) (4.9.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.14) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.14) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.14) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.14) (0.1.2)\n",
            "Successfully retrieved collection 'my_document_collection'.\n",
            "Enter your query: process flow of project\n",
            "\n",
            "Most Relevant Document:\n",
            "project progress report: digitizing & \n",
            "structuring physical documents for \n",
            "loan applications (ai chat for banking \n",
            "transactions)  \n",
            "name : mansi m kanojiya  \n",
            "prn: 23070243026  \n",
            "progress to date  \n",
            "workflow for pdf to postgresql ingestion  \n",
            "below is the workflow illustrating pdf ingestion and storage in postgresql:  \n",
            " \n",
            "1. executive summary  \n",
            "to date, i have developed a working prototype that automates the ingestion of scanned \n",
            "bank‐statement pdfs, structures their content in postgresql, and enables interactive q&a \n",
            "via large language models (llms). this pipelin e demonstrates the core objectives of the \n",
            "project synopsis —preserving contextual formatting for llm comprehension, standardizing \n",
            "financial data, and delivering user‐friendly insights —while laying the groundwork for an \n",
            "agentic, autogen‐driven architecture.  \n",
            "2. completed work  \n",
            "• data ingestion & storage  \n",
            "  - implemented pdf_parser.py in banking_chatbot.py.ipynb to extract transaction rows \n",
            "(date, description, debit/credit, balance) from diverse pdf layouts using pypdf2, pandas \n",
            "regex logic, and schema‐driven mappi ngs.  \n",
            "  - designed a normalized postgresql schema (transactions, accounts, metadata) and built \n",
            "db_loader.py to create tables and bulk‐load cleaned data via psycopg2.  \n",
            "• llm‐powered query engine  \n",
            "  - developed query_agent.py with two llm prompt templates: one to convert natural‐\n",
            "language questions into parameterized sql queries, and another to refine raw query results \n",
            "into coherent, context‐rich answers with source citations.  \n",
            "  - integrated openai’s gpt‐4 api, orchestrating the flow: user uploads pdf → ingestion  → \n",
            "sql generation → database execution → answer refinement.  \n",
            "  - validated the system on 100+ sample statements and 50+ question types, achieving > \n",
            "98% field‐extraction accuracy and 95% answer correctness.  \n",
            "• interactive interface  \n",
            "  - created a jupyter‐noteb ook ui (main_interface.py) with file‐upload widgets and chat \n",
            "input, enabling seamless user interaction and immediate feedback.  \n",
            "3. alignment with project synopsis  \n",
            "• preserving structural & contextual attributes: while initial parsing focuses on tabular \n",
            "data , the database metadata table already captures document‐level properties (document \n",
            "id, upload time, page count).  \n",
            "• data standardization: dates and currency formats are normalized during ingestion; \n",
            "transaction descriptions are cleaned and mapped to consiste nt categories.  \n",
            "• generative ai enrichment: the two‐step llm refinement ensures that answers retain \n",
            "semantic depth and cite original data sources, matching the synopsis’s goal of enriched llm \n",
            "understanding.  \n",
            "4. pending work & ongoing efforts  \n",
            "• document pre‐p rocessing & ocr enhancements (in progress)  \n",
            "  - implement noise removal, de‐skewing, and contrast enhancement using opencv and pil \n",
            "to improve ocr fidelity on scanned images.  \n",
            "  - integrate tesseract ocr with a google vision fallback to extract both text and layout \n",
            "metadata (headings, subheadings, font styles, tables, images, diagrams).  \n",
            "• ai‐based data enrichment & validation  \n",
            "  - develop generative‐ai modules for transaction classification (salary credits, loan repayments, rent, etc.) and anomaly detection (fr audulent patterns, missing entries).  \n",
            "  - implement nlp‐based cross‐verification routines to flag and correct inconsistencies in \n",
            "extracted data.  \n",
            "• structured output & system integration  \n",
            "  - extend the pipeline to export final structured data in excel/csv fo rmats and expose \n",
            "restful apis for real‐time integration with loan‐management crms and underwriting \n",
            "systems.  \n",
            "• agentic architecture with autogen  \n",
            "  - define and implement three autogen agents:  \n",
            "    1. ingestagent for automated monitoring and pre‐processing of  new document uploads  \n",
            "    2. structagent for schema validation, anomaly flagging, and metadata enrichment  \n",
            "    3. queryagent for managing user q&a workflows, chaining ocr, sql generation, \n",
            "execution, and answer refinement tools  \n",
            "  - integrate a vector‐databas e (e.g., faiss) for semantic search and build a human‐in‐the‐\n",
            "loop feedback dashboard to iteratively fine‐tune llm prompts and parsing heuristics.  \n",
            "5. upcoming milestones  \n",
            "• complete ocr enhancements and layout‐metadata extraction; begin ai‐based \n",
            "classificati on and anomaly modules.  \n",
            "• implement excel/csv export functionality and api integrations; finalize agent definitions \n",
            "and tool interfaces.  \n",
            "• deploy autogen agents in a test environment; iterate on human‐in‐the‐loop review \n",
            "dashboard; aim for sub‐2  s response latency and ≥  99% extraction accuracy.  \n",
            "6. conclusion  \n",
            "the foundational pipeline meets the core objectives of digitizing, structuring, and enabling \n",
            "llm‐driven q&a over financial documents. the upcoming phases will focus on enriching \n",
            "document metadata, automa ting workflows via autogen agents, and integrating with \n",
            "downstream loan‐processing systems —culminating in a scalable, production‐ready \n",
            "platform that revolutionizes loan application workflows.  \n",
            " \n",
            "thank you.  \n",
            "1\n",
            "trie++: towards end-to-end information\n",
            "extraction from visually rich documents\n",
            "zhanzhan cheng\u0003, peng zhang\u0003, can li\u0003, qiao liang, yunlu xu, pengfei li, shiliang pu, yi niu, and\n",
            "fei wu\n",
            "abstract —recently, automatically extracting information from visually rich documents ( e.g., tickets and resumes) has become a hot\n",
            "and vital research topic due to its widespread commercial value. most existing methods divide this task into two subparts: the text\n",
            "reading part for obtaining the plain text from the original document images and the information extraction part for extracting key\n",
            "contents. these methods mainly focus on improving the second, while neglecting that the two parts are highly correlated. this paper\n",
            "proposes a uniﬁed end-to-end information extraction framework from visually rich documents, where text reading and information\n",
            "extraction can reinforce each other via a well-designed multi-modal context block. speciﬁcally, the text reading part provides\n",
            "multi-modal features like visual, textual and layout features. the multi-modal context block is developed to fuse the generated\n",
            "multi-modal features and even the prior knowledge from the pre-trained language model for better semantic representation. the\n",
            "information extraction part is responsible for generating key contents with the fused context features. the framework can be trained in\n",
            "an end-to-end trainable manner, achieving global optimization. what is more, we deﬁne and group visually rich documents into four\n",
            "categories across two dimensions, the layout and text type. for each document category, we provide or recommend the corresponding\n",
            "benchmarks, experimental settings and strong baselines for remedying the problem that this research area lacks the uniform evaluation\n",
            "standard. extensive experiments on four kinds of benchmarks (from ﬁxed layout to variable layout, from full-structured text to\n",
            "semi-unstructured text) are reported, demonstrating the proposed method’s effectiveness. data, source code and models are available.\n",
            "index terms —end-to-end, information extraction, text reading, multi-modal context, visually rich documents.\n",
            "f\n",
            "1 i ntroduction\n",
            "extracting information from visually rich document ( abbr.\n",
            "vrd) is a traditional yet very important research topic\n",
            "[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]. this is because\n",
            "automatically understanding vrds can greatly facilitate the\n",
            "key information entry, retrieval and compliance check in\n",
            "enormous and various applications, including ﬁle under-\n",
            "standing in court trial, contract checking in the business\n",
            "system, statements analysis in accounting or ﬁnancial, case\n",
            "recognition in medical applications, invoice recognition in\n",
            "reimburses system, resume recognition in recruitment sys-\n",
            "tem, and automatically examining test paper in education\n",
            "applications, etc.\n",
            "in general, a vrd information extraction system can\n",
            "be divided into two separated parts: text reading and key\n",
            "information extraction. text reading module refers to ob-\n",
            "taining text positions as well as their character sequence in\n",
            "document images, which falls into the computer vision areas\n",
            "related to optical character recognition ( abbr. ocr) [11],\n",
            "[12], [13], [14], [15], [16], [17], [18]. information extraction\n",
            "(abbr. ie) module is responsible for mining key contents\n",
            "(e.g., entity, relation) from the captured plain text, related\n",
            "\u000fz. cheng and f. wu are with college of computer science and\n",
            "technology, zhejiang university, hangzhou, 310058, china (e-mail:\n",
            "11821104@zju.edu.cn, wufei@cs.zju.edu.cn). z. cheng is also with\n",
            "hikvision research institute, hangzhou, 310051, china.\n",
            "\u000fp. zhang, c. li, l. qiao, y. xu, p. li, s. pu and y. niu are with\n",
            "hikvision research institute, hangzhou, 310051, china (email: zhang-\n",
            "peng23@hikvision.com, lican9@hikvision.com, qiaoliang6@hikvision.com,\n",
            "xuyunlu@hikvision.com, lipengfei27@hikvision.com, pushil-\n",
            "iang.hri@hikvision.com, niuyi@hikvision.com).\n",
            "\u0003z. cheng, p. zhang and c. li contributed equally to this research.\n",
            "document\n",
            "imagestructured\n",
            "output\n",
            "text\n",
            "reading\n",
            "information\n",
            "extraction\n",
            "multi -modal\n",
            "contextfig. 1. illustration of the proposed end-to-end vrd information extrac-\n",
            "tion framework. it consists of three sub-modules: the text reading part for\n",
            "generating text layout and character strings, and the information extrac-\n",
            "tion module for outputting key contents. the multi-modal context block\n",
            "is responsible for fully assembling visual, textual, layout features, and\n",
            "even language knowledge, and bridges the text reading and information\n",
            "extraction parts in an end-to-end trainable manner. dashed lines denote\n",
            "back-propagation.\n",
            "to natural language processing ( abbr. nlp) techniques like\n",
            "named entity recognition ( abbr. ner) [19], [20], [21] and\n",
            "question-answer [22], [23], [24].\n",
            "early works [4], [5] implement the vrd information\n",
            "extraction frameworks by directly concatenating an ofﬂine\n",
            "ocr engine and the downstream ner-based ie module,\n",
            "which completely discards the visual features and posi-\n",
            "tion/layout1information from images. however, as appear-\n",
            "ing in many applications [1], [4], [8], [9], [25], [26], vrds\n",
            "are usually organized with both semantic text features and\n",
            "ﬂexible visual structure features in a regular way. for better\n",
            "results, researchers should consider the key characteristics\n",
            "of documents into their techniques, such as layout, tabular\n",
            "1. note that, terms of ‘position’ and ‘layout’ are two different but\n",
            "highly relevant concepts. the former refers to the speciﬁc coordinate lo-\n",
            "cations of candidate text regions generated by text reading module. the\n",
            "later means the abstract spatial information ( e.g., position arrangement\n",
            "of text regions) derived from the generated position results via some\n",
            "embedding operations. thus, layout can be treated as the high-level of\n",
            "spatial information in document understanding. in the follow-up, we\n",
            "use term ‘layout’ instead of term ‘position’ as one kind of modality.arxiv:2207.06744v1  [cs.cv]  14 jul 20222\n",
            "structure, or even font size in addition to the plain text. then\n",
            "recent works begin to incorporate these characteristics into\n",
            "the ie module by embedding multi-dimensional informa-\n",
            "tion such as text content and their layouts [2], [3], [27], [28],\n",
            "[29], and even image features [30], [31], [32].\n",
            "unfortunately, all existing methods suffer from two main\n",
            "problems: first, multi-modality features (like visual, textual\n",
            "and even layout features) are essential for vrd information\n",
            "extraction, but the exploitation of the multi-modal features\n",
            "is limited in previous methods. contributions of different\n",
            "kinds of features should be addressed for the ie part. for an-\n",
            "other, text reading and ie modules are highly correlated, but\n",
            "their contribution and relations have rarely been explored.\n",
            "therein, the real bottleneck of the whole framework has not\n",
            "been addressed well.\n",
            "intuitively, it is a way to combine an ocr engine ( e.g.,\n",
            "tesseract [33]) with a general ie module into a end-to-\n",
            "end framework, while it has several practical issues. (1) it\n",
            "is irresponsible to apply the general ocr engines on the\n",
            "speciﬁc scenes directly due to the domain gap problems,\n",
            "leading to the dramatic accuracy drop. (2) a general and big\n",
            "ie model like layoutlm [30], [32] may help achieve better\n",
            "ie performance, which however brings extra computational\n",
            "cost. (3) the non-trainable pipeline strategies will bring\n",
            "extra model maintenance costs and sub-optimal problems.\n",
            "considering the above issues, in this paper, we propose\n",
            "a novel end-to-end information extraction framework from\n",
            "vrds, named as trie++. the workﬂow is as shown in\n",
            "figure 1. instead of focusing on information extraction task\n",
            "only, we bridge text reading and information extraction tasks\n",
            "via a developed multi-modal context block. in this way, two\n",
            "separated tasks can reinforce each other amidst a uniﬁed\n",
            "framework. speciﬁcally, the text reading module produces\n",
            "diversiform features, including layout features, visual fea-\n",
            "tures and textual features. the multi-modal context block\n",
            "fuses multi-modal features with the following steps: (1)\n",
            "layout features, visual features and textual features are\n",
            "ﬁrst fed into the multi-modal embedding module, obtain-\n",
            "ing their embedding representation. (2) considering the\n",
            "effectiveness of the language model like bert [27], a prior\n",
            "language knowledge absorption mechanism is developed\n",
            "to incorporate the robust semantic representations of pre-\n",
            "trained language model. (3) the embedded features are\n",
            "then correlated with the spatial-aware attention to learn the\n",
            "instance-level interactions. it means different text instances\n",
            "may have explicit or implicit interactions, e.g., the ‘total-\n",
            "key’ and ‘total-value’ in receipts are highly correlated.\n",
            "consequently, the multi-modal context block can pro-\n",
            "vide robust features for the information extraction mod-\n",
            "ule, and the supervisions in information extraction also\n",
            "contribute to the optimization of text reading. since all\n",
            "the modules in the network are differentiable, the whole\n",
            "network could be trained in a global optimization way.\n",
            "to the best of our knowledge, this is the ﬁrst end-to-end\n",
            "trainable framework. to implement it, it is also challenging\n",
            "when considering the fusion of multi-modal information,\n",
            "the global optimization or even prior language knowledge\n",
            "absorption into a framework simultaneously, yet working\n",
            "well on different kinds of documents.\n",
            "we also notice that it is difﬁcult to compare existing\n",
            "methods directly due to the different benchmarks used(most of them are private), the non-uniform evaluation\n",
            "protocols, and even various experimental settings. as is\n",
            "known to all, text reading [43] is a rapidly growing research\n",
            "area, attributing to its various applications and its uniform\n",
            "benchmarks and evaluation protocols. we here reckon that\n",
            "these factors may restrict the study of document under-\n",
            "standing. to remedy this problem, we ﬁrst analyze many\n",
            "kinds of documents, and then categorize vrds into four\n",
            "groups along the dimensions of layout and text type .layout\n",
            "refers to the relative position distribution of texts or text\n",
            "blocks, which contains two modes: the ﬁxed mode and\n",
            "the variable mode. the former connotes documents that\n",
            "follow a uniform layout format, such as passport and the\n",
            "national value-added tax invoice, while the latter means\n",
            "that documents may appear in different layouts. referring\n",
            "to [44], [45], we deﬁne text type into two modalities2: the\n",
            "structured and the semi-structured. in detail, the structured\n",
            "type means that document information is organized in a\n",
            "predetermined schema, i.e., the key-value schema of the\n",
            "document is predeﬁned and often tabular in style, which\n",
            "delimits entities to be extracted directly. for example, taxi\n",
            "invoices usually have quite a uniform tabular-like layout\n",
            "and information structure like ‘invoice number’, ‘total’,\n",
            "‘date’ etc. the semi-structured type connotes that document\n",
            "content is usually ungrammatical, but each portion of the\n",
            "content is not necessarily organized in a predetermined for-\n",
            "mat. for example, a resume may include some predeﬁned\n",
            "ﬁelds such as job experience and education information.\n",
            "within the job experience ﬁelds, the document may include\n",
            "free text to describe the person’s job experience. then, the\n",
            "user may desire to search on free text only within the\n",
            "job experience ﬁeld. table 1 summarizes the categories of\n",
            "visually rich documents from the previous research litera-\n",
            "ture. secondly, we recommend or provide the corresponding\n",
            "benchmarks for each kind of documents, and also provide\n",
            "the uniform evaluation protocols, experimental settings and\n",
            "strong baselines, expecting to promote this research area.\n",
            "major contributions are summarized as follows. (1) we\n",
            "propose an end-to-end trainable framework trie++ for ex-\n",
            "tracting information from vrds, which can be trained from\n",
            "scratch, with no need for stage-wise training strategies. (2)\n",
            "we implement the framework by simultaneously learning\n",
            "text reading and information extraction tasks via a well-\n",
            "designed multi-modal context block, and also verify the mu-\n",
            "tual inﬂuence of text reading and information extraction. (3)\n",
            "to make evaluations more comprehensive and convincing,\n",
            "we deﬁne and divide vrds into four categories, in which\n",
            "three kinds of real-life benchmarks are collected with full\n",
            "annotations. for each kind of document, we provide or rec-\n",
            "ommend the corresponding benchmarks, experimental set-\n",
            "tings, and strong baselines. (4) extensive evaluations on four\n",
            "kinds of real-world benchmarks show superior performance\n",
            "compared with the state-of-the-art. those benchmarks cover\n",
            "diverse types of document images, from ﬁxed to variable\n",
            "layouts, from structured to semi-unstructured text types.\n",
            "declaration of major extensions compared to the con-\n",
            "ference version [1]: (1) instead of modelling context with\n",
            "2. another text type, the unstructured, is also deﬁned in [44], which\n",
            "means that document content is grammatically free text without explicit\n",
            "identiﬁers such as books. since such documents usually lack visually\n",
            "rich elements ( e.g., layout), we exclude it from the concept of vrd.3\n",
            "table 1\n",
            "categories of visually rich document scenarios for information extraction. a majority of them have been studied in existing research works.\n",
            "layouttext\n",
            "type structured semi-structured\n",
            "fixedcategory i :\n",
            "value-added tax invoice [29], passport [34], ﬁxed-format taxi invoice [1],\n",
            "national id card [35], train ticket [7], [31], business license [26]category ii :\n",
            "business email [36],\n",
            "national housing contract\n",
            "variablecategory iii :\n",
            "medical invoice [8], [31], paper head [37], bank card [35],\n",
            "free-format invoice [1], [2], [4], [28], [38], [39], [40], business card [34],\n",
            "purchase receipt [3], [7], [25], [29], [31], purchase orders [5], [30]category iv :\n",
            "personal resume [1],\n",
            "ﬁnancial report [36],newspaper [41],\n",
            "free-format sales contract [42]\n",
            "only layout and textual features in [1], we here enhance the\n",
            "multi-modal context block by fusing three kinds of features\n",
            "(i.e., layout, visual and textual features) with a spatial-aware\n",
            "attention mechanism. besides, we expand the application\n",
            "ranges of our method, showing the ability to handle with\n",
            "four kinds of vrds. (2) following the suggestions in the\n",
            "conference reviews that the prior knowledge may be helpful\n",
            "to our method, we also attempt to introduce the pre-trained\n",
            "language model [27] into the framework with a knowledge\n",
            "absorption module for further improving the information\n",
            "extraction performance. (3) we address the problem of per-\n",
            "formance comparison in existing methods, and then deﬁne\n",
            "the four categories of vrds. to promote the document un-\n",
            "derstanding area, we recommend the corresponding bench-\n",
            "marks, experimental settings, and strong baselines for each\n",
            "kind of document. (4) we explore the effects of the proposed\n",
            "framework with more extensive experimental evaluations\n",
            "and comparisons, which demonstrates its advantages.\n",
            "2 r elated works\n",
            "thanks to the rapid expansion of artiﬁcial intelligence tech-\n",
            "niques [46], advanced progress has been made in many\n",
            "isolated applications such as document layout analysis [30],\n",
            "[47], scene text spotting [48], [49], video understanding [50],\n",
            "named entities identiﬁcation [51], question answering [52],\n",
            "or even causal inference [53] etc. however, it is crucial to\n",
            "build multiple knowledge representations for understand-\n",
            "ing the complex and challenging world. vrd information\n",
            "extraction is such a real task greatly helping ofﬁce au-\n",
            "tomation, which relies on integrating multiple techniques,\n",
            "including object detection, sequence learning, information\n",
            "extraction and even the multi-modal knowledge representa-\n",
            "tion. here, we roughly brief techniques as follows.\n",
            "2.1 text reading\n",
            "text reading belongs to the ocr research ﬁeld and has been\n",
            "widely studied for decades. a text reading system usually\n",
            "consists of two parts: text detection and text recognition.\n",
            "intext detection , methods are usually divided into two\n",
            "categories: anchor-based methods and segmentation-based\n",
            "methods. following faster r-cnn [54], anchor-based meth-\n",
            "ods [14], [55], [56], [57], [58], [59], [60], [61] predicted the\n",
            "existence of texts and regress their location offsets at pre-\n",
            "deﬁned grid points of the input image. to localize arbitrary-\n",
            "shaped text, mask rcnn [62]-based methods [63], [64],\n",
            "[65] were developed to capture irregular text and achieve\n",
            "better performance. compared to anchor-based methods,segmentation can easily be used to describe the arbitrary-\n",
            "shaped text. therefore, many segmentation-based meth-\n",
            "ods [66], [67], [68], [69] were developed to learn the pixel-\n",
            "level classiﬁcation tasks to separate text regions apart from\n",
            "the background. in text recognition , the encoder-decoder\n",
            "architecture [70], [71], [72] dominates the research ﬁeld,\n",
            "including two mainstreaming routes: ctc [73]-based [17],\n",
            "[61], [74], [75] and attention-based [71], [72], [76] methods.\n",
            "to achieve the global optimization between detection and\n",
            "recognition, many end-to-end trainable methods [11], [12],\n",
            "[13], [48], [49], [77], [78], [79], [80] were proposed, and\n",
            "achieved better results than the pipeline approaches.\n",
            "global optimization is one of the importance research\n",
            "trend exactly in general object detection and text spotting.\n",
            "hence, it is easy to observe its importance in extracting\n",
            "information from vrds. based on this, we attempt to de-\n",
            "velop an end-to-end trainable framework from text reading\n",
            "to information extraction. unlike text spotting methods\n",
            "achieving global optimization via a roi-like operations [13],\n",
            "[48] only, the proposed framework relies on an all-around\n",
            "module, i.e., the multi-modal context block, to bridge the\n",
            "vision tasks and the nlp task. this module should have\n",
            "the ability of both handling complex multi-modal informa-\n",
            "tion and providing differentiable passages among different\n",
            "modules.\n",
            "2.2 information extraction\n",
            "information extraction is a traditional research topic and\n",
            "has been studied for many years. here, we divide existing\n",
            "methods into two categories as follows.\n",
            "2.2.1 rule-based methods\n",
            "before the advent of learning-based models, rule-based\n",
            "methods [8], [9], [47], [81], [82], [83] dominated this research\n",
            "area. it is intuitive that the key information can be identi-\n",
            "ﬁed by matching a predeﬁned pattern or template in the\n",
            "unstructured text. therefore, expressive pattern matching\n",
            "languages [81], [82] were developed to analyze syntactic\n",
            "sentence, and then output one or multiple target values.\n",
            "to extract information from general documents such as\n",
            "business documents, many solutions [8], [9], [39], [47], [84]\n",
            "were developed by using the pattern matching approaches.\n",
            "in detail, [9], [39], [85] required a predeﬁned document\n",
            "template with relevant key ﬁelds annotated, and then auto-\n",
            "matically generated patterns matching those ﬁelds. [8], [47],\n",
            "[84] all manually conﬁgured patterns based on keywords,\n",
            "parsing rules or positions. the rule-based methods heavily\n",
            "rely on the predeﬁned template, and are limited to the4\n",
            "documents with unseen templates. as a result, it usually\n",
            "requires deep expertise and a large time cost to conduct the\n",
            "templates’ design and maintenance.\n",
            "2.2.2 learning-based methods\n",
            "learning-based methods can automatically extract key in-\n",
            "formation by applying machine learning techniques to a\n",
            "prepared training dataset.\n",
            "traditionally machine learning techniques like logistic\n",
            "regression and svm were widely adopted in document\n",
            "analysis tasks. [86] proposed a general machine learning\n",
            "approach for the hierarchical segmentation and labeling of\n",
            "document layout structures. this approach modeled docu-\n",
            "ment layout as grammar and performed a global search for\n",
            "the optimal parse based on a grammatical cost function. this\n",
            "method utilized machine learning to discriminatively select\n",
            "features and set all parameters in the parsing process.\n",
            "the early methods often ignore the layout information in\n",
            "the document, and then the document understanding task is\n",
            "downgraded to the pure nlp problem. that is, many named\n",
            "entity recognition (ner) based methods [20], [21], [51], [87],\n",
            "[88], [89] can be applied to extract key information from the\n",
            "one-dimensional plain text. inspired by this idea, [4] pro-\n",
            "posed cloudscan, an invoice analysis system, which used\n",
            "recurrent neural networks to extract entities of interest from\n",
            "vrds instead of templates of invoice layout. [5] proposed\n",
            "a token level recurrent neural network for end-to-end table\n",
            "ﬁeld extraction that starts with the sequence of document\n",
            "tokens segmented by an ocr engine and directly tags each\n",
            "token with one of the possible ﬁeld types. however, they\n",
            "discard the layout information during the text serialization,\n",
            "which is crucial for document understanding.\n",
            "observing the rich layout and visual information con-\n",
            "tained in document images, researchers tended to incor-\n",
            "porate more details from vrds. some works [2], [3], [26],\n",
            "[27], [28] took the layout into consideration, and worked\n",
            "on the reconstructed character or word segmentation of\n",
            "the document. concretely, [2] ﬁrst achieved a new type\n",
            "of text representation by encoding each document page\n",
            "as a two-dimensional grid of characters. then they devel-\n",
            "oped a generic document understanding pipeline named\n",
            "chargrid for structured documents by a fully convolutional\n",
            "encoder-decoder network. as an extension of chargrid,\n",
            "[27] proposed bertgrid in combination with a fully con-\n",
            "volutional network on a semantic instance segmentation\n",
            "task for extracting ﬁelds from invoices. to further explore\n",
            "the effective information from both semantic meaning and\n",
            "spatial distribution of texts in documents, [3] proposed a\n",
            "convolutional universal text information extractor by apply-\n",
            "ing convolutional neural networks on gridding texts where\n",
            "texts are embedded as features with semantic connotations.\n",
            "[28] proposed the attend, copy, parse architecture, an end-\n",
            "to-end trainable model bypassing the need for word-level\n",
            "labels. [26] proposed a tag, copy or predict network by\n",
            "ﬁrst modelling the semantic and layout information in 2d\n",
            "ocr results, and then learning the information extraction\n",
            "in a weakly supervised manner. contemporaneous with\n",
            "the above-mentioned methods, there are methods [25], [29],\n",
            "[30], [32], [38], [90], [91] which resort to graph modeling\n",
            "to learn relations between multimodal inputs. [29] intro-\n",
            "duced a graph convolution-based model to combine tex-tual and layout information presented in vrds, in which\n",
            "graph embedding was trained to summarize the context\n",
            "of a text segment in the document, and further combined\n",
            "with text embedding for entity extraction. [38] presented a\n",
            "representation learning approach to extract structured in-\n",
            "formation from templatic documents, which worked in the\n",
            "pipeline of candidate generation, scoring and assignment.\n",
            "[25] modelled document images as dual-modality graphs by\n",
            "encoding both textual and visual features, then generated\n",
            "key information with the proposed spatial dual-modality\n",
            "graph reasoning method (sdmg-r). besides, they also\n",
            "released a new dataset named wildreceipt.\n",
            "recently, some researchers attempted to train the large\n",
            "pre-trained model for better results. [30] proposed the lay-\n",
            "outlm to jointly model interactions between text and layout\n",
            "information across scanned document images. then [32]\n",
            "released layoutlmv2 where new model characteristics and\n",
            "pre-training tasks were leveraged. [90] also proposed a pre-\n",
            "trained model and leveraged cell-level layout information\n",
            "instead of token-level used in [30], [32]. [91] proposed a\n",
            "uniﬁed framework to handle the entity labeling and entity\n",
            "linking sub-tasks, using token and segment level context.\n",
            "[92] proposed lambert by modifying the transformer\n",
            "encoder architecture for obtaining layout features from an\n",
            "ocr system, without the need to re-learn language se-\n",
            "mantics from scratch. [93] proposed tilt which relies on\n",
            "a pretrained encoder-decoder transformer to learns layout\n",
            "information, visual features, and textual semantics. [94]\n",
            "developed selfdoc by exploiting the positional, textual,\n",
            "and visual information of a document and modeling their\n",
            "contextualization. however, these works rely on the big\n",
            "models trained on the large datasets ( e.g., iit-cdip [95]),\n",
            "which suffers from the computational cost problem in real\n",
            "applications. on the other hand, they also ignore the mutual\n",
            "impacts between text reading and information extraction\n",
            "modules. they need to recompute visual features used in ie\n",
            "module, leading to double-computing problems, and cannot\n",
            "exploit supervision from ie on text reading.\n",
            "2.3 end-to-end information extraction from vrds\n",
            "two related concurrent works were presented in [34], [96].\n",
            "[34] proposed an entity-aware attention text extraction net-\n",
            "work to extract entities from vrds. however, it could only\n",
            "process documents of relatively ﬁxed layout and structured\n",
            "text, like train tickets, passports and business cards. [96]\n",
            "localized, recognized and classiﬁed each word in the doc-\n",
            "ument. since it worked in the word granularity, it required\n",
            "much more labeling efforts (layouts, content and category\n",
            "of each word) and had difﬁculties extracting those enti-\n",
            "ties which were embedded in word texts ( e.g., extracting\n",
            "‘51xxxx@xxx.com’ from ‘153-xxx97 j51xxxx@xxx.com’). be-\n",
            "sides, in its entity recognition branch, it still worked on the\n",
            "serialized word features, which were sorted and packed in\n",
            "the left to right and top to bottom order. the two existing\n",
            "works are strictly limited to documents of relatively ﬁxed\n",
            "layout and one type of text (structured or semi-structured).\n",
            "similar to the conference version [1] of our method, [37]\n",
            "recently proposed an end-to-end information extraction\n",
            "framework accompanied by a chinese examination paper\n",
            "head dataset. unlike them, our method acts as a general5\n",
            "information  \n",
            "extraction\n",
            "multimodal context block\n",
            "blstm\n",
            "text reading\n",
            "text\n",
            "detection\n",
            "instance \n",
            "feature\n",
            "text \n",
            "recognition \n",
            "shared\n",
            "conv net\n",
            "x0\n",
            "y0\n",
            "..\n",
            "x0\n",
            "y0\n",
            "..\n",
            "x0\n",
            "y0\n",
            "..position  features\n",
            "x0\n",
            "y0\n",
            "..\n",
            "visual  features\n",
            "m\n",
            "a\n",
            "r\n",
            "t\n",
            "i\n",
            "..\n",
            "a\n",
            "b\n",
            "c\n",
            "1\n",
            "2\n",
            "..\n",
            "2\n",
            "0\n",
            "1\n",
            "8\n",
            ".\n",
            "..textual  features\n",
            "g\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            "..\n",
            "name : \n",
            "e-mail: \n",
            "period : \n",
            "...: \n",
            "global \n",
            "context\n",
            "modelling\n",
            "multimodal\n",
            "embedding\n",
            "local \n",
            "context\n",
            "modelling\n",
            "fig. 2. the overall framework. the network predicts text locations, text\n",
            "contents and key entities in a single forward pass.\n",
            "vrd information extraction framework, and can handle\n",
            "documents of both ﬁxed and variable layouts, structured\n",
            "and semi-structured text types.\n",
            "3 m ethodology\n",
            "this section introduces the proposed framework, which has\n",
            "three parts: text reading, multi-modal context block and\n",
            "information extraction module, as shown in figure 2.\n",
            "3.1 text reading\n",
            "text reading module commonly includes a shared convo-\n",
            "lutional backbone, a text detection branch as well as a\n",
            "text recognition branch. we use resnet-d [97] and feature\n",
            "pyramid network (fpn) [98] as our backbone to extract the\n",
            "shared convolutional features. for an input image x, we\n",
            "denoteias the shared feature maps.\n",
            "text detection . the branch takes ias input and predicts\n",
            "the locations of all candidate text regions, i.e.,\n",
            "b=detector (i) (1)\n",
            "where the detector can be the anchor-based [14], [55], [59],\n",
            "[60] or segmentation-based [66], [67], [68] text detection\n",
            "heads.b= (b1;b2;:::;b m)is a set of mtext bounding\n",
            "boxes, and bi= (xi0;yi0; xi1;yi1)denotes the top-left\n",
            "and bottom-right positions of the i-th text. in mainstream\n",
            "methods, roi-like operations ( e.g., roi-pooling [54] used in\n",
            "[77], roi-align [62] used in [78], roi-rotate used in [48], or\n",
            "even roi-based arbitrary-shaped transformation [11], [12])\n",
            "are applied on the shared convolutional features ito get\n",
            "their text instance features. here, the text instance features\n",
            "are denoted asc= (c1;c2;:::;c m). the detailed network\n",
            "architecture is shown in section 5.1.\n",
            "text recognition . the branch predicts a character se-\n",
            "quence from each text region features ci. firstly, each in-\n",
            "stance feature ciis fed into an encoder ( e.g., cnn and\n",
            "lstm [99]) to extract a higher-level feature sequence h=\n",
            "(h1;h2;:::;h l), wherelis the length of the extracted feature\n",
            "sequence. then, a general sequence decoder ( e.g., attention-\n",
            "based [17], [72]) is adopted to generate the sequence of\n",
            "charactersy= (y1;y2;:::;y t), wheretis the length of\n",
            "label sequence. details are shown in section 5.1.\n",
            "we choose attention-based sequence decoder as the\n",
            "character recognizer. it is a recurrent neural network thatdirectly generates the character sequence yfrom an input\n",
            "feature sequence h. to generate the t-th character, the\n",
            "attention weight \u000btand the corresponding glimpse vec-\n",
            "torgtare computed as gt=pl\n",
            "k=1\u000bt;khkwhere\u000bt;k=\n",
            "exp(et;k)=pl\n",
            "j=1exp(et;j). hereet;j=wttanh(wsst\u00001+\n",
            "whhj+b), andw,ws,whandbare trainable weights. then\n",
            "the hidden state st\u00001is updated via,\n",
            "st=lstm (st\u00001;gt;yt\u00001): (2)\n",
            "finally,stis taken for predicting the current-step character,\n",
            "i.e.,p(yt) =softmax (wost+bo)where both woandboare\n",
            "learnable weights.\n",
            "3.2 multi-modal context block\n",
            "we design a multi-modal context block to consider layout\n",
            "features, visual features and textual features altogether.\n",
            "different modalities of information are complementary to\n",
            "each other, and fully fused for providing robust multi-modal\n",
            "feature representation.\n",
            "3.2.1 multi-modal feature generation\n",
            "document details such as the apparent color, font, layout\n",
            "and other informative features also play an important role\n",
            "in document understanding.\n",
            "a natural way of capturing the layout and visual\n",
            "features of a text is to resort to the convolutional neu-\n",
            "ral network. concretely, the position information of each\n",
            "text instance is obtained from the detection branch, i.e.,\n",
            "b= (b1;b2;:::;b m). for visual feature, different from [30],\n",
            "[32] which extract these features from scratch, we directly\n",
            "reuse text instance features c= (c1;c2;:::;c m)by text\n",
            "reading module as the visual features. thanks to the deep\n",
            "backbone and lateral connections introduced by fpn, each\n",
            "cisummarizes the rich local visual patterns of the i-th text.\n",
            "in sequence decoder, give the i-th text instance, its rep-\n",
            "resented feature of characters before softmax contain rich\n",
            "semantic information. for the attention-based decoder, we\n",
            "can directly use zi= (s1;s2;:::;s t)as its textual features.\n",
            "3.2.2 prior knowledge absorption\n",
            "since pre-trained language model contains general language\n",
            "knowledge like semantic properties, absorbing knowledge\n",
            "from the language model may help improve the perfor-\n",
            "mance of information extraction. compared to the confer-\n",
            "ence paper [1], we here attempt to bring the language model\n",
            "into our framework. however, prior language information\n",
            "has different contributions on different vrds. for example,\n",
            "on resume scenario that require semantics, prior language\n",
            "information contributes more, while on taxi scenario which\n",
            "requires less semantics, prior language information con-\n",
            "tributes less. inspired by the gating operation in lstm\n",
            "[99], we design a gated knowledge absorption mechanism\n",
            "to adjust the prior knowledge ﬂows in our framework, as\n",
            "shown in figure 3.\n",
            "concretely, we ﬁrst transform textual input zinto vo-\n",
            "cabulary space via a linear layer, then obtain the character\n",
            "sequence by conducting argmax operation ( ^), i.e.,\n",
            "z0=^(linear (z)): (3)6\n",
            "δ \n",
            " ＋\n",
            "z\n",
            "σzz\n",
            "o\n",
            "language\n",
            "model\n",
            "z' \n",
            "λ \n",
            "linearz\n",
            "a ɡ' \n",
            "r' \n",
            "fig. 3. the knowledge absorption mechanism. \u001band\u000eseparately mean\n",
            "the sigmoid and tanh operations. \u0002and +refer to the element-wise\n",
            "multiplication and addition, respectively. ^means the argmax operation.\n",
            "thenz0is fed into the language model, outputting its\n",
            "knowledge representation a.\n",
            "in order to dynamically determine the degree of depen-\n",
            "dency of the pre-trained model, we use an on-off gate g0\n",
            "g0=\u001b(wg0a+ug0z+bg0) (4)\n",
            "to balance the ﬂow of the prior knowledge activation r0\n",
            "r0=\u000e(wr0a+ur0z+br0): (5)\n",
            "here, the gate is used for determining whether general\n",
            "knowledge is needed. then the modulated textual feature\n",
            "ois calculated as\n",
            "o=g0\fr0+woz: (6)\n",
            "3.2.3 multi-modal context modelling\n",
            "we ﬁrst embed each modality information into feature se-\n",
            "quences with the same dimension, and fuse them with a\n",
            "normalization layer. inspired by the powerful transformer\n",
            "[32], [87], [100], [101], the self-attention mechanism is used\n",
            "to build deep relations among different modalities, whose\n",
            "input consists of queries qand keyskof dimension dk,\n",
            "and valuesvof dimension dv.\n",
            "multi-modal feature embedding given a document\n",
            "withmtext instance, we can capture the inputs of po-\n",
            "sitionb= (b1;b2;:::;b m), the inputs of visual feature\n",
            "c= (c1;c2;:::;c m)and the inputs of modulated textual\n",
            "featureo= (o1;o2;:::;o m).\n",
            "since position information provides layout information\n",
            "of documents, we introduce a position embedding layer to\n",
            "preserve layout information, for the i-th text instance in a\n",
            "document,\n",
            "pei=jbijx\n",
            "j=1embedding (bij); (7)\n",
            "whereembedding is a learnable embedding layer, bi=\n",
            "(xi0;yi0;xi1;yi1)andpei2rde.\n",
            "forcivisual feature, we embed it using a convolutional\n",
            "neural network layer with the same shape of pei,\n",
            "bci=convnet c(ci): (8)\n",
            "foroitextual feature, a convnet of multiple kernels\n",
            "similar to [102] is used to aggregate semantic character\n",
            "features inoiand outputs bzi2rde,\n",
            "bzi=convnet z(oi): (9)then, thei-th text’s embedding is fused of bci,bziandpei,\n",
            "followed by the layernorm normalization, deﬁned as\n",
            "embi=layernorm (bci+bzi+pei): (10)\n",
            "afterwards, we pack all the texts’ embedding vector to-\n",
            "gether, i.e., emb = (emb 1;emb 2;:::;emb m), which serves\n",
            "as thek,qandvin the scaled dot-product attention.\n",
            "spatial-aware self-attention to better learn pair-wise\n",
            "interactions between text instances, we use the spatial-\n",
            "aware self-attention mechanism instead of the original\n",
            "self-attention, and the correlative context features ec=\n",
            "(ec1;ec2;:::;fcm)are obtained by,\n",
            "ec=attention (q;k;v )\n",
            "=softmax (qkt\n",
            "pdinfo+pe\u0001b)v(11)\n",
            "wheredinfo is the dimension of text embedding, andpdinfo is the scaling factor. pe\u0001brefers to the spatial-aware\n",
            "information, and is calculated by embedding features of\n",
            "position relations \u0001bamong different text instances in b,\n",
            "i.e.,pe\u0001b=embedding (\u0001b). here, \u0001bis deﬁned as\n",
            "\u0001b=2\n",
            "6640b1\u0000b2\u0001\u0001\u0001b1\u0000bm\n",
            "b2\u0000b1 0\u0001\u0001\u0001b2\u0000bm\n",
            "\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001\n",
            "bm\u0000b1bm\u0000b2\u0001\u0001\u0001 03\n",
            "775: (12)\n",
            "to further improve the representation capacity of the at-\n",
            "tended feature, multi-head attention is introduced. each\n",
            "head corresponds to an independent scaled dot-product\n",
            "attention function and the text context features ecis given\n",
            "by:\n",
            "ec=multihead (q;k;v )\n",
            "= [head 1;head 2;:::;head n]winfo(13)\n",
            "head j=attention (qwq\n",
            "j;kwk\n",
            "j;vwv\n",
            "j) (14)\n",
            "wherewq\n",
            "j,wk\n",
            "jandwv\n",
            "j2r(dinfo\u0002dn)are the learned\n",
            "projection matrix for the j-th head,nis the number of\n",
            "heads, and winfo2r(dinfo\u0002dinfo). to prevent the multi-\n",
            "head attention model from becoming too large, we usually\n",
            "havedn=dinfo\n",
            "n.\n",
            "context fusion both the multi-modal context and tex-\n",
            "tual features matter in entity extraction. the multi-modal\n",
            "context features ( ec) provide necessary information to tell\n",
            "entities apart while the textual features oenable entity\n",
            "extraction in the character granularity, as they contain se-\n",
            "mantic features for each character in the text. thus, we need\n",
            "to fuse them further. that is, for the i-the text instance, we\n",
            "pack its multi-modal context vector eciand its modulated\n",
            "textual features oitogether along the channel dimension,\n",
            "i.e.,(ui1;ui2;:::;u it)whereuij= [oi;j;ci].\n",
            "3.3 information extraction\n",
            "then, a bidirectional-lstm is applied to further model the\n",
            "long dependencies within the characters,\n",
            "h0\n",
            "i= (h0\n",
            "i;1;h0\n",
            "i;2;:::;h0\n",
            "i;t) =bilstm (ui); (15)7\n",
            "which is followed by a fully connected network and a\n",
            "conditional random ﬁeld ( abbr, crf) layer, projecting the\n",
            "output to the dimension of iob3[103] label space.\n",
            "pinfo\n",
            "i;j=crf (linear (h0\n",
            "i;j)) (16)\n",
            "note that, unlike the spatial-aware self-attention learn-\n",
            "ing pair-wise interactions between text instances ( e.g,the\n",
            "‘phone number’ and ‘name’), bilstm is for depicting the\n",
            "relations among the characters in each instance ( e.g, the\n",
            "numbers in ‘phone number’ or characters in ‘name’).\n",
            "3.4 optimization\n",
            "the proposed network can be trained in an end-to-end\n",
            "manner and the losses are generated from three parts,\n",
            "l=ldet+\u0015recoglrecog +\u0015infolinfo (17)\n",
            "where hyper-parameters \u0015recog and\u0015info control the trade-\n",
            "off between losses.\n",
            "ldetis the loss of text detection branch, which can\n",
            "be formulated as different forms according to the selected\n",
            "detection heads. taking faster-rcnn [54] as the detection\n",
            "head, the detection part consists of a classiﬁcation loss and\n",
            "a regression loss.\n",
            "for sequence recognition part, the attention-based recog-\n",
            "nition loss is\n",
            "lrecog =\u00001\n",
            "tmx\n",
            "i=1tx\n",
            "t=1logp (^yi;tjh); (18)\n",
            "where ^yi;tis the ground-truth label of t-th character in i-th\n",
            "text from recognition branch.\n",
            "the information extraction loss is the crfloss, as used\n",
            "in [20], [37]. speciﬁcally, for the i-th text instance, the input\n",
            "sequence of lstm is ui= (ui1;ui2;:::;u it)2rt\u0002din,\n",
            "while the output is h0\n",
            "i2rt\u0002\n",
            ", where \n",
            "denotes the num-\n",
            "ber of entities. h0(we omit the subscript ifor convenience)\n",
            "denotes the scores of emissions matrix, and h0\n",
            "t;jrepresents\n",
            "the score of the j\u0000th entity of the t\u0000th character in ui. for\n",
            "a sequence of predictions y= (y1;y2;:::;y t), its scores can\n",
            "be deﬁned as follows\n",
            "s(ui;y) =tx\n",
            "j=0ayj;yj+1+tx\n",
            "j=1h0\n",
            "j;yj; (19)\n",
            "whereais a matrix of transition scores such that ai;j\n",
            "represents the score of a transition from the tag ito tagj.y0\n",
            "andytare the start and end tags of a sentence, that we add\n",
            "to the set of possible tags. ais therefore a square matrix of\n",
            "size\n",
            "+2 . a softmax over all possible tag sequences yields a\n",
            "probability for the sequence y, i.e.,p(yjui) =es(ui;y)\n",
            "p\n",
            "ey2y(ui)es(ui;ey)\n",
            "where y(ui)is all possible entity sequences for ui.\n",
            "during training, we minimize the negative log-\n",
            "likelihood estimation of the correct entity sequence as:\n",
            "linfo=\u0000log(p(yjui)) =\u0000s(ui;y) +log(x\n",
            "ey2y(ui)es(ui;ey)):\n",
            "(20)\n",
            "3. here, ‘i’ stands for ‘inside’, signifying that the word is inside a\n",
            "named entity. ‘o’ stands for ‘outside’, referring to the word is just\n",
            "a regular word outside of a named entity. ‘b’ stands for ‘beginning’,\n",
            "signifying beginning of a named entity.while decoding, we predict the output sequence that ob-\n",
            "tains the maximum score given by y\u0003= arg max\n",
            "ey2y(ui)p(eyjui).\n",
            "note that since text reading and information extrac-\n",
            "tion modules are bridged with the multi-modal context\n",
            "block, they can reinforce each other. speciﬁcally, the multi-\n",
            "modality features of text reading are fully fused and es-\n",
            "sential for information extraction. at the same time, the\n",
            "semantic feedback of information extraction also contributes\n",
            "to the optimization of the shared convolutions and text\n",
            "reading module.\n",
            "4 b enchmarks\n",
            "as addressed in section 1, most existing works verify their\n",
            "methods on private datasets due to their privacy policies. it\n",
            "leads to difﬁculties for fair comparisons between different\n",
            "approaches. though existing datasets like sroie [104] have\n",
            "been released, they mainly fall into category iii, i.e., doc-\n",
            "uments with variable layout and structured text type. the\n",
            "remaining three kinds of application scenarios (category i,\n",
            "ii and iv) have not been studied well because of the limited\n",
            "real-life datasets.\n",
            "4.1 dataset inventory\n",
            "to boost the research of vrd understanding, we here extend\n",
            "the benchmarks of vrd, especially on category i, ii and iv .\n",
            "table 2 shows the detailed statistics of these benchmarks.\n",
            "\u000fcategory i refers to document images with uniform\n",
            "layout and structured text type, which is very com-\n",
            "mon in everyday life. contrastively, its research\n",
            "datasets are very limited due to various privacy poli-\n",
            "cies. here, we ﬁnd only two available benchmarks,\n",
            "i.e., train ticket and passport dataset released by [34],\n",
            "which are generated with a synthetic data engine and\n",
            "provide only entity-level annotations. to remedy this\n",
            "issue, we release a new real-world dataset containing\n",
            "5000 taxi invoice images. except for providing the\n",
            "text position and character string information for\n",
            "ocr tasks (text detection and recognition), entity-\n",
            "level labels including 9 entities (invoice code, invoice\n",
            "number, date, get-on time, get-off time, price, dis-\n",
            "tance, wait time, total) are also provided. besides,\n",
            "this dataset is very challenging, as many images are\n",
            "in low-quality (such as blur and occlusion).\n",
            "\u000fcategory ii refers to those documents with ﬁxed\n",
            "layout and semi-structured text type, like business\n",
            "email or national housing contract. ner datasets\n",
            "like cluener2020 [107] are only collected for nlp\n",
            "tasks, and they provide only semantic content while\n",
            "ignoring the important layout information. as ad-\n",
            "dressed in section 1, the joint study of ocr and ie is\n",
            "essential. unfortunately, we have not found available\n",
            "datasets that contains both ocr and ie annotations.\n",
            "we also ascribe the issue to various privacy policies.\n",
            "we here collect a new business email dataset from\n",
            "rvl-cdip [36], which has 1645 email images with\n",
            "35346 text instances and 15 entities (to, from, cc,\n",
            "subject, bcc, text, attachment, date, to-key, from-\n",
            "key, cc-key, subject-key, bcc-key, attachment-key,\n",
            "date-key).8\n",
            "table 2\n",
            "statistics of popular vrd benchmarks. pos, text andentity mean the word position, character string and entity-level annotations, respectively.\n",
            "term ‘syn’ and ‘real’ separately mean the dataset is generated by a synthetic data engine and manual collection. the new collected datasets are\n",
            "available at https://davar-lab.github.io/dataset/vrd.html .\n",
            "category dataset #training #validation #testing #entity #instance annotation type source\n",
            "i train ticket [34] 271.44k 30.16k 400 5 - [ entity ] syn\n",
            "passport [34] 88.2k 9.8k 2k 5 - [ entity ] syn\n",
            "taxi invoice 4000 - 1000 9 136,240 [ pos, text, entity ] real\n",
            "ii business email 1146 - 499 16 35,346 [ pos, text, entity ] real\n",
            "iii sroie [104] 626 - 347 4 52,451 [ pos, text, entity ] real\n",
            "business card [34] 178.2k 19.8k 2k 9 - [ entity ] syn\n",
            "funsd [105] 149 - 50 4 31,485 [ pos, text, entity ] real\n",
            "cord3[106] 800 100 100 30 - [ pos, text, entity ] real\n",
            "ephoie [37] 1183 - 311 12 15,771 [ pos, text, entity ] real\n",
            "wildreceipt [25] 1267 - 472 26 69,000 [ pos, text, entity ] real\n",
            "iv kleister-nda [42] 254 83 203 4 - [ pos, text, entity ] real\n",
            "resume 1045 - 482 11 82,800 [ pos, text, entity ] real\n",
            "\u000fcategory iii means documents which are with vari-\n",
            "able layout and structured text type like purchase\n",
            "receipt dataset sroie [104]. these datasets are usu-\n",
            "ally composed of small documents ( e.g., purchase\n",
            "receipts, business cards, etc.), and entities are orga-\n",
            "nized in a predetermined schema. we note that most\n",
            "previous literature focus on this category. we here\n",
            "list ﬁve available datasets. sroie is a scanned receipt\n",
            "dataset widely evaluated in many methods, which is\n",
            "fully annotated and provides text position, character\n",
            "string and key-value labels. business card is a synthe-\n",
            "sized dataset released by [34], and has only key-value\n",
            "pair annotations without ocr annotations. funsd\n",
            "[105] is a dataset aiming at extracting and structuring\n",
            "the textual content from noisy scanned forms. it\n",
            "has only 199 forms with four kinds of entities, i.e.,\n",
            "question, answer, header and other. cord2[106]\n",
            "is a consolidated receipt dataset, in which images\n",
            "are with text position, character string and multi-\n",
            "level semantic labels. ephoie [37] is a chinese ex-\n",
            "amination paper head dataset, in which each image\n",
            "is cropped from the full examination paper. this\n",
            "dataset contains handwritten information, and is also\n",
            "fully annotated. wildreceipt [25] is a large receipt\n",
            "dataset collected from document images of unseen\n",
            "templates in the wild. it contains 25 key information\n",
            "categories, a total of about 69000 text boxes.\n",
            "\u000fcategory iv means documents that have variable\n",
            "layout and semi-structured text type. different from\n",
            "those datasets in category iii, kleister-nda [42]\n",
            "aims to understand long documents (i.e., non-\n",
            "disclosure agreements document), but it provides\n",
            "only 540 documents with four general entity classes.\n",
            "to enrich benchmarks in this category, we release a\n",
            "large-scale resume dataset, which has 1527 images\n",
            "with ten kinds of entities(name, time, school, de-\n",
            "gree, specialty, phone number, e-mail, birth, title, se-\n",
            "curity code). since resumes are personally designed\n",
            "and customized, it is a classic document dataset with\n",
            "variable layouts and semi-structured text.4.2 challenges in different kinds of documents\n",
            "it will be the most straightforward task to extract entities\n",
            "from documents in category i, which attributes to its com-\n",
            "plete ﬁxed layout and structured text type. for this kind\n",
            "of documents, challenges are mainly from the text reading\n",
            "part, such as the distorted interference. the standard object\n",
            "detection methods like faster-rcnn [54] also can be further\n",
            "developed to handle this task. in category ii, the layout is\n",
            "ﬁxed, but the text is semi-structured. thus, in addition to\n",
            "modelling layout information, we also should pay attention\n",
            "to mining textual information. then some nlp techniques\n",
            "like the pre-trained language model can be exploited. as to\n",
            "the text reading part, long text recognition is also challeng-\n",
            "ing. documents in category iii face the problem of complex\n",
            "layout. thus the layout modelling methods [29], [31] like\n",
            "graph neural networks are widely developed for coping\n",
            "with this issue. the documents in category iv are in the face\n",
            "of both complex layout and nlp problems, which becomes\n",
            "the most challenging task.\n",
            "we hope that the combed benchmarks can promote the\n",
            "increasingly important research topic. we want validate our\n",
            "model as well as some state-of-the-arts on the representative\n",
            "datasets, and also provide a group of solid baselines (see\n",
            "section 5.4) for this research community.\n",
            "5 e xperiments\n",
            "in subsection 5.1, we ﬁrst introduce the implementation\n",
            "details of network and training skills. in subsection 5.2,\n",
            "we perform ablation study to verify the effectiveness of\n",
            "the proposed method on four kinds of vrd datasets, i.e.,\n",
            "taxi invoice, business email, wildreceipt and resume.\n",
            "in subsection 5.3, we compare our method with existing\n",
            "approaches on several recent datasets like funsd, sroie,\n",
            "ephoie and wildreceipt, demonstrating the advantages of\n",
            "the proposed method. then, we provide a group of strong\n",
            "baselines on four kinds of vrds in subsection 5.4. finally,\n",
            "we discuss the challenges of the different categories of\n",
            "3. in the original paper, cord claim that 11,000 receipt images were\n",
            "collected with 54 sub-classes. up to now, only 1000 images have been\n",
            "released in total.9\n",
            "documents. codes and models are available at https://davar-\n",
            "lab.github.io/publication/trie++.html .\n",
            "5.1 implementation details\n",
            "5.1.1 data selecting\n",
            "to facilitate end-to-end document understanding ( text read-\n",
            "ingand information extraction ), datasets should have posi-\n",
            "tion, text and entity annotations. hence, we only consider\n",
            "those datasets which satisfy the above requirement. on the\n",
            "ablation and strong baseline experiments, we select one\n",
            "classic dataset from each category, which has the largest\n",
            "number of samples. they are taxi invoice dataset from\n",
            "category i, business email dataset from category ii, wil-\n",
            "dreceipt dataset from category iii and resume dataset from\n",
            "category iv . when compared with the state-of-the-arts,\n",
            "since they mainly report their results on popular sroie,\n",
            "funsd and ephoie benchmarks, we also include these\n",
            "benchmarks in section 5.3.\n",
            "5.1.2 network details\n",
            "the backbone of our model is resnet-d [97], followed by\n",
            "the fpn [98] to further enhance features. the text detection\n",
            "branch in text reading module adopts the faster r-cnn\n",
            "[54] network and outputs the predicted bounding boxes of\n",
            "possible texts for later sequential recognition. for each text\n",
            "region, its features are extracted from the shared convolu-\n",
            "tional features by roialign [62]. the shapes are represented\n",
            "as32\u0002256for taxi invoice and wildreceipt, and 32\u0002512\n",
            "for business email and resume. then, features are further\n",
            "decoded by lstm-based attention [72], where the number\n",
            "of hidden units is set to 256.\n",
            "in the multimodal context block , bert [87] is used as\n",
            "the pre-trained language model. then, convolutions of four\n",
            "kernel size [3;5;7;9]followed by max pooling are used\n",
            "to extract ﬁnal textual features. the dimension of linear\n",
            "layer in equation 3 is set as \n",
            "\u0002256, where \n",
            "denotes the\n",
            "vocabulary space.\n",
            "in the information extraction module , the number of hidden\n",
            "units of bilstm used in entity extraction is set to 128.\n",
            "hyper-parameters \u0015recog and\u0015info in equation 17 are all\n",
            "empirically set to 1 in our experiments.\n",
            "5.1.3 training details\n",
            "our model and its counterparts are implemented under the\n",
            "pytorch framework [108]. for our model, the adamw [109]\n",
            "optimization is used. we set the learning rate to 1e-4 at the\n",
            "beginning and decreased it to a tenth at 50, 70 and 80 epochs.\n",
            "the batch size is set to 2 per gpu. for the counterparts,\n",
            "we separately train text reading and information extraction\n",
            "tasks until they are fully converged. all the experiments are\n",
            "carried out on a workstation with 8 nvidia a100 gpus.\n",
            "5.1.4 evaluation protocols\n",
            "we also note that different evaluation protocols are adopted\n",
            "in previous works. for example in the evaluation of infor-\n",
            "mation extraction part, both eaten [34] and pick [31] used\n",
            "the deﬁned mean entity accuracy (mea) and mean entity\n",
            "f1-score (mef) as metrics. cutie [3] adopted the average\n",
            "precision (ap) as the metric, and chargrid [2] developed\n",
            "new evaluation metric like word error rate for evaluation.while the majority of methods [1], [30], [42] used the f1-\n",
            "score as the evaluation metric. as a result, the non-uniform\n",
            "evaluation protocols bring extra difﬁculties on comparisons.\n",
            "therefore, we attempt to describe a group of uniform evalu-\n",
            "ation protocols for vrd understanding by carefully analyz-\n",
            "ing previous methods, including the evaluation protocols of\n",
            "text reading and information extraction parts.\n",
            "text reading falls into the ocr community, and it has\n",
            "uniform evaluation standards by referring to mainstream\n",
            "text detection [14], [48], [65] and text recognition [70], [71],\n",
            "[72] methods. precision (abbr. pre d) and recall (abbr. rec d)\n",
            "are used to measure performance of text localization, and\n",
            "f-measure (abbr. fd-m) is the harmonic average of precision\n",
            "and recall . to evaluate text recognition, the accuracy (abbr.\n",
            "acc) used in [70], [71], [72] is treat as its measurement\n",
            "metric. when evaluating the performance of end-to-end text\n",
            "detection and recognition, the end-to-end level evaluating\n",
            "metrics like precision (denoted by pre r), recall (denoted\n",
            "by rec r) and f-measure (denoted by f r-m) following [110]\n",
            "without lexicon is used, in which all detection results are\n",
            "considered with an iou >0.5.\n",
            "for information extraction, we survey the evaluation\n",
            "metrics from recent research works [1], [29], [30], [32], [37],\n",
            "[42], [105], and ﬁnd that the precision, recall and f1-score of\n",
            "entity extraction are widely used. hereby, we recommend\n",
            "theentity precision (abbr. epre), entity recall (abbr. erec) and\n",
            "entity f1-score (ef1) as the evaluation metrics for this task.\n",
            "5.2 ablation study\n",
            "in this section, we perform the ablation study on taxi\n",
            "invoice, business email, wildreceipt and resume datasets\n",
            "to verify the effects of different components in the proposed\n",
            "framework.\n",
            "5.2.1 effects of multi-modality features\n",
            "to examine the contributions of visual, layout and textual\n",
            "features to information extraction, we perform the following\n",
            "ablation study on four kinds of datasets, and the results\n",
            "are shown in table 3. textual feature means that entities are\n",
            "extracted using features from the text reading module only.\n",
            "since the layout information is completely lost, this method\n",
            "presents the worst performance. introducing either the vi-\n",
            "sual features orlayout features brings signiﬁcant performance\n",
            "gains. further fusion of the above multi-modality features\n",
            "gives the best performance, which veriﬁes the effects. we\n",
            "also show examples in figure. 4 to verify their effects. by\n",
            "using the textual feature only, the model misses the ‘store-\n",
            "name’ and has confusion between ‘total’ and ‘product-\n",
            "price’ entities. combined with the layout feature , the model\n",
            "can recognize ‘product-price’ correctly. when combined\n",
            "with the visual feature , the model can recognize store-name,\n",
            "because the visual feature contains obvious visual clues such\n",
            "as the large font size. it shows the best result by integrating\n",
            "all modality features.\n",
            "5.2.2 effects of different components\n",
            "here we attempt to analyze the effects of the spatial-aware\n",
            "self-attention mechanism, pre-trained language model, the\n",
            "gating mechanism and bi-lstm in ie on four kinds of\n",
            "datasets, as shown in table 4.10\n",
            "textual textual + layout textual + visual textual + layout + visual groundtruth\n",
            "fig. 4. illustration of modality contributions. different colors denote different entities, such as store-name, date, time, product-item, product-\n",
            "quantity, product-price, total. best viewed in color.\n",
            "table 3\n",
            "accuracy results (ef1) with multi-modal features on information\n",
            "extraction.\n",
            "textual featurep p p p\n",
            "layout featurep p\n",
            "visual featurep p\n",
            "taxi invoice 90.34 98.45 98.71 98.73\n",
            "business email 74.51 82.88 86.02 87.33\n",
            "wildreceipt 72.9 87.75 83.62 89.62\n",
            "resume 76.73 82.26 82.62 83.16\n",
            "table 4\n",
            "accuracy results (ef1) with different components.\n",
            "original self-attentionp\n",
            "spatial-aware self-attentionp p p\n",
            "language model (bert)p\n",
            "language model (llm v2)p\n",
            "taxi invoice 98.54 98.56 98.73 98.52\n",
            "business email 85.25 85.73 87.33 86.14\n",
            "wildreceipt 85.11 88.84 89.62 90.10\n",
            "resume 80.43 80.48 83.16 82.57\n",
            "evaluation of spatial-aware self-attention (short by\n",
            "sasa). sasa is devoted to learn layout features to improve\n",
            "the ﬁnal performance. from table 4, we see that sasa can\n",
            "boost performance, especially on the wildreceipt. this is\n",
            "because, compared to the original self-attention using enti-\n",
            "ties’ absolute positions only, the spatial-aware self-attention\n",
            "also makes use of relative position offsets between entities,\n",
            "and learns their pairwise relations. visual examples are\n",
            "shown in figure. 5. we see that ‘product-item’ and ‘product-\n",
            "price’ always appear in pairs. spatial-aware self-attention\n",
            "can capture such pairwise relations and then improve model\n",
            "performances. its attention map is visualized in figure. 6,\n",
            "which demonstrates that the spatial-aware self-attention\n",
            "indeed learns the pairwise relations between entities ( e.g.,\n",
            "pair of ‘total-key’ and ‘total-value’, and pair of ‘product-\n",
            "item’ and ‘product-price’).\n",
            "we also note that improvements by sasa is not always\n",
            "signiﬁcant on all benchmarks like taxi invoice and resume.\n",
            "this is because they are dominated with similar layouts\n",
            "(e.g., see the layout distribution of taxi invoice in figure. 7).\n",
            "original self -attention\n",
            " spatial -aware self -attention groundtruthfig. 5. visual examples of original self-attention and spatial-aware self-\n",
            "attention. different colors denote different entities, such as store-name,\n",
            "date, time, product-item, product-quantity, product-price, total. best\n",
            "viewed in color.\n",
            "fig. 6. visualization of spatial-aware self-attention. total-key (total)\n",
            "and total-value ( $49:70), product-item (b8-20top) and product-price\n",
            "(6:40) always appear together, and their pairwise relations can be\n",
            "learned. best viewed in color and zoom in to observe other pairwise\n",
            "relations.\n",
            "thus, effectiveness of sasa on diversity of layouts has not\n",
            "been measured well. while for wildreceipt with complex\n",
            "layouts, the effects are well demonstrated.\n",
            "to further demonstrate effects of sasa, we supplement\n",
            "experiments on the complex scenes constructed by ran-\n",
            "domly zooming out the foreground over the background11\n",
            "taxi invoice business email\n",
            "wildreceipt resume\n",
            "fig. 7. layout distribution on four kinds of benchmarks, where positions\n",
            "of entity of ‘code’, ‘to’, ‘date’ and ‘school’ are projected on their canvas.\n",
            "table 5\n",
            "accuracy results (ef1) by sasa on more complex scenes.\n",
            "strategy taxi invoice business email wildreceipt resume\n",
            "wo.sasa 98.45 79.93 82.28 75.96\n",
            "w.sasa 98.55 83.23 87.63 78.55\n",
            "from 1.2x to 3x. table 5 shows the results, which denotes\n",
            "that sasa can signiﬁcantly improve the ﬁnal performance\n",
            "without any side effects.\n",
            "evaluation of pre-trained language models. when\n",
            "introducing the prior knowledge from bert [87], the perfor-\n",
            "mance of information extraction is signiﬁcantly improved\n",
            "on the scenarios that require semantics like wildreceipt,\n",
            "business email and resume. as shown in figure 8, in the\n",
            "resume case, introducing the pre-trained language model\n",
            "helps recognize ‘school’ and ‘specialty’ entities, which are\n",
            "hard to be extracted solely using textual features.\n",
            "we also attempt to apply layoutlm v2 as the prior\n",
            "model, showing effective improvements on most of scenes.\n",
            "however, it is not always better than results with bert on\n",
            "the four kinds of documents, as shown in table 4. we\n",
            "think that bert conveys the pure semantic features while\n",
            "layoutlm contains both vision and semantic features. the\n",
            "layoutlm has much overlap to the proposed multi-modal\n",
            "context block, i.e., information redundancy, and bears the\n",
            "problem of double-computing visual features. hence, we\n",
            "use bert as the pre-trained model by default.\n",
            "evaluation of gating mechanism in equation 6. we also\n",
            "compare the gating fusion with two classical feature fusion\n",
            "strategies, i.e., the element-wise summation and concatena-\n",
            "tion. the two classical fusion strategies have their strong\n",
            "points on corresponding datasets. for example, concatena-\n",
            "tion operation has better result on business email, while\n",
            "falls behind summation on other datasets. beneﬁting from\n",
            "the on-off role, gating mechanism can help achieve the best\n",
            "performance. table 6 shows the results.\n",
            "evaluation of bi-lstm in ie. as addressed in sec-\n",
            "tion 3.3, lstm can help learn relations between characters,\n",
            "which is useful on the token-level scenes like business email\n",
            "and resume. table 7 shows that lstm can signiﬁcantlytable 6\n",
            "effects (ef1) of gating mechanism.\n",
            "strategy concatenation summation gating\n",
            "taxi invoice 98.41 98.62 98.73\n",
            "business email 87.06 86.19 87.33\n",
            "wildreceipt 87.95 88.47 89.62\n",
            "resume 81.55 82.26 83.16\n",
            "without pretrained language model\n",
            " with pretrained language model\n",
            "fig. 8. illustration of pre-trained language model’s effects. best viewed\n",
            "in color and zoom in.\n",
            "boost the ﬁnal performance.\n",
            "table 7\n",
            "effects (ef1) of lstm in ie part.\n",
            "strategy email resume\n",
            "wo.lstm 85.29 78.06\n",
            "w.lstm 87.40 83.24\n",
            "5.2.3 effects of different number of layers and heads\n",
            "table 8 analyzes the effects of different numbers of layers\n",
            "and heads in the spatial-aware self-attention. taxi invoices\n",
            "is relatively simple and has a ﬁxed layout. thus the model\n",
            "with 1 or 2 layers and the small number of heads achieves\n",
            "promising results. for scenes with complex layout struc-\n",
            "tures like resumes and wildreceipt, deeper layers and\n",
            "heads can help improve the accuracy results. in practice,\n",
            "one can adjust these settings according to the complexity of\n",
            "a task.\n",
            "5.2.4 effects of the end-to-end training\n",
            "to verify the effects of the end-to-end framework on text\n",
            "reading and information extraction, we perform the follow-\n",
            "ing experiments on four kinds of vrd datasets. we ﬁrst\n",
            "deﬁne two strong baselines for comparison. (1) base1 . the\n",
            "detection, recognition and information extraction modules\n",
            "are separately trained, and then pipelined as an inference\n",
            "model. (2) base2 . the detection and recognition tasks are\n",
            "jointly optimized, and then pipelined with the separately\n",
            "trained information extraction task. while joint training of\n",
            "the three modules is denoted as our end-to-end (short by\n",
            "e2e) framework. notice that all multi-modal features (see\n",
            "section 5.2.1) are integrated. the layer and head numbers in\n",
            "self-attention are set as (2, 2, 4, 2) and (32, 32, 16, 32) for four\n",
            "different tasks (taxi invoice, business email, wildreceipt,\n",
            "resume in order), respectively. results are as shown in12\n",
            "table 8\n",
            "accuracy results (ef1) with different number of layers and heads in\n",
            "spatial-aware self-attention.\n",
            "datasets layersheads\n",
            "2 4 8 16 32\n",
            "taxi\n",
            "invoice1 98.27 98.57 98.45 98.62 98.00\n",
            "2 98.31 98.39 98.58 98.52 98.74\n",
            "3 98.51 98.54 98.48 98.51 98.56\n",
            "4 98.44 98.58 98.41 98.70 98.59\n",
            "business\n",
            "email1 86.05 86.41 85.74 86.94 86.43\n",
            "2 85.95 87.51 86.78 87.33 87.59\n",
            "3 86.52 87.86 87.24 87.15 88.01\n",
            "4 86.48 87.45 87.82 87.88 87.64\n",
            "wildreceipt1 78.17 87.8 88.73 88.18 88.67\n",
            "2 86.26 88.11 88.21 89.16 89.11\n",
            "3 77.1 88.62 88.95 89.48 89.69\n",
            "4 85.48 89.00 88.63 89.66 90.15\n",
            "resume1 82.18 82.52 81.99 81.83 82.49\n",
            "2 82.7 82.56 82.97 82.83 83.57\n",
            "3 82.86 82.09 83.05 82.78 82.96\n",
            "4 82.75 83.12 82.43 82.98 83.46\n",
            "table 9\n",
            "comparisons of pipelines and end-to-end training framework.\n",
            "dataset methoddetection\n",
            "(fd-m)spotting\n",
            "(fr-m)ie\n",
            "(ef1)\n",
            "taxi invoicebase1 95.72 91.15 88.29\n",
            "base2 95.21 91.05 88.28\n",
            "e2e 94.85 91.07 88.46\n",
            "business emailbase1 97.12 55.88 45.24\n",
            "base2 97.10 56.18 45.47\n",
            "e2e 97.22 56.83 45.71\n",
            "wildreceiptbase1 90.31 73.52 69.37\n",
            "base2 90.55 74.98 71.15\n",
            "e2e 90.73 76.50 73.12\n",
            "resumebase1 96.71 55.15 58.53\n",
            "base2 96.86 55.56 58.31\n",
            "e2e 96.88 55.66 58.77\n",
            "table 9, in which the end-to-end training framework can\n",
            "beneﬁt both text reading and information extraction tasks.\n",
            "concretely, on text reading ,e2esurpasses base1 and base2\n",
            "on most scenes (wildreceipt, business email and resume),\n",
            "while slightly falls behind base1 by 0.08 on taxi invoice\n",
            "dataset. in fact, the ﬁnal results on information extraction\n",
            "are important. we see that e2eobtains the best performance\n",
            "on all datasets, achieving the largest ef1 performance gain\n",
            "(3.75%) on wildreceipt. we also note that, on taxi invoice\n",
            "and resume cases, improvements by e2eare not signiﬁcant.\n",
            "there are two main reasons. (1) the ie part has achieved\n",
            "the saturated performance w.r.t the cared text in text spot-\n",
            "ting. and the slight performance gain on ie is hard-won.\n",
            "therefore, we calculate the relative ef1of ie w.r.t the cared\n",
            "text in text spotting (i.e.,ef1of ie\n",
            "cared fr-mof spotting) to obtain the\n",
            "relative ie performance, which gets rid of the impacts of text\n",
            "spotting. table 10 shows the results. for taxi invoice, the\n",
            "performance is approximate to the upper-bound (100%), the\n",
            "improvement is limited. and for wildreceipt and resume,\n",
            "the improvements are signiﬁcant. as to business email,\n",
            "though base2 has better result (table 10) on the evaluation\n",
            "of relative performance, e2eboosts the ﬁnal results in table\n",
            "9. (2) the text spotting capability limits the ﬁnal results\n",
            "directly. it means that text spotting maybe the real perfor-\n",
            "mance bottleneck for end-to-end vrd information extrac-table 10\n",
            "the relative ef1for ie w.r.tfr-m(cared text instances) of text spotting.\n",
            "methodtaxi\n",
            "invoicebusiness\n",
            "emailwildreceipt resume\n",
            "base2 99.41 95.76 94.68 95.12\n",
            "e2e 99.45 95.01 96.11 97.41\n",
            "tion. however, taxi invoice contains many low-quality ( e.g.,\n",
            "blur or occlusion text) text, while resume has amount of\n",
            "long text instance (once one character is recognized wrongly,\n",
            "the ﬁnal results is wrong.). techniques for handling the\n",
            "above obstinate problems are needed.\n",
            "in sum, compared to the strong baselines, the end-to-\n",
            "end trainable strategy is actually working, and then boost\n",
            "the ﬁnal performance.\n",
            "5.3 comparisons with the state-of-the-arts\n",
            "recent methods [30], [32], [90], [91] focused on the informa-\n",
            "tion extraction task by adding great number of extra training\n",
            "samples like iit-cdip dataset [95] and docbank [115], and\n",
            "then have impressive results on the downstream datasets.\n",
            "following the typical routine, we also compare our method\n",
            "with them on several popular benchmarks. note that, all\n",
            "results are reported by using the ofﬁcial ocr annotations.\n",
            "evaluation on funsd the dataset is a noisy scanned\n",
            "from the dataset with 200 images. the results are shown\n",
            "in funsd column of table 11. to be fair, we ﬁrst compare\n",
            "our method with those without introducing extra data. our\n",
            "method signiﬁcantly outperforms them with a large margin\n",
            "(83.53 v.s.81.33 of matchvie [114]). when comparing with\n",
            "models trained with extra data, our method is still competi-\n",
            "tive. it only falls behind the llmv2 [32] and slm [90].\n",
            "evaluation on sroie the dataset has 963 scanned\n",
            "receipt images, which is evaluated on four entities in many\n",
            "works. most of the results are impressive, as shown in\n",
            "sroie column of table 11. this is because methods tend\n",
            "to achieve the performance upper bound of this dataset.\n",
            "for example, structext [91] (with extra data) has achieved\n",
            "96.88 of ef1, which only has slight advantage over 96.57\n",
            "of matchvie [114]. our method shows promising results\n",
            "on this benchmark, with 96.80 ef1in the token granularity\n",
            "(same to most works [1], [26], [30], [31], [32], [37], [114]) and\n",
            "98.37 in the segment granularity (same to structext [91]).\n",
            "evaluation on ephoie the dataset is a chinese exami-\n",
            "nation paper head dataset. our method obviously surpasses\n",
            "previous methods without extra data, achieving the best re-\n",
            "sult (98.85%), and is competitive to the large model llmv2\n",
            "[32]. similar to sroie, its performance upper bound is\n",
            "limited. that is, only 1.15% of improvement space is left.\n",
            "evaluation on wildreceipt this receipt dataset [25]\n",
            "is more challenging than sroie, which is collected from\n",
            "document images with unseen templates in the wild. most\n",
            "of the methods like gat [112] have rapid performance\n",
            "degradation compared to results in sroie and ephoie.\n",
            "while our method still has the best result (90.15% of ef1)\n",
            "compared to existing methods trained without extra data,\n",
            "which veriﬁes the advantages of the proposed method.13\n",
            "table 11\n",
            "comparison ( ef1) with the state-of-the-arts on funsd, sroie, ephoie, and wildreceipt.\u0003refers to the results are reported by our\n",
            "re-implemented model. on sroie dataset, italic denotes the segment-level accuracy while others are token-level accuracy.\n",
            "methodsfunsd sroie ephoie wildreceipt speed\n",
            "erec epre ef 1erec epre ef 1erec epre ef 1erec epre ef 1 avg fps\n",
            "llm [30] 82.19 75.96 78.95 95.24 95.24 95.24 - - - - - - -\n",
            "llmv2 [32] 85.19 83.24 84.20 96.61 96.61 96.61 99.51\u000399.06\u000399.28\u000391.78\u000392.45\u000392.05\u00033.93\n",
            "slm [90] 86.81 83.52 85.14 - - - - - - - - - -\n",
            "structext [91] 80.97 85.68 83.0998.81\n",
            "98.5292.77\n",
            "95.8495.62\n",
            "96.88- - 97.95 - - - -\n",
            "lambert [92] - - - - - 98.17 - - - - - - -\n",
            "tilt [93] - - - - - 98.10 - - - - - - -\n",
            "selfdoc [94] - - 83.36 - - - - - - - - - -\n",
            "spade [111] - - 70.50 - - - - - - - - - -\n",
            "lstm-crf [20] - - 62.13 - - 90.85 - - 89.10 82.60\u000383.90\u000383.20\u00035.43\n",
            "chargrid [2] 39.98\u000373.45\u000350.50\u0003- - 80.9 77.82\u000373.41\u000375.23\u000375.64\u000375.22\u000375.39\u000312.42\n",
            "gat [112] 70.81\u000371.03\u000370.73\u000383.14\u000391.73\u000387.23\u000397.36\u000396.48\u000396.90\u000384.57\u000386.37\u000385.43\u000315.73\n",
            "vrd [29] - - 72.43 - - 95.10 - - 92.55 84.57 86.37 85.70 -\n",
            "graphie [113] - - 72.12 - - 94.46 - - 90.26 - - - -\n",
            "vies [37] - - - - - 96.12 - - 95.23 - - - -\n",
            "pick [31] - - - 95.46 96.79 96.12 - - - - - - -\n",
            "matchvie [114] - - 81.33 - - 96.57 - - 96.87 - - - -\n",
            "textlattice [26] - - - - - 96.54 - - 98.06 - - - -\n",
            "sdmg-r [25] - - - - - 87.10 - - - - - 88.70 -\n",
            "trie [1] w. gt - - 78.86 - - 96.18 - - 93.21 84.69 87.58 85.99 14.52\n",
            "trie++ w. gt 82.94 84.37 83.5395.89\n",
            "98.4097.72\n",
            "98.3596.80\n",
            "98.3798.96 98.75 98.85 89.59 90.84 90.15 13.88\n",
            "inference speed . we test the inference speed (the av-\n",
            "erage frames per second, avg fps) of the re-implemented\n",
            "models for evaluating their efﬁciency. our model surpass\n",
            "llmv2 [32] largely, and has competitive efﬁciency with\n",
            "chargrid [2] and gat [112].\n",
            "in sum, compared to methods trained without extra data,\n",
            "the proposed method achieves the new state-of-the-arts on\n",
            "the pure information extraction task, and also shows strong\n",
            "generalization ability across various vrds. even compared\n",
            "with the large models, our method is also competitive.\n",
            "5.4 strong baselines on four categories of vrd\n",
            "for the pure information extraction task, their results (as\n",
            "shown in table 11) are calculated based on the ground\n",
            "truth of detection and recognition. however, the inﬂuence\n",
            "of ocr should not be neglected in reality. considering\n",
            "the real applications, i.e., end-to-end extracting information\n",
            "from vrd, one way is to divide the task as two pipelined\n",
            "steps: (1) obtaining text spotting results with a public ocr\n",
            "engines, (2) and then performing the information extraction.\n",
            "we here provide several groups of solid baselines on four\n",
            "kinds of vrds.\n",
            "5.4.1 comparisons among different experimental settings\n",
            "we ﬁrst build the basic baselines by combining the public\n",
            "ocr engines (including ppocr [116] and tesseract [33])\n",
            "and the pre-trained information extraction model (denoted\n",
            "as ie trie) used in section 5.3. furthermore, we use the\n",
            "text reading (denoted as tr) part in base2 (see table 9)\n",
            "as the ocr engine, and take the results as the input of\n",
            "information extraction counterparts, i.e., gat [112], lstm-\n",
            "crf [20], [21], chargrid [2], bert-crf [87] and layoutlm\n",
            "[32]. concretely, lstm-crf [21] is a classic 1d information\n",
            "extraction paradigm. its input embedding and hidden units\n",
            "in the following bilstm are all set to 128, followed by\n",
            "a crf layer. chargrid [2] is a 2d-cnn information ex-\n",
            "traction framework. the input character embedding is setto128 and the rest of network is identical to the paper.\n",
            "as for graph-based information extraction framework, we\n",
            "adopt gat [112], which is similar to [29]. bert-based and\n",
            "layoutlm-based model are treated as the large model for\n",
            "information extraction. as a result, ﬁve kinds of strong base-\n",
            "lines are constructed. they are ‘tr+gat’, ‘tr+lstm-crf’,\n",
            "‘tr+chargrid’, ‘tr+bert-crf’ and ‘tr+llm v2’. results\n",
            "are shown in table 12.\n",
            "comparison with public ocr engines . as expected,\n",
            "it is irresponsible to apply open ocr engines on the these\n",
            "speciﬁc scenes directly, showing poor results on text spot-\n",
            "ting and information extraction.\n",
            "comparison with classical methods . we compare our\n",
            "method with several classic methods (including ‘tr+gat’,\n",
            "‘tr+lstm-crf’ and ‘tr+chargrid’), and our method ob-\n",
            "viously outperforms them on all scenes.\n",
            "comparison with large models . since large models\n",
            "(e.g., bert or layoutlm) can boost the ie performance at-\n",
            "tributing to its big model of capacity as well as amount of\n",
            "samples, we here also construct such experimental settings\n",
            "for comparison. we see that our method is superior to\n",
            "the bert-based method, and achieves competitive results\n",
            "compared to layoutlm v2-based method, i.e., only falling\n",
            "behind it on the wildreceipt and business email scenes.\n",
            "5.4.2 comparison of inference speed\n",
            "we evaluate the running time of our model and its coun-\n",
            "terparts in frames per second ( abbr. fps). results are as\n",
            "shown in the last column of table 12. thanks to feature\n",
            "sharing between text reading and information extraction mod-\n",
            "ules, our end-to-end framework runs faster than its pipeline\n",
            "counterparts, especially compared to the large models ( e.g.,\n",
            "layoutlm v2). because of the enhanced multi-modal con-\n",
            "text block, the proposed method is slightly slower than its\n",
            "conference version (trie [1]) a more prominent trend is\n",
            "that the algorithm runs faster in scenarios where the length\n",
            "of texts is short in a document ( e.g., taxi invoice and wil-14\n",
            "table 12\n",
            "strong baselines for detection, end-to-end text spotting and information extraction on four kinds of datasets, i.e., taxi invoice, business email,\n",
            "wildreceipt and resume. their entity-level results are shown at https://davar-lab.github.io/publication/trie++.html. ‘tr’ means the text reading\n",
            "module in base2 (see table 9) is used for end-to-end evaluation.\n",
            "dataset methodstext detection text spotting end-to-end ie speed\n",
            "rec dpre dfd-mrec rpre rfr-merecepreef1fps\n",
            "taxi\n",
            "invoiceppocr+ie trie++ 76.73 59.13 66.79 50.23 38.72 43.73 49.66 52.85 50.87 4.78\n",
            "tesseract+ie trie++ 21.96 45.20 29.56 2.91 5.99 3.92 6.98 18.24 9.68 1.53\n",
            "tr+chargrid\n",
            "99.18 91.55 95.21 94.85 87.55 91.0592.11 84.6 88.14 2.83\n",
            "tr+gat 91.58 84.35 87.78 3.86\n",
            "tr+lstm-crf 90.73 83.2 86.74 3.62\n",
            "tr+bert-crf 88.24 85.25 86.74 3.25\n",
            "tr+llm v2 88.91 85.42 87.14 2.86\n",
            "tr+ie trie++ 99.18 91.55 95.21 94.85 87.55 91.05 92.15 84.8 88.28 3.69\n",
            "trie 99.18 91.45 95.16 94.78 87.39 90.94 91.95 84.74 88.16 5.10\n",
            "trie++ 99.22 90.85 94.85 95.27 87.23 91.07 92.5 84.85 88.46 4.26\n",
            "business\n",
            "emailppocr+ie trie++ 89.79 82.92 86.22 11.30 10.43 10.85 8.45 8.74 8.45 2.55\n",
            "tesseract+ie trie++ 68.96 81.07 74.53 25.25 29.68 27.28 23.90 27.39 25.39 1.73\n",
            "tr+chargrid\n",
            "96.4 97.81 97.10 55.77 56.59 56.1832.18 33.27 32.64 2.30\n",
            "tr+gat 42.34 44.22 43.09 2.30\n",
            "tr+lstm-crf 43.27 45.43 44.2 1.30\n",
            "tr+bert-crf 42.62 47.84 44.76 1.80\n",
            "tr+llm v2 45.22 48.49 46.68 1.50\n",
            "tr+ie trie++ 96.4 97.81 97.10 55.77 56.59 56.18 44.92 46.32 45.47 2.22\n",
            "trie 96.66 97.72 97.18 56.02 56.64 56.33 40.43 44.96 42.01 2.65\n",
            "trie++ 96.62 97.83 97.22 56.48 57.18 56.83 45.7 45.85 45.71 2.30\n",
            "wildreceiptppocr+ie trie++ 70.32 77.42 73.70 33.74 37.14 35.36 33.91 40.14 35.73 3.75\n",
            "tesseract+ie trie++ 41.11 58.25 48.20 12.20 17.29 14.31 11.14 21.99 14.59 1.26\n",
            "tr+chargrid\n",
            "90.65 90.45 90.55 75.06 74.89 74.9861.27 60.92 61.04 3.97\n",
            "tr+gat 68.16 69.21 68.66 4.25\n",
            "tr+lstm-crf 65.85 67.95 66.84 2.81\n",
            "tr+bert-crf 69.21 72.24 70.71 3.26\n",
            "tr+llm v2 72.49 74.07 73.26 2.31\n",
            "tr++ie trie++ 90.65 90.45 90.55 75.06 74.89 74.98 71.05 71.32 71.15 4.10\n",
            "trie 90.42 90.79 90.61 75.29 75.60 75.44 70.24 66.81 68.19 6.21\n",
            "trie++ 90.23 91.22 90.73 76.09 76.92 76.50 72.33 74.04 73.12 6.21\n",
            "resumeppocr+ie trie++ 92.18 85.67 88.81 22.49 20.90 21.66 26.65 26.41 26.51 1.27\n",
            "tesseract+ie trie++ 60.35 80.17 68.86 23.46 31.16 26.76 27.48 28.56 27.99 1.20\n",
            "tr+chargrid\n",
            "96.03 97.71 96.86 55.09 56.05 55.5641.46 38.18 39.62 2.10\n",
            "tr+gat 55.96 53.42 54.65 2.20\n",
            "tr+lstm-crf 55.63 54.02 54.64 0.71\n",
            "tr+bert-crf 56.96 57.67 57.31 1.43\n",
            "tr+llm v2 57.33 57.70 57.49 0.90\n",
            "tr+ie trie++ 96.03 97.71 96.86 55.09 56.05 55.56 59.08 57.62 58.33 2.09\n",
            "trie 95.99 97.82 96.89 55.16 56.21 55.68 55.94 56.67 56.29 2.39\n",
            "trie++ 95.96 97.82 96.88 55.13 56.20 55.66 59.43 58.16 58.77 2.31\n",
            "dreceipt), while on resume/business email datasets with\n",
            "long texts, the fps drops slightly.\n",
            "5.4.3 evaluations among different modules\n",
            "in the detection part, all methods achieve the satisfactory\n",
            "performance of fd-m(larger than 90%), while the perfor-\n",
            "mance on wildreceipt is the lowest. this is because the\n",
            "receipt images in wildreceipt are captured in the wild,\n",
            "and they are of non-front views, even with folds. when\n",
            "considering the end-to-end text spotting task, results on\n",
            "business and resume are poor due to the problems of char-\n",
            "acter distortion and long text. this problem will be a new\n",
            "research direction for ocr. for the end-to-end information\n",
            "extraction, results on business email are the worst, and the\n",
            "second-worst is resume. it reveals that there is plenty of\n",
            "work to do concerning end-to-end information extraction.\n",
            "from the perspective of systems, we surprisingly dis-\n",
            "cover that the text recognition may be the top bottleneck for\n",
            "end-to-end understanding vrd on category ii, iii and iv .\n",
            "the information extraction is another bottleneck due to the\n",
            "complex layouts and long character sentence (referring totable 12, 3 and 4). luckily, the end-to-end training strategy\n",
            "can enhance both the text reading and the ﬁnal information\n",
            "extraction task. in future, more attention should be paid to\n",
            "the effects of text reading w.r.t information extraction.\n",
            "6 d iscussion\n",
            "6.1 effects of large pre-trained model\n",
            "the large pre-trained model (short by lpm) becomes a\n",
            "hot research topic in many research communities, while\n",
            "discussions on its industrial application are rarely reported.\n",
            "from our point of view, lpm can help achieve bet-\n",
            "ter baseline performance, and has good generalization on\n",
            "various scenarios. it is suited to platform applications like\n",
            "ai open platform, providing basic trials for various ap-\n",
            "plications. however, it is not ‘green’ to training or main-\n",
            "taining lpm, where ‘green’ means lightweight and electric\n",
            "economic. lpm also suffers from the computational cost\n",
            "problem, which is impractical when applied in terminal\n",
            "devices ( e.g., receipt scanner used in logistics industry).\n",
            "perhaps it is a practical problem on how transfer beneﬁcial15\n",
            "knowledge from a large model to a small model. in fact,\n",
            "when applied model on the speciﬁc scene, ﬁnetuning on the\n",
            "corresponding data is essential for better results. in this way,\n",
            "performance gain from lpm may be limited. unlike them,\n",
            "our method belongs to another researching routine.\n",
            "6.2 necessity of end-to-end model\n",
            "we here point out that the public ocr engines like pad-\n",
            "dleocr or tesseract are irresponsible to apply on the\n",
            "vertical commerce scenes directly, as shown in table 12.\n",
            "therefore, it is inevitable to train/ﬁnetune ocr models\n",
            "on the corresponding scenarios. end-to-end framework can\n",
            "help achieve the global optimization, and also reduce model\n",
            "maintenance cost.\n",
            "6.3 annotation cost problem\n",
            "annotation cost is a realistic problem. unfortunately, it is\n",
            "hard to be avoided in real commerce applications ( e.g.,\n",
            "ﬁnancial auditing). once users (party a) pay money for\n",
            "applications, they tend to have high accuracy requirements\n",
            "but with few samples. for deep model, better application\n",
            "performance means more data or annotations. developers\n",
            "(party b) have to annotate labels as detailed as possible on\n",
            "the given datasets for better results. then some researchers\n",
            "attempt to pre-train large model with big data ( e.g.,collected\n",
            "from internet or other public ways) to relieve the problem.\n",
            "however, once when one applies model on speciﬁc scenes,\n",
            "developers have to ﬁnetune models on the given dataset\n",
            "for better performance. in fact, previous methods ( e.g., lay-\n",
            "outlm) also relied on the ﬁne-grained annotations to obtain\n",
            "the state-of-the-arts. thus, more annotations will be better.\n",
            "6.4 limitations\n",
            "first, our method currently requires the annotations of\n",
            "position, character string and entity labels of texts in a\n",
            "document, and the labeling process is cost-expensive. we\n",
            "will resort to semi/weakly-supervised learning algorithms\n",
            "to alleviate the problem in the future. another limitation is\n",
            "that the multi-modal context block captures context in the\n",
            "instance granularity, which can be much more ﬁne-grained\n",
            "if introduced token/ character granularity context. much\n",
            "more ﬁne-grained context is beneﬁcial to extracting entities\n",
            "across text instances.\n",
            "7 c onclusion\n",
            "in this paper, we present an end-to-end trainable network\n",
            "integrating text reading and information extraction for doc-\n",
            "ument understanding. these two tasks can mutually rein-\n",
            "force each other via a multi-modal context block, i.e., the\n",
            "multi-modal features, like visual, layout and textual fea-\n",
            "tures, can boost the performances of information extraction,\n",
            "while the loss of information extraction can also supervise\n",
            "the optimization of text reading. on various benchmarks,\n",
            "from structured to unstructured text type and ﬁxed to vari-\n",
            "able layout, the proposed method signiﬁcantly outperforms\n",
            "previous methods. to promote the vrd understanding\n",
            "research, we provide four kinds of benchmarks along the\n",
            "dimensions of layout and text type, and also contribute four\n",
            "groups of strong baselines for the future study.references\n",
            "[1] p . zhang, y. xu, z. cheng, s. pu, j. lu, l. qiao, y. niu, and f. wu,\n",
            "“trie: end-to-end text reading and information extraction for\n",
            "document understanding,” in acm mm , 2020, pp. 1413–1422.\n",
            "[2] a. r. katti, c. reisswig, c. guder, s. brarda, s. bickel, j. h ¨ohne,\n",
            "and j. b. faddoul, “chargrid: towards understanding 2d docu-\n",
            "ments,” in emnlp , 2018, pp. 4459–4469.\n",
            "[3] x. zhao, z. wu, and x. wang, “cutie: learning to understand\n",
            "documents with convolutional universal text information ex-\n",
            "tractor,” arxiv preprint arxiv:1903.12363 , 2019.\n",
            "[4] r. b. palm, o. winther, and f. laws, “cloudscan - a\n",
            "conﬁguration-free invoice analysis system using recurrent\n",
            "neural networks,” in icdar , 2017, pp. 406–413.\n",
            "[5] c. sage, a. aussem, h. elghazel, v . eglin, and j. espinas, “re-\n",
            "current neural network approach for table field extraction in\n",
            "business documents,” in icdar , 2019, pp. 1308–1313.\n",
            "[6] e. aslan, t. karakaya, e. unver, and y. akg ¨ul, “a part based\n",
            "modeling approach for invoice parsing,” in international confer-\n",
            "ence on computer vision theory and applications , vol. 4, 2016, pp.\n",
            "390–397.\n",
            "[7] b. janssen, e. saund, e. a. bier, p . wall, and m. sprague, “re-\n",
            "ceipts2go: the big world of small documents,” in proceedings of\n",
            "the 2012 acm symposium on document engineering , 2012, pp. 121–\n",
            "124.\n",
            "[8] a. dengel and b. klein, “smartfix: a requirements-driven\n",
            "system for document analysis and understanding,” in das , ser.\n",
            "lecture notes in computer science, vol. 2423, 2002, pp. 433–444.\n",
            "[9] d. schuster, k. muthmann, d. esser, a. schill, m. berger, c. wei-\n",
            "dling, k. aliyev, and a. hofmeier, “intellix - end-user trained\n",
            "information extraction for document archiving,” in icdar ,\n",
            "2013, pp. 101–105.\n",
            "[10] a. simon, j.-c. pret, and a. p . johnson, “a fast algorithm for\n",
            "bottom-up document layout analysis,” ieee tpami , vol. 19,\n",
            "pp. 273–277, 1997.\n",
            "[11] h. wang, p . lu, h. zhang, m. yang, x. bai, y. xu, m. he, y. wang,\n",
            "and w. liu, “all you need is boundary: toward arbitrary-\n",
            "shaped text spotting,” in aaai , vol. 34, no. 07, 2020, pp. 12 160–\n",
            "12 167.\n",
            "[12] l. qiao, s. tang, z. cheng, y. xu, y. niu, s. pu, and f. wu,\n",
            "“text perceptron: towards end-to-end arbitrary-shaped text\n",
            "spotting,” in aaai , vol. 34, no. 07, 2020, pp. 11 899–11 907.\n",
            "[13] w. feng, w. he, f. yin, x. zhang, and c. liu, “textdragon: an\n",
            "end-to-end framework for arbitrary shaped text spotting,” in\n",
            "iccv , 2019, pp. 9075–9084.\n",
            "[14] m. liao, b. shi, x. bai, x. wang, and w. liu, “textboxes: a fast\n",
            "text detector with a single deep neural network,” in aaai ,\n",
            "2017, pp. 4161–4167.\n",
            "[15] m. jaderberg, k. simonyan, a. vedaldi, and a. zisserman, “read-\n",
            "ing text in the wild with convolutional neural networks,” ijcv ,\n",
            "vol. 116, no. 1, pp. 1–20, 2016.\n",
            "[16] t. wang, d. j. wu, a. coates, and a. y. ng, “end-to-end text\n",
            "recognition with convolutional neural networks,” in icpr , 2012,\n",
            "pp. 3304–3308.\n",
            "[17] b. shi, x. bai, and c. yao, “an end-to-end trainable neural net-\n",
            "work for image-based sequence recognition and its application\n",
            "to scene text recognition,” ieee tpami , vol. 39, no. 11, pp. 2298–\n",
            "2304, 2017.\n",
            "[18] p . lyu, m. liao, c. yao, w. wu, and x. bai, “mask textspotter:\n",
            "an end-to-end trainable neural network for spotting text with\n",
            "arbitrary shapes,” in eccv , ser. lecture notes in computer\n",
            "science, vol. 11218, 2018, pp. 71–88.\n",
            "[19] k. shaalan, “a survey of arabic named entity recognition and\n",
            "classiﬁcation,” comput. linguistics , vol. 40, no. 2, pp. 469–510,\n",
            "2014.\n",
            "[20] g. lample, m. ballesteros, s. subramanian, k. kawakami, and\n",
            "c. dyer, “neural architectures for named entity recognition,”\n",
            "innaacl-hlt , 2016, pp. 260–270.\n",
            "[21] x. ma and e. h. hovy, “end-to-end sequence labeling via bi-\n",
            "directional lstm-cnns-crf,” in acl , 2016.\n",
            "[22] z. yang, x. he, j. gao, l. deng, and a. j. smola, “stacked\n",
            "attention networks for image question answering,” in cvpr ,\n",
            "2016, pp. 21–29.\n",
            "[23] p . anderson, x. he, c. buehler, d. teney, m. johnson, s. gould,\n",
            "and l. zhang, “bottom-up and top-down attention for image\n",
            "captioning and visual question answering,” in cvpr , 2018, pp.\n",
            "6077–6086.16\n",
            "[24] a. fukui, d. h. park, d. yang, a. rohrbach, t. darrell, and\n",
            "m. rohrbach, “multimodal compact bilinear pooling for visual\n",
            "question answering and visual grounding,” in emnlp , 2016,\n",
            "pp. 457–468.\n",
            "[25] h. sun, z. kuang, x. yue, c. lin, and w. zhang, “spatial dual-\n",
            "modality graph reasoning for key information extraction,”\n",
            "arxiv preprint arxiv:2103.14470 , 2021.\n",
            "[26] j. wang, t. wang, g. tang, l. jin, w. ma, k. ding, and y. huang,\n",
            "“tag, copy or predict: a uniﬁed weakly-supervised learning\n",
            "framework for visual information extraction using sequences,”\n",
            "arxiv preprint arxiv:2106.10681 , 2021.\n",
            "[27] t. i. denk and c. reisswig, “bertgrid: contextualized embed-\n",
            "ding for 2d document representation and understanding,” in\n",
            "workshop on document intelligence at neurips 2019 , 2019.\n",
            "[28] r. b. palm, f. laws, and o. winther, “attend, copy, parse end-\n",
            "to-end information extraction from documents,” in icdar , 2019,\n",
            "pp. 329–336.\n",
            "[29] x. liu, f. gao, q. zhang, and h. zhao, “graph convolution\n",
            "for multimodal information extraction from visually rich docu-\n",
            "ments,” in naacl-hlt , 2019, pp. 32–39.\n",
            "[30] y. xu, m. li, l. cui, s. huang, f. wei, and m. zhou, in layoutlm:\n",
            "pre-training of text and layout for document image understanding ,\n",
            "2020, pp. 1192–1200.\n",
            "[31] w. yu, n. lu, x. qi, p . gong, and r. xiao, “pick: processing key\n",
            "information extraction from documents using improved graph\n",
            "learning-convolutional networks,” icpr , 2020.\n",
            "[32] y. xu, y. xu, t. lv, l. cui, f. wei, g. wang, y. lu, d. flor ˆencio,\n",
            "c. zhang, w. che, m. zhang, and l. zhou, “layoutlmv2: multi-\n",
            "modal pre-training for visually-rich document understanding,”\n",
            "arxiv , vol. abs/2012.14740, 2020.\n",
            "[33] r. smith, “an overview of the tesseract ocr engine,” in icdar ,\n",
            "2007, pp. 629–633.\n",
            "[34] h. guo, x. qin, j. liu, j. han, j. liu, and e. ding, “eaten:\n",
            "entity-aware attention for single shot visual text extraction,”\n",
            "inicdar , 2019, pp. 254–259.\n",
            "[35] x. zhen-long, z. shui-geng, z. cheng, b. fan, n. yi, and p . shil-\n",
            "iang, “towards pure end-to-end learning for recognizing mul-\n",
            "tiple text sequences from an image,” arxiv: computer vision and\n",
            "pattern recognition , 2019.\n",
            "[36] a. w. harley, a. ufkes, and k. derpanis, “evaluation of deep\n",
            "convolutional nets for document image classiﬁcation and re-\n",
            "trieval,” icdar , pp. 991–995, 2015.\n",
            "[37] j. wang, c. liu, l. jin, g. tang, j. zhang, s. zhang, q. wang,\n",
            "y. wu, and m. cai, “towards robust visual information extrac-\n",
            "tion in real world: new dataset and novel solution,” vol. 35,\n",
            "no. 4, 2021, pp. 2738–2745.\n",
            "[38] b. p . majumder, n. potti, s. tata, j. b. wendt, q. zhao, and\n",
            "m. najork, “representation learning for information extraction\n",
            "from form-like documents,” in acl , 2020, pp. 6495–6504.\n",
            "[39] m. rusi ˜nol, t. benkhelfallah, and v . p . d’andecy, “field extrac-\n",
            "tion from administrative documents by incremental structural\n",
            "templates,” in icdar , 2013, pp. 1100–1104.\n",
            "[40] h. t. ha, m. medved, z. neverilov ´a, and a. horak, “recognition\n",
            "of ocr invoice metadata block types,” in international conference\n",
            "on text, speech, and dialogue . springer, 2018, pp. 304–312.\n",
            "[41] x. yang, e. yumer, p . asente, m. kraley, d. kifer, and c. l. giles,\n",
            "“learning to extract semantic structure from documents using\n",
            "multimodal fully convolutional neural networks,” in cvpr ,\n",
            "2017, pp. 4342–4351.\n",
            "[42] f. gralinski, t. stanislawek, a. wr ´oblewska, d. lipi ´nski, a. k.\n",
            "kaliska, p . rosalska, b. topolski, and p . biecek, “kleister: a novel\n",
            "task for information extraction involving long documents with\n",
            "complex layout,” arxiv , vol. abs/2003.02356, 2020.\n",
            "[43] x. chen, l. jin, y. zhu, c. luo, and t. wang, “text recognition\n",
            "in the wild: a survey,” acm computing surveys (csur) , vol. 54,\n",
            "no. 2, pp. 1–35, 2021.\n",
            "[44] d. r. judd, b. karsh, r. subbaroyan, t. toman, r. lahiri, and\n",
            "p . lok, “apparatus and method for searching and retrieving\n",
            "structured, semi-structured and unstructured content,” mar. 4\n",
            "2004, us patent app. 10/439,338.\n",
            "[45] s. soderland, “learning information extraction rules for semi-\n",
            "structured and free text,” mach. learn. , vol. 34, no. 1-3, pp. 233–\n",
            "272, 1999.\n",
            "[46] y. zhuang, m. cai, x. li, x. luo, q. yang, and f. wu, “the next\n",
            "breakthroughs of artiﬁcial intelligence: the interdisciplinary\n",
            "nature of ai,” engineering , vol. 6, no. 3, pp. 245–247, 2020.[47] d. esser, d. schuster, k. muthmann, m. berger, and a. schill,\n",
            "“automatic indexing of scanned documents: a layout-based ap-\n",
            "proach,” in document recognition and retrieval xix, part of the\n",
            "is&t-spie electronic imaging symposium , ser. spie proceedings,\n",
            "vol. 8297, p. 82970h.\n",
            "[48] x. liu, d. liang, s. yan, d. chen, y. qiao, and j. yan, “fots: fast\n",
            "oriented text spotting with a uniﬁed network,” in cvpr , 2018,\n",
            "pp. 5676–5685.\n",
            "[49] l. qiao, y. chen, z. cheng, y. xu, y. niu, s. pu, and f. wu,\n",
            "“mango: a mask attention guided one-stage scene text\n",
            "spotter,” in aaai , vol. 35, no. 3, 2021, pp. 2467–2476.\n",
            "[50] y. xu, c. zhang, z. cheng, j. xie, y. niu, s. pu, and f. wu,\n",
            "“segregated temporal assembly recurrent networks for weakly\n",
            "supervised multiple action detection,” in aaai , vol. 33, 2019,\n",
            "pp. 9070–9078.\n",
            "[51] v . yadav and s. bethard, “a survey on recent advances in\n",
            "named entity recognition from deep learning models,” in\n",
            "coling , 2018, pp. 2145–2158.\n",
            "[52] x.-y. duan, s.-l. tang, s.-y. zhang, y. zhang, z. zhao, j.-r. xue,\n",
            "y.-t. zhuang, and f. wu, “temporality-enhanced knowledge\n",
            "memory network for factoid question answering,” frontiers of\n",
            "information technology & electronic engineering , vol. 19, no. 1, pp.\n",
            "104–115, 2018.\n",
            "[53] k. kuang, l. li, z. geng, l. xu, k. zhang, b. liao, h. huang,\n",
            "p . ding, w. miao, and z. jiang, “causal inference,” engineering ,\n",
            "vol. 6, pp. 253–263, 2020.\n",
            "[54] s. ren, k. he, r. b. girshick, and j. sun, “faster r-cnn: towards\n",
            "real-time object detection with region proposal networks,” in\n",
            "neurips , 2015, pp. 91–99.\n",
            "[55] p . he, w. huang, t. he, q. zhu, y. qiao, and x. li, “single shot\n",
            "text detector with regional attention,” in iccv , 2017, pp. 3066–\n",
            "3074.\n",
            "[56] m. liao, b. shi, and x. bai, “textboxes++: a single-shot oriented\n",
            "scene text detector,” ieee tip , vol. 27, no. 8, pp. 3676–3690, 2018.\n",
            "[57] m. liao, z. zhu, b. shi, g. xia, and x. bai, “rotation-sensitive\n",
            "regression for oriented scene text detection,” in cvpr , 2018,\n",
            "pp. 5909–5918.\n",
            "[58] j. ma, w. shao, h. ye, l. wang, h. wang, y. zheng, and x. xue,\n",
            "“arbitrary-oriented scene text detection via rotation propos-\n",
            "als,” ieee tmm , vol. 20, no. 11, pp. 3111–3122, 2018.\n",
            "[59] y. liu and l. jin, “deep matching prior network: toward tighter\n",
            "multi-oriented text detection,” in cvpr , 2017, pp. 3454–3461.\n",
            "[60] b. shi, x. bai, and s. j. belongie, “detecting oriented text in\n",
            "natural images by linking segments,” in cvpr .\n",
            "[61] f. borisyuk, a. gordo, and v . sivakumar, “rosetta: large scale\n",
            "system for text detection and recognition in images,” in kdd ,\n",
            "2018, pp. 71–79.\n",
            "[62] k. he, g. gkioxari, p . doll ´ar, and r. b. girshick, “mask r-cnn,”\n",
            "iniccv , 2017, pp. 2980–2988.\n",
            "[63] e. xie, y. zang, s. shao, g. yu, c. yao, and g. li, “scene text\n",
            "detection with supervised pyramid context network,” in aaai ,\n",
            "vol. 33, no. 01, 2019, pp. 9038–9045.\n",
            "[64] c. zhang, b. liang, z. huang, m. en, j. han, e. ding, and\n",
            "x. ding, “look more than once: an accurate detector for text\n",
            "of arbitrary shapes,” in cvpr , 2019, pp. 10 552–10 561.\n",
            "[65] z. liu, g. lin, s. yang, f. liu, w. lin, and w. l. goh, “towards\n",
            "robust curve text detection with conditional spatial expan-\n",
            "sion,” in cvpr , 2019, pp. 7269–7278.\n",
            "[66] x. zhou, c. yao, h. wen, y. wang, s. zhou, w. he, and j. liang,\n",
            "“east: an efﬁcient and accurate scene text detector,” in cvpr ,\n",
            "2017, pp. 2642–2651.\n",
            "[67] s. long, j. ruan, w. zhang, x. he, w. wu, and c. yao,\n",
            "“textsnake: a flexible representation for detecting text of arbi-\n",
            "trary shapes,” in eccv , 2018, pp. 19–35.\n",
            "[68] w. wang, e. xie, x. li, w. hou, t. lu, g. yu, and s. shao,\n",
            "“shape robust text detection with progressive scale expansion\n",
            "network,” in cvpr , june 2019.\n",
            "[69] y. xu, y. wang, w. zhou, y. wang, z. yang, and x. bai, “textﬁeld:\n",
            "learning a deep direction ﬁeld for irregular scene text detection,”\n",
            "ieee tip , vol. 28, no. 11, pp. 5566–5579, 2019.\n",
            "[70] b. shi, x. bai, and c. yao, “an end-to-end trainable neural\n",
            "network for image-based sequence recognition and its appli-\n",
            "cation to scene text recognition,” ieee tpami. , vol. 39, no. 11,\n",
            "pp. 2298–2304, 2017.\n",
            "[71] b. shi, m. yang, x. wang, p . lyu, c. yao, and x. bai, “aster: an\n",
            "attentional scene text recognizer with flexible rectiﬁcation,”\n",
            "ieee tpami , pp. 1–1, 2018.17\n",
            "[72] z. cheng, f. bai, y. xu, g. zheng, s. pu, and s. zhou, “focus-\n",
            "ing attention: towards accurate text recognition in natural\n",
            "images,” in iccv , 2017, pp. 5086–5094.\n",
            "[73] a. graves, s. fern ´andez, f. gomez, and j. schmidhuber, “con-\n",
            "nectionist temporal classiﬁcation : labelling unsegmented se-\n",
            "quence data with recurrent neural networks,” in icml , 2006,\n",
            "pp. 369–376.\n",
            "[74] j. wang and x. hu, “gated recurrent convolution neural network\n",
            "for ocr,” in neurips , 2017, pp. 335–344.\n",
            "[75] c. lee and s. osindero, “recursive recurrent nets with atten-\n",
            "tion modeling for ocr in the wild,” in cvpr , 2016, pp. 2231–\n",
            "2239.\n",
            "[76] z. cheng, y. xu, f. bai, y. niu, s. pu, and s. zhou, “aon:\n",
            "towards arbitrarily-oriented text recognition,” in cvpr , 2018,\n",
            "pp. 5571–5579.\n",
            "[77] h. li, p . wang, and c. shen, “towards end-to-end text spotting\n",
            "with convolutional recurrent neural networks,” in iccv , 2017,\n",
            "pp. 5248–5256.\n",
            "[78] t. he, z. tian, w. huang, c. shen, y. qiao, and c. sun, “an end-\n",
            "to-end textspotter with explicit alignment and attention,” in\n",
            "cvpr , 2018, pp. 5020–5029.\n",
            "[79] m. busta, l. neumann, and j. matas, “deep textspotter: an\n",
            "end-to-end trainable scene text localization and recognition\n",
            "framework,” in iccv , 2017, pp. 2223–2231.\n",
            "[80] p . lyu, m. liao, c. yao, w. wu, and x. bai, “mask textspotter:\n",
            "an end-to-end trainable neural network for spotting text with\n",
            "arbitrary shapes,” in eccv , 2018, pp. 71–88.\n",
            "[81] e. riloff, “automatically constructing a dictionary for informa-\n",
            "tion extraction tasks,” in ncai , 1993, pp. 811–816.\n",
            "[82] s. b. huffman, “learning information extraction patterns from\n",
            "examples,” in connectionist, statistical, and symbolic approaches\n",
            "to learning for natural language processing , ser. lecture notes in\n",
            "computer science, vol. 1040, 1995, pp. 246–260.\n",
            "[83] i. muslea et al. , “extraction patterns for information extraction\n",
            "tasks: a survey,” in aaai , vol. 2, no. 2, 1999.\n",
            "[84] e. medvet, a. bartoli, and g. davanzo, “a probabilistic approach\n",
            "to printed document understanding,” ijdar , vol. 14, pp. 335–\n",
            "347, 2010.\n",
            "[85] f. cesarini, e. francesconi, m. gori, and g. soda, “analysis and\n",
            "understanding of multi-class invoices,” dar , vol. 6, pp. 102–114,\n",
            "2003.\n",
            "[86] m. shilman, p . liang, and p . viola, “learning nongenerative\n",
            "grammatical models for document analysis,” in iccv , 2005, pp.\n",
            "962–969.\n",
            "[87] j. devlin, m. chang, k. lee, and k. toutanova, “bert: pre-\n",
            "training of deep bidirectional transformers for language un-\n",
            "derstanding,” in naacl-hlt , 2019, pp. 4171–4186.\n",
            "[88] z. dai, z. yang, y. yang, j. g. carbonell, q. v . le, and r. salakhut-\n",
            "dinov, “transformer-xl: attentive language models beyond a\n",
            "fixed-length context,” in acl , 2019, pp. 2978–2988.\n",
            "[89] z. yang, z. dai, y. yang, j. g. carbonell, r. salakhutdinov, and\n",
            "q. v . le, “xlnet: generalized autoregressive pretraining for\n",
            "language understanding,” in neurips , 2019, pp. 5754–5764.\n",
            "[90] c. li, b. bi, m. yan, w. wang, s. huang, f. huang, and l. si,\n",
            "“structurallm: structural pre-training for form understanding,”\n",
            "arxiv preprint arxiv:2105.11210 , 2021.\n",
            "[91] y. li, y. qian, y. yu, x. qin, c. zhang, y. liu, k. yao, j. han,\n",
            "j. liu, and e. ding, “structext: structured text understanding\n",
            "with multi-modal transformers,” in acm mm , 2021, pp. 1912–\n",
            "1920.\n",
            "[92] ł. garncarek, r. powalski, t. stanisławek, b. topolski, p . halama,\n",
            "m. turski, and f. grali ´nski, “lambert: layout-aware language\n",
            "modeling for information extraction,” in icdar . springer, 2021,\n",
            "pp. 532–547.\n",
            "[93] r. powalski, ł. borchmann, d. jurkiewicz, t. dwojak,\n",
            "m. pietruszka, and g. pałka, “going full-tilt boogie on doc-\n",
            "ument understanding with text-image-layout transformer,” in\n",
            "icdar . springer, 2021, pp. 732–747.\n",
            "[94] p . li, j. gu, j. kuen, v . i. morariu, h. zhao, r. jain, v . manjunatha,\n",
            "and h. liu, “selfdoc: self-supervised document representation\n",
            "learning,” in cvpr , 2021, pp. 5652–5660.\n",
            "[95] d. lewis, g. agam, s. argamon, o. frieder, d. grossman, and\n",
            "j. heard, “building a test collection for complex document infor-\n",
            "mation processing,” in proceedings of the 29th annual international\n",
            "acm sigir conference on research and development in information\n",
            "retrieval , 2006, pp. 665–666.[96] m. carbonell, a. forn ´es, m. villegas, and j. llad ´os, “treynet:\n",
            "a neural model for text localization, transcription and named\n",
            "entity recognition in full pages,” pattern recognition letters , vol.\n",
            "136, pp. 219–227, 2020.\n",
            "[97] t. he, z. zhang, h. zhang, z. zhang, j. xie, and m. li,\n",
            "“bag of tricks for image classiﬁcation with convolutional neural\n",
            "networks,” in cvpr , 2019, pp. 558–567.\n",
            "[98] t. lin, p . doll ´ar, r. b. girshick, k. he, b. hariharan, and s. j.\n",
            "belongie, “feature pyramid networks for object detection,” in\n",
            "cvpr , 2017, pp. 936–944.\n",
            "[99] s. hochreiter and j. schmidhuber, “long short-term memory,”\n",
            "neural computation , vol. 9, no. 8, pp. 1735–1780, 1997.\n",
            "[100] l. h. li, m. yatskar, d. yin, c. hsieh, and k. chang, “visualbert:\n",
            "a simple and performant baseline for vision and language,”\n",
            "arxiv preprint arxiv:1908.03557 , 2019.\n",
            "[101] j. lu, d. batra, d. parikh, and s. lee, “vilbert: pretrain-\n",
            "ing task-agnostic visiolinguistic representations for vision-and-\n",
            "language tasks,” in neurips , 2019, pp. 13–23.\n",
            "[102] x. zhang, j. j. zhao, and y. lecun, “character-level convolu-\n",
            "tional networks for text classiﬁcation,” in neurips , 2015, pp.\n",
            "649–657.\n",
            "[103] e. f. t. k. sang and j. veenstra, “representing text chunks,” in\n",
            "eacl , 1999, pp. 173–179.\n",
            "[104] z. huang, k. chen, j. he, x. bai, d. karatzas, s. lu, and c. v .\n",
            "jawahar, “icdar2019 competition on scanned receipt ocr and\n",
            "information extraction,” in icdar , 2019, pp. 1516–1520.\n",
            "[105] g. jaume, h. k. ekenel, and j. thiran, “funsd: a dataset for\n",
            "form understanding in noisy scanned documents,” icdarw ,\n",
            "vol. 2, pp. 1–6, 2019.\n",
            "[106] s. park, s. shin, b. lee, j. lee, j. surh, m. seo, and h.-s. lee,\n",
            "“cord: a consolidated receipt dataset for post-ocr parsing,”\n",
            "inworkshop on document intelligence at neurips 2019 , 2019.\n",
            "[107] l. xu, q. dong, y. liao, c. yu, y. tian, w. liu, l. li, c. liu,\n",
            "x. zhang et al. , “cluener2020: ﬁne-grained named entity\n",
            "recognition dataset and benchmark for chinese,” arxiv preprint\n",
            "arxiv:2001.04351 , 2020.\n",
            "[108] a. paszke, s. gross, f. massa, a. lerer, j. bradbury, g. chanan,\n",
            "t. killeen, z. lin, n. gimelshein, l. antiga, a. desmaison,\n",
            "a. k ¨opf, e. yang, z. devito, m. raison, a. tejani, s. chil-\n",
            "amkurthy, b. steiner, l. fang, j. bai, and s. chintala, “pytorch:\n",
            "an imperative style, high-performance deep learning library,”\n",
            "inneurips , 2019, pp. 8024–8035.\n",
            "[109] i. loshchilov and f. hutter, “decoupled weight decay regulariza-\n",
            "tion,” in international conference on learning representations , 2018.\n",
            "[110] k. wang, b. babenko, and s. j. belongie, “end-to-end scene text\n",
            "recognition,” in iccv , d. n. metaxas, l. quan, a. sanfeliu, and\n",
            "l. v . gool, eds., 2011, pp. 1457–1464.\n",
            "[111] w. hwang, j. yim, s. park, s. yang, and m. seo, “spatial de-\n",
            "pendency parsing for semi-structured document information\n",
            "extraction,” arxiv preprint arxiv:2005.00642 , 2020.\n",
            "[112] p . veli ˇckovi ´c, g. cucurull, a. casanova, a. romero, p . li `o, and\n",
            "y. bengio, “graph attention networks,” in iclr , 2018.\n",
            "[113] y. qian, e. santus, z. jin, j. guo, and r. barzilay, “graphie: a\n",
            "graph-based framework for information extraction,” in naacl-\n",
            "hlt, 2019.\n",
            "[114] g. tang, l. xie, l. jin, j. wang, j. chen, z. xu, q. wang, y. wu,\n",
            "and h. li, “matchvie: exploiting match relevancy between\n",
            "entities for visual information extraction,” ijcai , 2021.\n",
            "[115] m. li, y. xu, l. cui, s. huang, f. wei, z. li, and m. zhou,\n",
            "“docbank: a benchmark dataset for document layout analysis,”\n",
            "incoling , 2020, pp. 949–960.\n",
            "[116] y. du, c. li, r. guo, x. yin, w. liu, j. zhou, y. bai, z. yu, y. yang,\n",
            "q. dang et al. , “pp-ocr: a practical ultra lightweight ocr\n",
            "system,” arxiv preprint arxiv:2009.09941 , 2020.\n",
            "layoutlmv3: pre-training for document ai\n",
            "with unified text and image masking\n",
            "yupan huang∗\n",
            "sun yat-sen university\n",
            "huangyp28@mail2.sysu.edu.cntengchao lv\n",
            "microsoft research asia\n",
            "tengchaolv@microsoft.comlei cui\n",
            "microsoft research asia\n",
            "lecu@microsoft.com\n",
            "yutong lu\n",
            "sun yat-sen university\n",
            "luyutong@mail.sysu.edu.cnfuru wei\n",
            "microsoft research asia\n",
            "fuwei@microsoft.com\n",
            "abstract\n",
            "self-supervised pre-training techniques have achieved remarkable\n",
            "progress in document ai. most multimodal pre-trained models\n",
            "use a masked language modeling objective to learn bidirectional\n",
            "representations on the text modality, but they differ in pre-training\n",
            "objectives for the image modality. this discrepancy adds difficulty\n",
            "to multimodal representation learning. in this paper, we propose\n",
            "layoutlmv3 to pre-train multimodal transformers for document\n",
            "ai with unified text and image masking. additionally, layoutlmv3\n",
            "is pre-trained with a word-patch alignment objective to learn cross-\n",
            "modal alignment by predicting whether the corresponding image\n",
            "patch of a text word is masked. the simple unified architecture\n",
            "and training objectives make layoutlmv3 a general-purpose pre-\n",
            "trained model for both text-centric and image-centric document ai\n",
            "tasks. experimental results show that layoutlmv3 achieves state-\n",
            "of-the-art performance not only in text-centric tasks, including\n",
            "form understanding, receipt understanding, and document visual\n",
            "question answering, but also in image-centric tasks such as docu-\n",
            "ment image classification and document layout analysis. the code\n",
            "and models are publicly available at https://aka.ms/layoutlmv3.\n",
            "ccs concepts\n",
            "•applied computing →document analysis ;•computing\n",
            "methodologies→natural language processing .\n",
            "keywords\n",
            "document ai, layoutlm, multimodal pre-training, vision-and-language\n",
            "acm reference format:\n",
            "yupan huang, tengchao lv, lei cui, yutong lu, and furu wei. 2022. lay-\n",
            "outlmv3: pre-training for document ai with unified text and image mask-\n",
            "ing. in proceedings of the 30th acm international conference on multimedia\n",
            "(mm ’22), october 10–14, 2022, lisboa, portugal. acm, new york, ny, usa,\n",
            "10 pages. https://doi.org/10.1145/3503161.3548112\n",
            "∗contribution during internship at microsoft research. corresponding authors: lei\n",
            "cui and furu wei.\n",
            "permission to make digital or hard copies of all or part of this work for personal or\n",
            "classroom use is granted without fee provided that copies are not made or distributed\n",
            "for profit or commercial advantage and that copies bear this notice and the full citation\n",
            "on the first page. copyrights for components of this work owned by others than acm\n",
            "must be honored. abstracting with credit is permitted. to copy otherwise, or republish,\n",
            "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
            "fee. request permissions from permissions@acm.org.\n",
            "mm ’22, october 10–14, 2022, lisboa, portugal\n",
            "©2022 association for computing machinery.\n",
            "acm isbn 978-1-4503-9203-7/22/10. . . $15.00\n",
            "https://doi.org/10.1145/3503161.3548112\n",
            "(a) text-centric form under-\n",
            "standing on funsd\n",
            "(b) image-centric layout anal-\n",
            "ysis on publaynet\n",
            "figure 1: examples of document ai tasks.\n",
            "1 introduction\n",
            "in recent years, pre-training techniques have been making waves\n",
            "in the document ai community by achieving remarkable progress\n",
            "on document understanding tasks [ 2,13–15,17,25,28,31,32,40,\n",
            "41,50,52,54–56]. as shown in figure 1, a pre-trained document\n",
            "ai model can parse layout and extract key information for various\n",
            "documents such as scanned forms and academic papers, which is\n",
            "important for industrial applications and academic research [8].\n",
            "self-supervised pre-training techniques have made rapid progress\n",
            "in representation learning due to their successful applications of re-\n",
            "constructive pre-training objectives. in nlp research, bert firstly\n",
            "proposed “masked language modeling” (mlm) to learn bidirec-\n",
            "tional representations by predicting the original vocabulary id of\n",
            "a randomly masked word token based on its context [9]. whereas\n",
            "most performant multimodal pre-trained document ai models use\n",
            "the mlm proposed by bert for text modality, they differ in pre-\n",
            "training objectives for image modality as depicted in figure 2. for\n",
            "example, docformer learns to reconstruct image pixels through a\n",
            "cnn decoder [ 2], which tends to learn noisy details rather than\n",
            "high-level structures such as document layouts [ 43,45]. selfdoc\n",
            "proposes to regress masked region features [ 31], which is noisier\n",
            "and harder to learn than classifying discrete features in a smaller\n",
            "vocabulary [ 6,18]. the different granularities of image (e.g., dense\n",
            "image pixels or contiguous region features) and text (i.e., discretearxiv:2204.08387v3  [cs.cl]  19 jul 2022faster r -cnnlinear embedding\n",
            "cnnword embeddingmasked word \n",
            "token classification\n",
            "visual transformer (optional)origin image \n",
            "reconstruction\n",
            "(a) patch\n",
            "(layoutlmv3)“a few \n",
            "number of \n",
            "tumor cell…”masked patch \n",
            "token classification\n",
            "(c) region\n",
            "(e.g., selfdoc )(b) grid\n",
            "(e.g., docformer )masked region \n",
            "feature regression\n",
            "𝑖𝑑𝑡1,𝑖𝑑𝑡2,𝑖𝑑𝑡3,\n",
            "𝑖𝑑𝑡4,𝑖𝑑𝑡5,𝑖𝑑𝑡6,…\n",
            "textmultimodal transformer(2)pre-training objectives of image modality\n",
            "(1)image embedding\n",
            "image\n",
            "𝑖𝑑𝑖1𝑖𝑑𝑖2𝑖𝑑𝑖3\n",
            "𝑖𝑑𝑖4𝑖𝑑𝑖5𝑖𝑑𝑖6\n",
            "𝑖𝑑𝑖7𝑖𝑑𝑖8𝑖𝑑𝑖9figure 2: comparisons with existing works (e.g., docformer\n",
            "[2] and selfdoc [31]) on (1) image embedding: our lay-\n",
            "outlmv3 uses linear patches to reduce the computational\n",
            "bottleneck of cnns and eliminate the need for region super-\n",
            "vision in training object detectors; (2) pre-training objectives\n",
            "on image modality: our layoutlmv3 learns to reconstruct\n",
            "discrete image tokens of masked patches instead of raw pix-\n",
            "els or region features to capture high-level layout structures\n",
            "rather than noisy details.\n",
            "tokens) objectives further add difficulty to cross-modal alignment\n",
            "learning, which is essential to multimodal representation learning.\n",
            "to overcome the discrepancy in pre-training objectives of text\n",
            "and image modalities and facilitate multimodal representation learn-\n",
            "ing, we propose layoutlmv3 to pre-train multimodal transform-\n",
            "ers for document ai with unified text and image masking objectives\n",
            "mlm and mim. as shown in figure 3, layoutlmv3 learns to recon-\n",
            "struct masked word tokens of the text modality and symmetrically\n",
            "reconstruct masked patch tokens of the image modality. inspired\n",
            "by dall-e [ 43] and beit [ 3], we obtain the target image tokens\n",
            "from latent codes of a discrete vae. for documents, each text word\n",
            "corresponds to an image patch. to learn this cross-modal alignment,\n",
            "we propose a word-patch alignment (wpa) objective to predict\n",
            "whether the corresponding image patch of a text word is masked.\n",
            "inspired by vit [ 11] and vilt [ 22], layoutlmv3 directly lever-\n",
            "ages raw image patches from document images without complex\n",
            "pre-processing steps such as page object detection. layoutlmv3\n",
            "jointly learns image, text and multimodal representations in a trans-\n",
            "former model with unified mlm, mim and wpa objectives. this\n",
            "makes layoutlmv3 the first multimodal pre-trained document aimodel without cnns for image embeddings, which significantly\n",
            "saves parameters and gets rid of region annotations. the simple\n",
            "unified architecture and objectives make layoutlmv3 a general-\n",
            "purpose pre-trained model for both text-centric tasks and image-\n",
            "centric document ai tasks.\n",
            "we evaluated pre-trained layoutlmv3 models across five pub-\n",
            "lic benchmarks, including text-centric benchmarks: funsd [ 20]\n",
            "for form understanding, cord [ 39] for receipt understanding,\n",
            "docvqa [ 38] for document visual question answering, and image-\n",
            "centric benchmarks: rvl-cdip [ 16] for document image classifi-\n",
            "cation, publaynet [ 59] for document layout analysis. experiment\n",
            "results demonstrate that layoutlmv3 achieves state-of-the-art per-\n",
            "formance on these benchmarks with parameter efficiency. further-\n",
            "more, layoutlmv3 is easy to reproduce for its simple and neat\n",
            "architecture and pre-training objectives.\n",
            "our contributions are summarized as follows:\n",
            "•layoutlmv3 is the first multimodal model in document ai\n",
            "that does not rely on a pre-trained cnn or faster r-cnn\n",
            "backbone to extract visual features, which significantly saves\n",
            "parameters and eliminates region annotations.\n",
            "•layoutlmv3 mitigates the discrepancy between text and\n",
            "image multimodal representation learning with unified dis-\n",
            "crete token reconstructive objectives mlm and mim. we\n",
            "further propose a word-patch alignment (wpa) objective\n",
            "to facilitate cross-modal alignment learning.\n",
            "•layoutlmv3 is a general-purpose model for both text-centric\n",
            "and image-centric document ai tasks. for the first time, we\n",
            "demonstrate the generality of multimodal transformers to\n",
            "vision tasks in document ai.\n",
            "•experimental results show that layoutlmv3 achieves state-\n",
            "of-the-art performance in text-centric tasks and image-centric\n",
            "tasks in document ai. the code and models are publicly\n",
            "available at https://aka.ms/layoutlmv3.\n",
            "2 layoutlmv3\n",
            "figure 3 gives an overview of the layoutlmv3.\n",
            "2.1 model architecture\n",
            "layoutlmv3 applies a unified text-image multimodal transformer\n",
            "to learn cross-modal representations. the transformer has a multi-\n",
            "layer architecture and each layer mainly consists of multi-head\n",
            "self-attention and position-wise fully connected feed-forward net-\n",
            "works [ 49]. the input of transformer is a concatenation of text\n",
            "embedding y=y1:𝐿and image embedding x=x1:𝑀sequences,\n",
            "where𝐿and𝑀are sequence lengths for text and image respectively.\n",
            "through the transformer, the last layer outputs text-and-image\n",
            "contextual representations.\n",
            "text embedding. text embedding is a combination of word em-\n",
            "beddings and position embeddings. we pre-processed document\n",
            "images with an off-the-shelf ocr toolkit to obtain textual con-\n",
            "tent and corresponding 2d position information. we initialize the\n",
            "word embeddings with a word embedding matrix from a pre-trained\n",
            "model roberta [ 36].the position embeddings include 1d position\n",
            "and 2d layout position embeddings, where the 1d position refers\n",
            "to the index of tokens within the text sequence, and the 2d lay-\n",
            "out position refers to the bounding box coordinates of the textmultimodal transformer\n",
            "seg1\n",
            "[mask]1+\n",
            "+seg1\n",
            "[mask]2+\n",
            "+seg2\n",
            "t33+\n",
            "+\n",
            "v1[mask]1\n",
            "+2\n",
            "+3\n",
            "+\n",
            "[mask] [spe]0\n",
            "+seg3\n",
            "t44+\n",
            "+segpad\n",
            "[cls]0+\n",
            "+4\n",
            "+\n",
            "v4hmlm head mim headt2\n",
            "hv2\n",
            "hv3\n",
            "pre-training\n",
            "objectives\n",
            "word/patch\n",
            "embedding1d position\n",
            "embedding\n",
            "flatten \n",
            "maskingocr parser\n",
            "masking2d position\n",
            "embeddinght1\n",
            "segpad\n",
            "[sep]5+\n",
            "+hwpa headaligned\n",
            "hunaligned\n",
            "patch1\n",
            "+patch2\n",
            "+patch3\n",
            "+patchpad\n",
            "+patch4\n",
            "+\n",
            "image patchesdocument imageresize\n",
            "split(unaligned)\n",
            "(aligned)(t1)(t2) (v2)(v3)\n",
            "linear\n",
            "embeddingword\n",
            "embedding\n",
            "figure 3: the architecture and pre-training objectives of layoutlmv3. layoutlmv3 is a pre-trained multimodal transformer\n",
            "for document ai with unified text and image masking objectives. given an input document image and its corresponding text\n",
            "and layout position information, the model takes the linear projection of patches and word tokens as inputs and encodes them\n",
            "into contextualized vector representations. layoutlmv3 is pre-trained with discrete token reconstructive objectives of masked\n",
            "language modeling (mlm) and masked image modeling (mim). additionally, layoutlmv3 is pre-trained with a word-patch\n",
            "alignment (wpa) objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text\n",
            "word is masked. “seg” denotes segment-level positions. “[cls]”, “[mask]”, “[sep]” and “[spe]” are special tokens.\n",
            "sequence. following the layoutlm, we normalize all coordinates\n",
            "by the size of images, and use embedding layers to embed x-axis,\n",
            "y-axis, width and height features separately [ 54]. the layoutlm\n",
            "and layoutlmv2 adopt word-level layout positions, where each\n",
            "word has its positions. instead, we adopt segment-level layout posi-\n",
            "tions that words in a segment share the same 2d position since the\n",
            "words usually express the same semantic meaning [28].\n",
            "image embedding. existing multimodal models in document ai\n",
            "either extract cnn grid features [ 2,56] or rely on an object detector\n",
            "like faster r-cnn [ 44] to extract region features [ 14,31,40,54]\n",
            "for image embeddings, which accounts for heavy computation\n",
            "bottleneck or require region supervision. inspired by vit [ 11] and\n",
            "vilt [ 22], we represent document images with linear projection\n",
            "features of image patches before feeding them into the multimodal\n",
            "transformer. specifically, we resize a document image into 𝐻×𝑊\n",
            "and denote the image with i∈r𝐶×𝐻×𝑊, where𝐶,𝐻and𝑊are the\n",
            "channel size, width and height of the image respectively. we then\n",
            "split the image into a sequence of uniform 𝑃×𝑃patches, linearly\n",
            "project the image patches to 𝐷dimensions and flatten them into\n",
            "a sequence of vectors, which length is 𝑀=𝐻𝑊/𝑃2. then we addlearnable 1d position embeddings to each patch since we have\n",
            "not observed improvements from using 2d position embeddings in\n",
            "our preliminary experiments. layoutlmv3 is the first multimodal\n",
            "model in document ai that does not rely on cnns to extract image\n",
            "features, which is vital to document ai models to reduce parameters\n",
            "or remove complex pre-processing steps.\n",
            "we insert semantic 1d relative position and spatial 2d relative\n",
            "position as bias terms in self-attention networks for text and image\n",
            "modalities following layoutlmv2[56].\n",
            "2.2 pre-training objectives\n",
            "layoutlmv3 is pre-trained with the mlm, mim, and wpa objec-\n",
            "tives to learn multimodal representation in a self-supervised learn-\n",
            "ing manner. full pre-training objectives of layoutlmv3 is defined\n",
            "as𝐿=𝐿𝑀𝐿𝑀+𝐿𝑀𝐼𝑀+𝐿𝑊𝑃𝐴 .\n",
            "objective i: masked language modeling (mlm). for the lan-\n",
            "guage side, our mlm is inspired by the masked language mod-\n",
            "eling in bert [ 9] and masked visual-language modeling in lay-\n",
            "outlm [ 54] and layoutlmv2 [ 56]. we mask 30% of text tokens witha span masking strategy with span lengths drawn from a poisson\n",
            "distribution ( 𝜆=3) [21,27]. the pre-training objective is to maxi-\n",
            "mize the log-likelihood of the correct masked text tokens y𝑙based\n",
            "on the contextual representations of corrupted sequences of image\n",
            "tokens x𝑀′and text tokens y𝐿′, where𝑀′and𝐿′represent the\n",
            "masked positions. we denote parameters of the transformer model\n",
            "with𝜃and minimize the subsequent cross-entropy loss:\n",
            "𝐿𝑀𝐿𝑀(𝜃)=−𝐿′∑︁\n",
            "𝑙=1log𝑝𝜃\u0010\n",
            "yℓ|x𝑀′,y𝐿′\u0011\n",
            "(1)\n",
            "as we keep the layout information unchanged, this objective fa-\n",
            "cilitates the model to learn the correspondence between layout\n",
            "information and text and image context.\n",
            "objective ii: masked image modeling (mim). to encourage\n",
            "the model to interpret visual content from contextual text and\n",
            "image representations, we adapt the mim pre-training objective in\n",
            "beit [ 3] to our multimodal transformer model. the mim objective\n",
            "is a symmetry to the mlm objective, that we randomly mask a\n",
            "percentage of about 40% image tokens with the blockwise masking\n",
            "strategy [ 3]. the mim objective is driven by a cross-entropy loss\n",
            "to reconstruct the masked image tokens x𝑚under the context of\n",
            "their surrounding text and image tokens.\n",
            "𝐿𝑀𝐼𝑀(𝜃)=−𝑀′∑︁\n",
            "𝑚=1log𝑝𝜃\u0010\n",
            "x𝑚|x𝑀′,y𝐿′\u0011\n",
            "(2)\n",
            "the labels of image tokens come from an image tokenizer, which\n",
            "can transform dense image pixels into discrete tokens according to\n",
            "a visual vocabulary [ 43]. thus mim facilitates learning high-level\n",
            "layout structures rather than noisy low-level details.\n",
            "objective iii: word-patch alignment (wpa). for documents,\n",
            "each text word corresponds to an image patch. as we randomly\n",
            "mask text and image tokens with mlm and mim respectively, there\n",
            "is no explicit alignment learning between text and image modalities.\n",
            "we thus propose a wpa objective to learn a fine-grained alignment\n",
            "between text words and image patches. the wpa objective is to\n",
            "predict whether the corresponding image patches of a text word\n",
            "are masked. specifically, we assign an aligned label to an unmasked\n",
            "text token when its corresponding image tokens are also unmasked.\n",
            "otherwise, we assign an unaligned label. we exclude the masked\n",
            "text tokens when calculating wpa loss to prevent the model from\n",
            "learning a correspondence between masked text words and image\n",
            "patches. we use a two-layer mlp head that inputs contextual text\n",
            "and image and outputs the binary aligned/unaligned labels with a\n",
            "binary cross-entropy loss:\n",
            "𝐿𝑊𝑃𝐴(𝜃)=−𝐿−𝐿′∑︁\n",
            "ℓ=1log𝑝𝜃\u0010\n",
            "zℓ|x𝑀′,y𝐿′\u0011\n",
            ", (3)\n",
            "where𝐿−𝐿′is the number of unmasked text tokens, zℓis the binary\n",
            "label of language token in the ℓposition.\n",
            "3 experiments\n",
            "3.1 model configurations\n",
            "the network architecture of layoutlmv3 follows that of layoutlm [ 54]\n",
            "and layoutlmv2 [ 56] for a fair comparison. we use base and large\n",
            "model sizes for layoutlmv3. layoutlmv3 base adopts a 12-layertransformer encoder with 12-head self-attention, hidden size of\n",
            "𝐷=768, and 3,072 intermediate size of feed-forward networks.\n",
            "layoutlmv3 large adopts a 24-layer transformer encoder with\n",
            "16-head self-attention, hidden size of 𝐷=1,024, and 4,096 interme-\n",
            "diate size of feed-forward networks. to pre-process the text input,\n",
            "we tokenize the text sequence with byte-pair encoding (bpe) [ 46]\n",
            "with a maximum sequence length 𝐿=512. we add a [cls] and\n",
            "a [sep] token at the beginning and end of each text sequence.\n",
            "when the length of the text sequence is shorter than 𝐿, we append\n",
            "[pad] tokens to it. the bounding box coordinates of these spe-\n",
            "cial tokens are all zeros. the parameters for image embedding are\n",
            "𝐶×𝐻×𝑊=3×224×224,𝑃=16,𝑀=196.\n",
            "we adopt distributed and mixed-precision training to reduce\n",
            "memory costs and speed up training procedures. we also use a\n",
            "gradient accumulation mechanism to split the batch of samples into\n",
            "several mini-batches to overcome memory constraints for large\n",
            "batch sizes. we further use a gradient checkpointing technique for\n",
            "document layout analysis to reduce memory costs. to stabilize train-\n",
            "ing, we follow cogview [ 10] to change the computation of atten-\n",
            "tion to softmax\u0010q𝑇k√\n",
            "𝑑\u0011\n",
            "=softmax\u0010\u0010q𝑇\n",
            "𝛼√\n",
            "𝑑k−max\u0010q𝑇\n",
            "𝛼√\n",
            "𝑑k\u0011\u0011\n",
            "×𝛼\u0011\n",
            ",\n",
            "where𝛼is 32.\n",
            "3.2 pre-training layoutlmv3\n",
            "to learn a universal representation for various document tasks,\n",
            "we pre-train layoutlmv3 on a large iit-cdip dataset. the iit-\n",
            "cdip test collection 1.0 is a large-scale scanned document im-\n",
            "age dataset, which contains about 11 million document images\n",
            "and can split into 42 million pages [ 26]. we only use 11 million\n",
            "of them to train layoutlmv3. we do not do image augmentation\n",
            "following layoutlm models [ 54,56]. for the multimodal trans-\n",
            "former encoder along with the text embedding layer, layoutlmv3\n",
            "is initialized from the pre-trained weights of roberta [ 36]. our\n",
            "image tokenizer is initialized from a pre-trained image tokenizer\n",
            "in dit, a self-supervised pre-trained document image transformer\n",
            "model [ 30]. the vocabulary size of image tokens is 8,192. we ran-\n",
            "domly initialized the rest model parameters. we pre-train lay-\n",
            "outlmv3 using adam optimizer [ 23] with a batch size of 2,048 for\n",
            "500,000 steps. we use a weight decay of 1𝑒−2, and (𝛽1,𝛽2) = (0.9,\n",
            "0.98). for the layoutlmv3 base model, we use a learning rate of\n",
            "1𝑒−4, and we linearly warm up the learning rate over the first 4.8%\n",
            "steps. for layoutlmv3 large , the learning rate and warm-up ratio\n",
            "are5𝑒−5and 10%, respectively.\n",
            "3.3 fine-tuning on multimodal tasks\n",
            "we compare layoutlmv3 with typical self-supervised pre-training\n",
            "approaches and categorize them by their pre-training modalities.\n",
            "•[t] text modality : bert [ 9] and roberta [ 36] are typical\n",
            "pre-trained language models which only use text information\n",
            "with transformer architecture. we use funsd and rvl-\n",
            "cdip results of the roberta from layoutlm [ 54] and results\n",
            "of bert from layoutlmv2 [ 56]. we reproduce and report\n",
            "the cord and docvqa results of the roberta.\n",
            "•[t+l] text and layout modalities : layoutlm incorporates\n",
            "layout information by adding word-level spatial embeddings\n",
            "to embeddings of bert [ 54]. structurallm leverages segment-\n",
            "level layout information [ 28]. bros encodes relative layouttable 1: comparison with existing published models on the cord [39], funsd [20], rvl-cdip [16], and docvqa [38] datasets.\n",
            "“t/l/i” denotes “text/layout/image” modality. “r/g/p” denotes “region/grid/patch” image embedding. we multiply all values\n",
            "by a hundred for better readability.†in the udoc paper [14], the cord splits are 626/247 receipts for training/test instead of\n",
            "the official 800/100 training/test receipts adopted by other works. thus the score†is not directly comparable to other scores.\n",
            "models denoted with‡use more data to train docvqa and are expected to score higher. for example, tilt introduces one\n",
            "more supervised training stage on more qa datasets [40]. structurallm additionally uses the validation set in training [28].\n",
            "model parameters modality image embeddingfunsd cord rvl-cdip docvqa\n",
            "f1↑ f1↑ accuracy↑ anls↑\n",
            "bert base [9] 110m t none 60.26 89.68 89.81 63.72\n",
            "roberta base [36] 125m t none 66.48 93.54 90.06 66.42\n",
            "bros base [17] 110m t+l none 83.05 95.73 - -\n",
            "lilt base [50] - t+l none 88.41 96.07 95.68* -\n",
            "layoutlmbase [54] 160m t+l+i (r) resnet-101 (fine-tune) 79.27 - 94.42 -\n",
            "selfdoc [31] - t+l+i (r) resnext-101 83.36 - 92.81 -\n",
            "udoc [14] 272m t+l+i (r) resnet-50 87.93 98.94†95.05 -\n",
            "tilt base [40] 230m t+l+i (r) u-net - 95.11 95.25 83.92‡\n",
            "xylayoutlmbase [15] - t+l+i (g) resnext-101 83.35 - - -\n",
            "layoutlmv2base [56] 200m t+l+i (g) resnext101-fpn 82.76 94.95 95.25 78.08\n",
            "docformer base [2] 183m t+l+i (g) resnet-50 83.34 96.33 96.17 -\n",
            "layoutlmv3base (ours) 133m t+l+i (p) linear 90.29 96.56 95.44 78.76\n",
            "bert large [9] 340m t none 65.63 90.25 89.92 67.45\n",
            "roberta large [36] 355m t none 70.72 93.80 90.11 69.52\n",
            "layoutlmlarge [54] 343m t+l none 77.89 - 91.90 -\n",
            "bros large [17] 340m t+l none 84.52 97.40 - -\n",
            "structurallm large [28] 355m t+l none 85.14 - 96.08 83.94‡\n",
            "formnet [25] 217m t+l none 84.69 - - -\n",
            "formnet [25] 345m t+l none - 97.28 - -\n",
            "tilt large [40] 780m t+l+i (r) u-net - 96.33 95.52 87.05‡\n",
            "layoutlmv2large [56] 426m t+l+i (g) resnext101-fpn 84.20 96.01 95.64 83.48\n",
            "docformer large [2] 536m t+l+i (g) resnet-50 84.55 96.99 95.50 -\n",
            "layoutlmv3large (ours) 368m t+l+i (p) linear 92.08 97.46 95.93 83.37\n",
            "* lilt uses image features with resnext101-fpn backbone in fine-tuning rvl-cdip.\n",
            "positions [ 17]. lilt fine-tunes on different languages with\n",
            "pre-trained textual models [ 50]. formnet leverages the spa-\n",
            "tial relationship between tokens in a form [25].\n",
            "•[t+l+i (r)] text, layout and image modalities with faster\n",
            "r-cnn region features : this line of works extract im-\n",
            "age region features from roi heads in the faster r-cnn\n",
            "model [ 44]. among them, layoutlm [ 54] and tilt [ 40] use\n",
            "ocr words’ bounding box to serve as region proposals and\n",
            "add the region features to corresponding text embeddings.\n",
            "selfdoc [ 31] and udoc [ 14] use document object proposals\n",
            "and concatenate region features with text embeddings.\n",
            "•[t+l+i (g)] text, layout and image modalities with cnn\n",
            "grid features : layoutlmv2 [ 56] and docformer [ 2] extract\n",
            "image grid features with a cnn backbone without object\n",
            "detection. xylayoutlm [ 15] adopts the architecture of lay-\n",
            "outlmv2 and improves layout representation.\n",
            "•[t+l+i (p)] text, layout, and image modalities with lin-\n",
            "ear patch features : layoutlmv3 replaces cnn backbones\n",
            "with simple linear embedding to encode image patches.we fine-tune layoutlmv3 on multimodal tasks on publicly avail-\n",
            "able benchmarks. results are shown in table 1.\n",
            "task i: form and receipt understanding. form and receipt\n",
            "understanding tasks require extracting and structuring forms and\n",
            "receipts’ textual content. the tasks are a sequence labeling problem\n",
            "aiming to tag each word with a label. we predict the label of the\n",
            "last hidden state of each text token with a linear layer and an mlp\n",
            "classifier for form and receipt understanding tasks, respectively.\n",
            "we conduct experiments on the funsd dataset and the cord\n",
            "dataset. the funsd [20] is a noisy scanned form understand-\n",
            "ing dataset sampled from the rvl-cdip dataset [16]. the funsd\n",
            "dataset contains 199 documents with comprehensive annotations\n",
            "for 9,707 semantic entities. we focus on the semantic entity labeling\n",
            "task on the funsd dataset to assign each semantic entity a label\n",
            "among “question”, “answer”, “header” or “other”. the training and\n",
            "test splits contain 149 and 50 samples, respectively. cord [39] is a\n",
            "receipt key information extraction dataset with 30 semantic labels\n",
            "defined under 4 categories. it contains 1,000 receipts of 800 training,\n",
            "100 validation, and 100 test examples. we use officially-provided\n",
            "images and ocr annotations. we fine-tune layoutlmv3 for 1,000table 2: document layout analysis map @ iou [0.50:0.95] on publaynet validation set. all models use only information from\n",
            "the vision modality. layoutlmv3 outperforms the compared resnets [14, 59] and vision transformer [30] backbones.\n",
            "model framework backbone text title list table figure overall\n",
            "publaynet[59] mask r-cnn resnet-101 91.6 84.0 88.6 96.0 94.9 91.0\n",
            "dit base [30] mask r-cnn transformer 93.4 87.1 92.9 97.3 96.7 93.5\n",
            "udoc [14] faster r-cnn resnet-50 93.9 88.5 93.7 97.3 96.4 93.9\n",
            "dit base [30] cascade r-cnn transformer 94.4 88.9 94.8 97.6 96.9 94.5\n",
            "layoutlmv3base (ours) cascade r-cnn transformer 94.5 90.6 95.5 97.9 97.0 95.1\n",
            "steps with a learning rate of 1𝑒−5and a batch size of 16 for funsd,\n",
            "and5𝑒−5and 64 for cord.\n",
            "we report f1 scores for this task. for the large model size, the\n",
            "layoutlmv3 achieves an f1 score of 92.08 on the funsd dataset,\n",
            "which significantly outperforms the sota result of 85.14 provided\n",
            "by structurallm [ 28]. note that layoutlmv3 and structurallm use\n",
            "segment-level layout positions, while the other works use word-\n",
            "level layout positions. using segment-level positions may benefit\n",
            "the semantic entity labeling task on funsd [ 28], so the two types\n",
            "of work are not directly comparable. the layoutlmv3 also achieves\n",
            "sota f1 scores on the cord dataset for both base and large model\n",
            "sizes. the results show that layoutlmv3 can significantly benefit\n",
            "the text-centric form and receipt understanding tasks.\n",
            "task ii: document image classification. the document image\n",
            "classification task aims to predict the category of document images.\n",
            "we feed the output hidden state of the special classification token\n",
            "([cls]) into an mlp classifier to predict the class labels.\n",
            "we conduct experiments on the rvl-cdip dataset. it is a subset\n",
            "of the iit-cdip collection labeled with 16 categories [ 16]. rvl-cdip\n",
            "dataset contains 400,000 document images, among them 320,000\n",
            "are training images, 40,000 are validation images, and 40,000 are\n",
            "test images. we extract text and layout information using microsoft\n",
            "read api. we fine-tune layoutlmv3 for 20,000 steps with a batch\n",
            "size of 64 and a learning rate of 2𝑒−5.\n",
            "the evaluation metric is the overall classification accuracy. lay-\n",
            "outlmv3 achieves better or comparable results with a much smaller\n",
            "model size than previous works. for example, compared to lay-\n",
            "outlmv2, layoutlmv3 achieves an absolute improvement of 0.19%\n",
            "and 0.29% in the base model and large model size, respectively, with\n",
            "a much simpler image embedding (i.e., linear vs. resnext101-fpn).\n",
            "the results show that our simple image embeddings can achieve\n",
            "desirable results on image-centric tasks.\n",
            "task iii: document visual question answering. document\n",
            "visual question answering requires a model to take a document\n",
            "image and a question as input and output an answer [ 38]. we\n",
            "formalize this task as an extractive qa problem, where the model\n",
            "predicts start and end positions by classifying the last hidden state\n",
            "of each text token with a binary classifier.\n",
            "we conduct experiments on the docvqa dataset, a standard\n",
            "dataset for visual question answering on document images [ 38]. the\n",
            "official partition of the docvqa dataset consists of 10,194/1,286/1,287\n",
            "images and 39,463/5,349/5,188 questions for training/validation/test\n",
            "set, respectively. we train our model on the training set, evalu-\n",
            "ate the model on the test set, and report results by submitting\n",
            "them to the official evaluation website. we use microsoft read apito extract text and bounding boxes from images and use heuris-\n",
            "tics to find given answers in the extracted text as in layoutlmv2.\n",
            "we fine-tune layoutlmv3base for 100,000 steps with a batch size\n",
            "of 128, a learning rate of 3𝑒−5, and a warmup ratio of 0.048.\n",
            "forlayoutlmv3large , the step size, batch size, learning rate and\n",
            "warmup ratio are 200,000, 32, 1𝑒−5, and 0.1, respectively.\n",
            "we report the commonly-used edit distance-based metric anls\n",
            "(also known as average normalized levenshtein similarity). the\n",
            "layoutlmv3base improves the anls score of layoutlmv2base\n",
            "from 78.08 to 78.76, with much simpler image embedding (i.e., from\n",
            "resnext101-fpn to linear embedding). the layoutlmv3large\n",
            "further gains an absolute anls score of 4.61 over layoutlmv3base .\n",
            "the results show that layoutlmv3 is effective for the document\n",
            "visual question answering task.\n",
            "3.4 fine-tuning on a vision task\n",
            "to demonstrate the generality of layoutlmv3 from the multimodal\n",
            "domain to the visual domain, we transfer layoutlmv3 to a docu-\n",
            "ment layout analysis task. this task is about detecting the layouts\n",
            "of unstructured digital documents by providing bounding boxes and\n",
            "categories such as tables, figures, texts, etc. this task helps parse the\n",
            "documents into a machine-readable format for downstream appli-\n",
            "cations. we model this task as an object detection problem without\n",
            "text embedding, which is effective in existing works [ 14,30,59].\n",
            "we integrate the layoutlmv3 as feature backbone in the cascade\n",
            "r-cnn detector [ 4] with fpn [ 34] implemented using the detec-\n",
            "tron2 [ 53]. we adopt the standard practice to extract single-scale\n",
            "features from different transformer layers, such as layers 4, 6, 8, and\n",
            "12 of the layoutlmv3 base model. we use resolution-modifying\n",
            "modules to convert the single-scale features into the multiscale fpn\n",
            "features [1, 30, 33].\n",
            "we conduct experiments on publaynet dataset [ 59]. the dataset\n",
            "contains research paper images annotated with bounding boxes and\n",
            "polygonal segmentation across five document layout categories:\n",
            "text, title, list, figure, and table. the official splits contain 335,703\n",
            "training images, 11,245 validation images, and 11,405 test images.\n",
            "we train our model on the training split and evaluate our model\n",
            "on the validation split following standard practice [ 14,30,59]. we\n",
            "train our model for 60,000 steps using the adamw optimizer with\n",
            "1,000 warm-up steps and a weight decay of 0.05 following dit [ 30].\n",
            "since layoutlmv3 is pre-trained with inputs from both vision and\n",
            "language modalities, we use a larger batch size of 32 and a lower\n",
            "learning rate of 2𝑒−4empirically. we do not use flipping or crop-\n",
            "ping augmentation strategy in the fine-tuning stage to be consistenttable 3: ablation study on image embeddings and pre-training objectives on typical text-centric tasks (form and receipt under-\n",
            "standing on funsd and cord) and image-centric tasks (document image classification on rvl-cdip and document layout\n",
            "analysis on publaynet). all models were trained at base size on 1 million data for 150,000 steps with learning rate 3𝑒−4.\n",
            "#imageparameterspre-training funsd cord rvl-cdip publaynet\n",
            "embed objective(s) f1↑ f1↑ accuracy↑ map↑\n",
            "1 none 125m mlm 88.64 96.27 95.33 not applicable\n",
            "2 linear 126m mlm 89.39 96.11 95.00 loss divergence\n",
            "3 linear 132m mlm+mim 89.19 96.30 95.42 94.38\n",
            "4 linear 133m mlm+mim+wpa 89.78 96.49 95.53 94.43\n",
            "figure 4: loss convergence curves of fine-tuning the ablated\n",
            "models of layoutlmv3 on publaynet dataset. the loss of\n",
            "model #2 did not converge. by incorporating the mim objec-\n",
            "tive, the loss converges normally. the wpa objective further\n",
            "decreases the loss. best viewed in color.\n",
            "with our pre-training stage. we do not use relative positions in self-\n",
            "attention networks as dit.\n",
            "we measure the performance using the mean average preci-\n",
            "sion (map) @ intersection over union (iou) [0.50:0.95] of bound-\n",
            "ing boxes and report results in table 2. we compare with the\n",
            "resnets [ 14,59] and the concurrent vision transformer [ 30] back-\n",
            "bones. layoutlmv3 outperforms the other models in all metrics,\n",
            "achieving an overall map score of 95.1. layoutlmv3 achieves a\n",
            "high gain in the “title” category. since titles are typically much\n",
            "smaller than other categories and can be identified by their tex-\n",
            "tual content, we attribute this improvement to our incorporation\n",
            "of language modality in pre-training layoutlmv3. these results\n",
            "demonstrate the generality and superiority of layoutlmv3.\n",
            "3.5 ablation study\n",
            "in table 3 we study the effect of our image embeddings and pre-\n",
            "training objectives. we first build a baseline model #1 that uses text\n",
            "and layout information, pre-trained with mlm objective. then we\n",
            "use linearly projected image patches as the image embedding of the\n",
            "baseline model, denoted as model #2. we further pre-train model#2 with mim and wpa objectives step by step and denote the new\n",
            "models as #3 and #4, respectively.\n",
            "in figure 4, we visualize losses of models #2, #3, and #4 when\n",
            "fine-tuned on the publaynet dataset with a batch size of 16 and a\n",
            "learning rate of 2𝑒−4. we have tried to train the model #2 with\n",
            "learning rates of { 1𝑒−4,2𝑒−4,4𝑒−4} combined with batch sizes\n",
            "of {16,32}, but the loss of model #2 did not converge and the map\n",
            "score on publaynet is near zero.\n",
            "effect of linear image embedding. we observe that model #1\n",
            "without image embedding has achieved good results on some tasks.\n",
            "this suggests that language modality, including text and layout\n",
            "information, plays a vital role in document understanding. how-\n",
            "ever, the results are still unsatisfactory. moreover, model #1 cannot\n",
            "conduct some image-centric document analysis tasks without vi-\n",
            "sion modality. for example, the vision modality is critical for the\n",
            "document layout analysis task on publaynet because bounding\n",
            "boxes are tightly integrated with images. our simple design of\n",
            "linear image embedding combined with appropriate pre-training\n",
            "objectives can consistently improve not only image-centric tasks,\n",
            "but also some text-centric tasks further.\n",
            "effect of mim pre-training objective. simply concatenating lin-\n",
            "ear image embedding with text embedding as input to model #2\n",
            "deteriorates performance on cord and rvl-cdip, while the loss\n",
            "on publaynet diverges. we speculate that the model failed to learn\n",
            "meaningful visual representation on the linear patch embeddings\n",
            "without any pre-training objective associated with image modality.\n",
            "the mim objective mitigates this problem by preserving the image\n",
            "information until the last layer of the model by randomly masking\n",
            "out a portion of input image patches and reconstructing them in\n",
            "the output [ 22]. comparing the results of model #3 and model #2,\n",
            "the mim objective benefits cord and rvl-cdip. as simply using\n",
            "linear image embedding has improved funsd, mim does not fur-\n",
            "ther contribute to funsd. by incorporating the mim objective in\n",
            "training, the loss converges when fine-tuning publaynet as shown\n",
            "in figure 4, and we obtain a desirable map score. the results indi-\n",
            "cate that mim can help regularize the training. thus mim is critical\n",
            "for vision tasks like document layout analysis on publaynet.\n",
            "effect of wpa pre-training objective. by comparing models #3\n",
            "and #4 in table 3, we observe that the wpa objective consistently\n",
            "improves all tasks. moreover, the wpa objective decreases the loss\n",
            "of the vision task on publaynet in figure 4. these results confirm\n",
            "the effectiveness of wpa not only in cross-modal representation\n",
            "learning, but also in image representation learning.parameter comparisons. the table shows that incorporating\n",
            "image embedding for a 16×16patch projection (#1 →#2) introduces\n",
            "only 0.6m parameters. the parameters are negligible compared to\n",
            "the parameters of cnn backbones (e.g., 44m for resnet-101). a\n",
            "mim head and a wpa head introduce 6.9m and 0.6m parameters\n",
            "in the pre-training stage. the parameter overhead introduced by\n",
            "image embedding is marginal compared to the mlm head, which\n",
            "has 39.2m parameters for a text vocabulary size of 50,265. we did\n",
            "not take count of the image tokenizer when calculating parameters\n",
            "as the tokenizer is a standalone module for generating the labels of\n",
            "mim but is not integrated into the transformer backbone.\n",
            "4 related work\n",
            "multimodal self-supervised pre-training technique has made a\n",
            "rapid progress in document intelligence due to its successful applica-\n",
            "tions of document layout and image representation learning [ 2,13–\n",
            "15,17,25,28,31,32,40,41,50,52,54–56]. layoutlm and following\n",
            "works joint layout representation learning by encoding spatial co-\n",
            "ordinates of text [ 17,25,28,54]. various works then joint image\n",
            "representation learning by combining cnns with transformer [ 49]\n",
            "self-attention networks. these works either extract cnn grid fea-\n",
            "tures [ 2,56] or rely on an object detector to extract region fea-\n",
            "tures [ 14,31,40,54], which accounts for heavy computation bottle-\n",
            "neck or requires region supervision. in the field of natural images\n",
            "vision-and-language pre-training (vlp), research works have seen\n",
            "a shift from region features [ 5,47,48] to grid features [ 19] to lift\n",
            "limitations of pre-defined object classes and region supervision. in-\n",
            "spired by vision transformer (vit) [ 11], there have also been recent\n",
            "efforts in vlp without cnns to overcome the weakness of cnn.\n",
            "still, most rely on separate self-attention networks to learn visual\n",
            "features; thus, their computational cost is not reduced [ 12,29,57].\n",
            "an exception is vilt, which learns visual features with a light-\n",
            "weight linear layer and significantly cuts down the model size and\n",
            "running time [ 22]. inspired by vilt, our layoutlmv3 is the first\n",
            "multimodal model in document ai that utilizes image embeddings\n",
            "without cnns.\n",
            "reconstructive pre-training objectives revolutionized repre-\n",
            "sentation learning. in nlp research, bert firstly proposed “masked\n",
            "language modeling” (mlm) to learn bidirectional representations\n",
            "and advanced the state of the arts on broad language understanding\n",
            "tasks [ 9]. in the field of cv, masked image modeling (mim) aims\n",
            "to learn rich visual representations via predicting masked content\n",
            "conditioning in visible context. for example, vit reconstructs the\n",
            "mean color of masked patches, which leads to performance gains\n",
            "in imagenet classification [ 11]. beit reconstructs visual tokens\n",
            "learned by a discrete vae, achieving competitive results in image\n",
            "classification and semantic segmentation [ 3]. dit extends beit to\n",
            "document images to document layout analysis [30].\n",
            "inspired by mlm and mim, researchers in the field of vision-\n",
            "and-language have explored reconstructive objectives for multi-\n",
            "modal representation learning . whereas most well-performing\n",
            "vision-and-language pre-training (vlp) models use the mlm pro-\n",
            "posed by bert on text modality, they differ in their pre-training ob-\n",
            "jectives for the image modality. there are three variants of mim cor-\n",
            "responding to different image embeddings: masked region modeling\n",
            "(mrm), masked grid modeling (mgm), and masked patch modeling(mpm). mrm has been proven to be effective in regressing original\n",
            "region features [ 5,31,48] or classifying object labels [ 5,37,48] for\n",
            "masked regions. mgm has also been explored in the soho, whose\n",
            "objective is to predict the mapping index in a visual dictionary for\n",
            "masked grid features [ 19]. for patch-level image embedding, visual\n",
            "parsing [ 57] proposed to mask visual tokens according to the atten-\n",
            "tion weights in their self-attention image encoder, which does not\n",
            "apply to simple linear image encoders. vilt [ 22] and meter [ 12]\n",
            "attempt to leverage mpm similar to vit [ 11] and beit [ 3], which re-\n",
            "spectively reconstruct the mean color and discrete tokens in visual\n",
            "vocabularies for image patches, but resulted in degraded perfor-\n",
            "mance on downstream tasks. our layoutlmv3 firstly demonstrates\n",
            "the effectiveness of mim for linear patch image embedding.\n",
            "various cross-modal objectives are further developed for vi-\n",
            "sion and language (vl) alignment learning in multimodal models.\n",
            "image-text matching is widely used to learn a coarse-grained vl\n",
            "alignment [ 2,5,19,22,56]. to learn a fine-grained vl alignment,\n",
            "uniter proposes a word-region alignment objective based on opti-\n",
            "mal transports, which calculates the minimum cost of transporting\n",
            "the contextualized image embeddings to word embeddings [ 5].\n",
            "vilt extends this objective to patch-level image embeddings [ 22].\n",
            "unlike natural images, document images imply an explicit fine-\n",
            "grained alignment relationship between text words and image ar-\n",
            "eas. using this relationship, udoc uses contrastive learning and\n",
            "similarity distillation to align the image and text belonging to the\n",
            "same area [ 14]. layoutlmv2 covers some text lines in raw images\n",
            "and predicts whether each text token is covered [ 56]. in contrast,\n",
            "we naturally utilize the masking operations in mim to construct\n",
            "aligned/unaligned pairs in an effective and unified way.\n",
            "5 conclusion and future work\n",
            "in this paper, we present layoutlmv3 to pre-train the multimodal\n",
            "transformer for document ai, which redesigns the model archi-\n",
            "tecture and pre-training objectives for layoutlm. distinguishing\n",
            "from the existing multimodal model in document ai, layoutlmv3\n",
            "does not rely on a pre-trained cnn or faster r-cnn backbone to\n",
            "extract visual features, significantly saving parameters and elimi-\n",
            "nating region annotations. we use unified text and image masking\n",
            "pre-training objectives: masked language modeling, masked image\n",
            "modeling, and word-patch alignment, to learn multimodal repre-\n",
            "sentations. extensive experimental results have demonstrated the\n",
            "generality and superiority of layoutlmv3 for both text-centric and\n",
            "image-centric document ai tasks with the simple architecture and\n",
            "unified objectives. in future research, we will investigate scaling up\n",
            "pre-trained models so that the models can leverage more training\n",
            "data to drive sota results further. in addition, we will explore few-\n",
            "shot and zero-shot learning capabilities to facilitate more real-world\n",
            "business scenarios in the document ai industry.\n",
            "6 acknowledgement\n",
            "we are grateful to yiheng xu for fruitful discussions and inspiration.\n",
            "this work was supported by the nsfc (u1811461) and the program\n",
            "for guangdong introducing innovative and entrepreneurial teams\n",
            "under grant no.2016zt06d211.table 4: visual information extraction in chinese f1 score on the ephoie test set.\n",
            "model subject test time name school #examination #seat class #student grade score mean\n",
            "bilstm+crf [24] 98.51 100.0 98.87 98.80 75.86 72.73 94.04 84.44 98.18 69.57 89.10\n",
            "gcn-based [35] 98.18 100.0 99.52 100.0 88.17 86.00 97.39 80.00 94.44 81.82 92.55\n",
            "graphie [42] 94.00 100.0 95.84 97.06 82.19 84.44 93.07 85.33 94.44 76.19 90.26\n",
            "trie [58] 98.79 100.0 99.46 99.64 88.64 85.92 97.94 84.32 97.02 80.39 93.21\n",
            "vies [51] 99.39 100.0 99.67 99.28 91.81 88.73 99.29 89.47 98.35 86.27 95.23\n",
            "structext [32] 99.25 100.0 99.47 99.83 97.98 95.43 98.29 97.33 99.25 93.73 97.95\n",
            "layoutlmv3-chinesebase (ours) 98.99 100.0 99.77 99.20 100.0 100.0 98.82 99.78 98.31 97.27 99.21\n",
            "a appendix\n",
            "a.1 layoutlmv3 in chinese\n",
            "pre-training layoutlmv3 in chinese. to demonstrate the effec-\n",
            "tiveness of layoutlmv3 in not only english but also in the chinese\n",
            "language, we pre-train a layoutlmv3-chinese model in base size.\n",
            "it is trained on 50 million document pages in chinese. we collect\n",
            "large-scale chinese documents by downloading publicly available\n",
            "digital-born documents and following the principles of common\n",
            "crawl (https://commoncrawl.org/) to process these documents. for\n",
            "the multimodal transformer encoder along with the text embed-\n",
            "ding layer, layoutlmv3-chinese is initialized from the pre-trained\n",
            "weights of xlm-r [ 7]. we randomly initialized the rest model pa-\n",
            "rameters. other training setting is the same as layoutlmv3.\n",
            "fine-tuning on visual information extraction. the visual in-\n",
            "formation extraction (vie) requires extracting key information from\n",
            "document images. the task is a sequence labeling problem aiming\n",
            "to tag each word with a pre-defined label. we predict the label of\n",
            "the last hidden state of each text token with a linear layer.\n",
            "we conduct experiments on the ephoie dataset. the ephoie [51]\n",
            "is a visual information extraction dataset consisting of examina-\n",
            "tion paper heads with diverse layouts and backgrounds. it contains\n",
            "1,494 images with comprehensive annotations for 15,771 chinese\n",
            "text instances. we focus on a token-level entity labeling task on\n",
            "the ephoie dataset to assign each character a label among ten\n",
            "pre-defined categories. the training and test sets contain 1,183 and\n",
            "311 images, respectively. we fine-tune layoutlmv3-chinese for\n",
            "100 epochs. the batch size is 16, and the learning rate is 5𝑒−5with\n",
            "linear warmup over the first epoch.\n",
            "we report f1 scores for this task and report results in table 4. the\n",
            "layoutlmv3-chinese shows superior performance on most metrics\n",
            "and achieves a sota mean f1 score of 99.21%. the results show\n",
            "that layoutlmv3 significantly benefits the vie task in chinese.\n",
            "references\n",
            "[1]alaaeldin ali, hugo touvron, mathilde caron, piotr bojanowski, matthijs douze,\n",
            "armand joulin, ivan laptev, natalia neverova, gabriel synnaeve, jakob verbeek,\n",
            "et al. 2021. xcit: cross-covariance image transformers. in neurips .\n",
            "[2]srikar appalaraju, bhavan jasani, bhargava urala kota, yusheng xie, and r. man-\n",
            "matha. 2021. docformer: end-to-end transformer for document understanding.\n",
            "iniccv .\n",
            "[3]hangbo bao, li dong, songhao piao, and furu wei. 2022. beit: bert pre-\n",
            "training of image transformers. in iclr .\n",
            "[4]zhaowei cai and nuno vasconcelos. 2018. cascade r-cnn: delving into high\n",
            "quality object detection. in cvpr .\n",
            "[5]yen-chun chen, linjie li, licheng yu, ahmed el kholy, faisal ahmed, zhe gan,\n",
            "yu cheng, and jingjing liu. 2020. uniter: universal image-text representation\n",
            "learning. in eccv .[6]jaemin cho, jiasen lu, dustin schwenk, hannaneh hajishirzi, and aniruddha\n",
            "kembhavi. 2020. x-lxmert: paint, caption and answer questions with multi-\n",
            "modal transformers. in emnlp .\n",
            "[7]alexis conneau, kartikay khandelwal, naman goyal, vishrav chaudhary, guil-\n",
            "laume wenzek, francisco guzmán, édouard grave, myle ott, luke zettlemoyer,\n",
            "and veselin stoyanov. 2020. unsupervised cross-lingual representation learning\n",
            "at scale. in acl.\n",
            "[8]lei cui, yiheng xu, tengchao lv, and furu wei. 2021. document ai: benchmarks,\n",
            "models and applications. arxiv preprint arxiv:2111.08609 (2021).\n",
            "[9]jacob devlin, ming-wei chang, kenton lee, and kristina toutanova. 2019. bert:\n",
            "pre-training of deep bidirectional transformers for language understanding. in\n",
            "naacl .\n",
            "[10] ming ding, zhuoyi yang, wenyi hong, wendi zheng, chang zhou, da yin,\n",
            "junyang lin, xu zou, zhou shao, hongxia yang, et al .2021. cogview: mastering\n",
            "text-to-image generation via transformers. in neurips .\n",
            "[11] alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xi-\n",
            "aohua zhai, thomas unterthiner, mostafa dehghani, matthias minderer, georg\n",
            "heigold, sylvain gelly, jakob uszkoreit, and neil houlsby. 2021. an image is\n",
            "worth 16x16 words: transformers for image recognition at scale. in iclr .\n",
            "[12] zi-yi dou, yichong xu, zhe gan, jianfeng wang, shuohang wang, lijuan wang,\n",
            "chenguang zhu, zicheng liu, michael zeng, et al .2021. an empirical study\n",
            "of training end-to-end vision-and-language transformers. arxiv preprint\n",
            "arxiv:2111.02387 (2021).\n",
            "[13] łukasz garncarek, rafał powalski, tomasz stanisławek, bartosz topolski, piotr\n",
            "halama, michał turski, and filip graliński. 2021. lambert: layout-aware\n",
            "language modeling for information extraction. in icdar .\n",
            "[14] jiuxiang gu, jason kuen, vlad morariu, handong zhao, rajiv jain, nikolaos\n",
            "barmpalios, ani nenkova, and tong sun. 2021. unidoc: unified pretraining\n",
            "framework for document understanding. in neurips .\n",
            "[15] zhangxuan gu, changhua meng, ke wang, jun lan, weiqiang wang, ming\n",
            "gu, and liqing zhang. 2022. xylayoutlm: towards layout-aware multimodal\n",
            "networks for visually-rich document understanding. in cvpr .\n",
            "[16] adam w harley, alex ufkes, and konstantinos g derpanis. 2015. evaluation of\n",
            "deep convolutional nets for document image classification and retrieval. in\n",
            "icdar .\n",
            "[17] teakgyu hong, donghyun kim, mingi ji, wonseok hwang, daehyun nam, and\n",
            "sungrae park. 2022. bros: a pre-trained language model focusing on text and\n",
            "layout for better key information extraction from documents. in aaai .\n",
            "[18] yupan huang, hongwei xue, bei liu, and yutong lu. 2021. unifying multimodal\n",
            "transformer for bi-directional image and text generation. in acm multimedia .\n",
            "[19] zhicheng huang, zhaoyang zeng, yupan huang, bei liu, dongmei fu, and\n",
            "jianlong fu. 2021. seeing out of the box: end-to-end pre-training for vision-\n",
            "language representation learning. in cvpr .\n",
            "[20] guillaume jaume, hazim kemal ekenel, and jean-philippe thiran. 2019. funsd:\n",
            "a dataset for form understanding in noisy scanned documents. in icdarw .\n",
            "[21] mandar joshi, danqi chen, yinhan liu, daniel s weld, luke zettlemoyer, and\n",
            "omer levy. 2020. spanbert: improving pre-training by representing and predict-\n",
            "ing spans. transactions of the association for computational linguistics 8 (2020),\n",
            "64–77.\n",
            "[22] wonjae kim, bokyung son, and ildoo kim. 2021. vilt: vision-and-language\n",
            "transformer without convolution or region supervision. in icml .\n",
            "[23] diederik p kingma and jimmy ba. 2014. adam: a method for stochastic opti-\n",
            "mization. arxiv preprint arxiv:1412.6980 (2014).\n",
            "[24] guillaume lample, miguel ballesteros, sandeep subramanian, kazuya kawakami,\n",
            "and chris dyer. 2016. neural architectures for named entity recognition. in\n",
            "naacl hlt .\n",
            "[25] chen-yu lee, chun-liang li, timothy dozat, vincent perot, guolong su, nan\n",
            "hua, joshua ainslie, renshen wang, yasuhisa fujii, and tomas pfister. 2022.\n",
            "formnet: structural encoding beyond sequential modeling in form document\n",
            "information extraction. in acl.\n",
            "[26] d. lewis, g. agam, s. argamon, o. frieder, d. grossman, and j. heard. 2006.\n",
            "building a test collection for complex document information processing. in\n",
            "sigir .[27] mike lewis, yinhan liu, naman goyal, marjan ghazvininejad, abdelrahman\n",
            "mohamed, omer levy, veselin stoyanov, and luke zettlemoyer. 2020. bart:\n",
            "denoising sequence-to-sequence pre-training for natural language generation,\n",
            "translation, and comprehension. in acl.\n",
            "[28] chenliang li, bin bi, ming yan, wei wang, songfang huang, fei huang, and luo\n",
            "si. 2021. structurallm: structural pre-training for form understanding. in acl.\n",
            "[29] junnan li, ramprasaath selvaraju, akhilesh gotmare, shafiq joty, caiming xiong,\n",
            "and steven chu hong hoi. 2021. align before fuse: vision and language repre-\n",
            "sentation learning with momentum distillation. in neurips .\n",
            "[30] junlong li, yiheng xu, tengchao lv, lei cui, cha zhang, and furu wei. 2022. dit:\n",
            "self-supervised pre-training for document image transformer. arxiv preprint\n",
            "arxiv:2203.02378 (2022).\n",
            "[31] peizhao li, jiuxiang gu, jason kuen, vlad i morariu, handong zhao, rajiv jain,\n",
            "varun manjunatha, and hongfu liu. 2021. selfdoc: self-supervised document\n",
            "representation learning. in cvpr .\n",
            "[32] yulin li, yuxi qian, yuechen yu, xiameng qin, chengquan zhang, yan liu, kun\n",
            "yao, junyu han, jingtuo liu, and errui ding. 2021. structext: structured text\n",
            "understanding with multi-modal transformers. in acm multimedia .\n",
            "[33] yanghao li, saining xie, xinlei chen, piotr dollar, kaiming he, and ross girshick.\n",
            "2021. benchmarking detection transfer learning with vision transformers. arxiv\n",
            "preprint arxiv:2111.11429 (2021).\n",
            "[34] tsung-yi lin, piotr dollár, ross girshick, kaiming he, bharath hariharan, and\n",
            "serge belongie. 2017. feature pyramid networks for object detection. in cvpr .\n",
            "[35] xiaojing liu, feiyu gao, qiong zhang, and huasha zhao. 2019. graph convolu-\n",
            "tion for multimodal information extraction from visually rich documents. in\n",
            "naacl hlt .\n",
            "[36] yinhan liu, myle ott, naman goyal, jingfei du, mandar joshi, danqi chen, omer\n",
            "levy, mike lewis, luke zettlemoyer, and veselin stoyanov. 2019. roberta: a\n",
            "robustly optimized bert pretraining approach. arxiv preprint arxiv:1907.11692\n",
            "(2019).\n",
            "[37] jiasen lu, dhruv batra, devi parikh, and stefan lee. 2019. vilbert: pretraining\n",
            "task-agnostic visiolinguistic representations for vision-and-language tasks. in\n",
            "neurips .\n",
            "[38] minesh mathew, dimosthenis karatzas, and cv jawahar. 2021. docvqa: a dataset\n",
            "for vqa on document images. in wacv .\n",
            "[39] seunghyun park, seung shin, bado lee, junyeop lee, jaeheung surh, minjoon\n",
            "seo, and hwalsuk lee. 2019. cord: a consolidated receipt dataset for post-\n",
            "ocr parsing. in document intelligence workshop at neural information processing\n",
            "systems .\n",
            "[40] rafal powalski, łukasz borchmann, dawid jurkiewicz, tomasz dwojak, michal\n",
            "pietruszka, and gabriela pałka. 2021. going full-tilt boogie on document\n",
            "understanding with text-image-layout transformer. in icdar .\n",
            "[41] subhojeet pramanik, shashank mujumdar, and hima patel. 2020. towards a\n",
            "multi-modal, multi-task learning based pre-training framework for document\n",
            "representation learning. arxiv preprint arxiv:2009.14457 (2020).\n",
            "[42] yujie qian. 2019. a graph-based framework for information extraction . ph. d.\n",
            "dissertation. massachusetts institute of technology.[43] aditya ramesh, mikhail pavlov, gabriel goh, scott gray, chelsea voss, alec\n",
            "radford, mark chen, and ilya sutskever. 2021. zero-shot text-to-image generation.\n",
            "inicml .\n",
            "[44] shaoqing ren, kaiming he, ross b. girshick, and jian sun. 2015. faster r-cnn:\n",
            "towards real-time object detection with region proposal networks. tpami 39,\n",
            "1137–1149.\n",
            "[45] tim salimans, andrej karpathy, xi chen, and diederik p. kingma. 2017. pixel-\n",
            "cnn++: improving the pixelcnn with discretized logistic mixture likelihood\n",
            "and other modifications. in iclr .\n",
            "[46] rico sennrich, barry haddow, and alexandra birch. 2016. neural machine\n",
            "translation of rare words with subword units. in acl.\n",
            "[47] weijie su, xizhou zhu, yue cao, bin li, lewei lu, furu wei, and jifeng dai. 2019.\n",
            "vl-bert: pre-training of generic visual-linguistic representations. in iclr .\n",
            "[48] hao tan and mohit bansal. 2019. lxmert: learning cross-modality encoder\n",
            "representations from transformers. in emnlp .\n",
            "[49] ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones,\n",
            "aidan n gomez, łukasz kaiser, and illia polosukhin. 2017. attention is all\n",
            "you need. in neurips .\n",
            "[50] jiapeng wang, lianwen jin, and kai ding. 2022. lilt: a simple yet effective\n",
            "language-independent layout transformer for structured document under-\n",
            "standing. in acl.\n",
            "[51] jiapeng wang, chongyu liu, lianwen jin, guozhi tang, jiaxin zhang, shuaitao\n",
            "zhang, qianying wang, yaqiang wu, and mingxiang cai. 2021. towards robust\n",
            "visual information extraction in real world: new dataset and novel solution. in\n",
            "aaai .\n",
            "[52] te-lin wu, cheng li, mingyang zhang, tao chen, spurthi amba hombaiah, and\n",
            "michael bendersky. 2021. lampret: layout-aware multimodal pretraining for\n",
            "document understanding. arxiv preprint arxiv:2104.08405 (2021).\n",
            "[53] yuxin wu, alexander kirillov, francisco massa, wan-yen lo, and ross girshick.\n",
            "2019. detectron2. https://github.com/facebookresearch/detectron2.\n",
            "[54] yiheng xu, minghao li, lei cui, shaohan huang, furu wei, and ming zhou. 2020.\n",
            "layoutlm: pre-training of text and layout for document image understanding. in\n",
            "kdd .\n",
            "[55] yiheng xu, tengchao lv, lei cui, guoxin wang, yijuan lu, dinei florencio, cha\n",
            "zhang, and furu wei. 2021. layoutxlm: multimodal pre-training for multilingual\n",
            "visually-rich document understanding. arxiv preprint arxiv:2104.08836 (2021).\n",
            "[56] yang xu, yiheng xu, tengchao lv, lei cui, furu wei, guoxin wang, yijuan lu,\n",
            "dinei florencio, cha zhang, wanxiang che, min zhang, and lidong zhou. 2021.\n",
            "layoutlmv2: multi-modal pre-training for visually-rich document understand-\n",
            "ing. in acl.\n",
            "[57] hongwei xue, yupan huang, bei liu, houwen peng, jianlong fu, houqiang li,\n",
            "and jiebo luo. 2021. probing inter-modality: visual parsing with self-attention\n",
            "for vision-and-language pre-training. in neurips .\n",
            "[58] peng zhang, yunlu xu, zhanzhan cheng, shiliang pu, jing lu, liang qiao, yi\n",
            "niu, and fei wu. 2020. trie: end-to-end text reading and information extraction\n",
            "for document understanding. in acm multimedia .\n",
            "[59] xu zhong, jianbin tang, and antonio jimeno yepes. 2019. publaynet: largest\n",
            "dataset ever for document layout analysis. in icdar .\n",
            "ocr-free document understanding transformer\n",
            "geewook kim1∗, teakgyu hong4†, moonbin yim2†, jeongyeon nam1,\n",
            "jinyoung park5†, jinyeong yim6†, wonseok hwang7†, sangdoo yun3,\n",
            "dongyoon han3, and seunghyun park1\n",
            "1naver clova2naver search3naver ai lab\n",
            "4upstage5tmax6google7lbox\n",
            "abstract. understanding document images ( e.g., invoices) is a core but\n",
            "challenging task since it requires complex functions such as reading text\n",
            "and a holistic understanding of the document . current visual document\n",
            "understanding (vdu) methods outsource the task of reading text to off-\n",
            "the-shelf optical character recognition (ocr) engines and focus on the\n",
            "understanding task with the ocr outputs. although such ocr-based\n",
            "approaches have shown promising performance, they suffer from 1) high\n",
            "computational costs for using ocr; 2) inflexibility of ocr models on\n",
            "languages or types of documents; 3) ocr error propagation to the sub-\n",
            "sequent process. to address these issues, in this paper, we introduce a\n",
            "novel ocr-free vdu model named donut , which stands for document\n",
            "understanding transformer. as the first step in ocr-free vdu research,\n",
            "we propose a simple architecture ( i.e., transformer) with a pre-training\n",
            "objective ( i.e.,cross-entropy loss). donut is conceptually simple yet ef-\n",
            "fective. through extensive experiments and analyses, we show a simple\n",
            "ocr-free vdu model, donut, achieves state-of-the-art performances on\n",
            "various vdu tasks in terms of both speed and accuracy. in addition, we\n",
            "offer a synthetic data generator that helps the model pre-training to be\n",
            "flexible in various languages and domains. the code, trained model, and\n",
            "synthetic data are available at https://github.com/clovaai/donut .\n",
            "keywords: visual document understanding, document information\n",
            "extraction, optical character recognition, end-to-end transformer\n",
            "1 introduction\n",
            "document images, such as commercial invoices, receipts, and business cards,\n",
            "are easy to find in modern working environments. to extract useful informa-\n",
            "tion from such document images, visual document understanding (vdu) has\n",
            "not been only an essential task for industry, but also a challenging topic for re-\n",
            "searchers, with applications including document classification [27,1], information\n",
            "extraction [22,42], and visual question answering [44,57].\n",
            "∗corresponding author: gwkim.rsrch@gmail.com\n",
            "†this work was done while the authors were at naver clova.arxiv:2111.15664v5  [cs.lg]  6 oct 20222 g. kim et al.\n",
            "document image{ \"items\": [     {      \"name\": \"3002-kyoto choco mochi\",      \"count\": 2,      \"priceinfo\": {        \"unitprice\": 14000,        \"price\": 28000      }    }, ...  ],  \"total\": [ {       \"menuqty_cnt\": 4,       \"total_price\": 50000    }   ]}{ \"words\": [        {            \"bbox\":[[0.11,0.21],...,[0.19,0.22]],            \"text\": \"3002-kyoto\"        }, {            \"bbox\":[[0.21,0.22],...,[0.45,0.23]],            \"text\": \"choco\"        }, {            \"bbox\":[[0.46,0.22],...,[0.52,0.23]],            \"text\": \"mochi\"        }, …, {            \"bbox\":[[0.66,0.31],...,[0.72,0.32]],            \"text\": \"50.000\"        }    ]}structured information(a)(b)(c)(d)\n",
            "fig. 1. the schema of the conventional document information extraction\n",
            "(ie) pipeline. (a) the goal is to extract the structured information from a given semi-\n",
            "structured document image. in the pipeline, (b) text detection is conducted to obtain\n",
            "text locations and (c) each box is passed to the recognizer to comprehend characters.\n",
            "(d) finally, the recognized texts and its locations are passed to the following module\n",
            "to be processed for the desired structured form of the information\n",
            "imageocrdownstream modeloutputas-is (ocr + bert, layout lm, …)\n",
            "imagee2e modeloutputdonut 🍩\n",
            "(proposed)1.20.9time (sec/img)9182accuracy (%)0.8\n",
            "~0.6 sec143179+amemory (m)\n",
            "(a) pipeline overview. (b) system benchmarks.\n",
            "fig. 2. the pipeline overview and benchmarks. the proposed end-to-end model,\n",
            "donut , outperforms the recent ocr-dependent vdu models in memory, time cost\n",
            "and accuracy. performances on visual document ie [45] are shown in (b). more results\n",
            "on various vdu tasks are available at section 3 showing the same trend\n",
            "current vdu methods [22,24,65,64,18] solve the task in a two-stage manner:\n",
            "1) reading the texts in the document image; 2) holistic understanding of the doc-\n",
            "ument. they usually rely on deep-learning-based optical character recognition\n",
            "(ocr) [4,3] for the text reading task and focus on modeling the understanding\n",
            "part. for example, as shown in figure 1, a conventional pipeline for extracting\n",
            "structured information from documents (a.k.a. document parsing) consists of\n",
            "three separate modules for text detection, text recognition, and parsing [22,24].\n",
            "however, the ocr-dependent approach has critical problems. first of all, us-\n",
            "ing ocr as a pre-processing method is expensive. we can utilize pre-trained off-\n",
            "the-shelf ocr engines; however, the computational cost for inference would be\n",
            "expensive for high-quality ocr results. moreover, the off-the-shelf ocr meth-\n",
            "ods rarely have flexibility dealing with different languages or domain changes,\n",
            "which may lead to poor generalization ability. if we train an ocr model, it also\n",
            "requires extensive training costs and large-scale datasets [4,3,39,46]. another\n",
            "problem is, ocr errors would propagate to the vdu system and negatively\n",
            "influence subsequent processes [54,23]. this problem becomes more severe inocr-free document understanding transformer 3\n",
            "languages with complex character sets, such as korean or chinese, where the\n",
            "quality of ocr is relatively low [50]. to deal with this, post-ocr correction\n",
            "module [51,50,10] is usually adopted. however, it is not a practical solution for\n",
            "real application environments since it increases the entire system size and main-\n",
            "tenance cost.\n",
            "we go beyond the traditional framework by modeling a direct mapping from\n",
            "a raw input image to the desired output without ocr. we introduce a new\n",
            "ocr-free vdu model to address the problems induced by the ocr-dependency.\n",
            "our model is based on transformer-only architecture, referred to as document\n",
            "understanding transformer ( donut ), following the huge success in vision and\n",
            "language [8,9,29]. we present a minimal baseline including a simple architecture\n",
            "and pre-training method. despite its simplicity, donut shows comparable or\n",
            "better overall performance than previous methods as shown in figure 2.\n",
            "we take pre-train-and-fine-tune scheme [8,65] on donut training. in the\n",
            "pre-training phase, donut learns how to read the texts by predicting the next\n",
            "words by conditioning jointly on the image and previous text contexts. donut\n",
            "is pre-trained with document images and their text annotations. since our pre-\n",
            "training objective is simple ( i.e., reading the texts), we can realize domain and\n",
            "language flexibility straightforwardly pre-training with synthetic data. during\n",
            "fine-tuning stage, donut learns how to understand the whole document accord-\n",
            "ing to the downstream task. we demonstrate donut has a strong understanding\n",
            "ability through extensive evaluation on various vdu tasks and datasets. the\n",
            "experiments show a simple ocr-free vdu model can achieve state-of-the-art\n",
            "performance in terms of both speed and accuracy.\n",
            "the contributions are summarized as follows:\n",
            "1. we propose a novel ocr-free approach for vdu. to the best of our knowl-\n",
            "edge, this is the first method based on an ocr-free transformer trained in\n",
            "end-to-end manner.\n",
            "2. we introduce a simple pre-training scheme that enables the utilization of\n",
            "synthetic data. by using our generator synthdog, we show donut can\n",
            "easily be extended to a multi-lingual setting, which is not applicable for the\n",
            "conventional approaches that need to retrain an off-the-shelf ocr engine.\n",
            "3. we conduct extensive experiments and analyses on both public benchmarks\n",
            "and private industrial datasets, showing that the proposed method achieves\n",
            "not only state-of-the-art performances on benchmarks but also has many\n",
            "practical advantages (e.g., cost-effective ) in real-world applications.\n",
            "4. the codebase, pre-trained model, and synthetic data are available at github.1\n",
            "2 method\n",
            "2.1 preliminary: background\n",
            "there have been various visual document understanding (vdu) methods to un-\n",
            "derstand and extract essential information from the semi-structured documents\n",
            "such as receipts [20,25,18], invoices [49], and form documents [14,6,43].\n",
            "1https://github.com/clovaai/donut .4 g. kim et al.\n",
            "<vqa><question>what is the price of choco mochi?</question><answer>converted jsontransformer encoder\n",
            "input image and prompt\n",
            "transformer decoderdonut 🍩\n",
            "<classification><parsing><class>receipt</class></classification>14,000</answer></vqa><item><name>3002-kyoto choco mochi</name>・・・ </parsing>{ \"items\": [{\"name\": \"3002-kyoto choco mochi\",      \"count\": 2,      \"unitprice\": 14000, …}], … }output sequence{ \"class\":\"receipt\" }{ \"question\": \"what is the price of choco mochi?\",   \"answer\": \"14,000\" }\n",
            "fig. 3. the pipeline of donut. the encoder maps a given document image into\n",
            "embeddings. with the encoded embeddings, the decoder generates a sequence of tokens\n",
            "that can be converted into a target type of information in a structured form\n",
            "earlier vdu attempts have been done with ocr-independent visual back-\n",
            "bones [27,1,15,12,31], but the performances are limited. later, with the remark-\n",
            "able advances of ocr [4,3] and bert [8], various ocr-dependent vdu models\n",
            "have been proposed by combining them [22,24,23]. more recently, in order to get\n",
            "a more general vdu, most state-of-the-arts [64,18] use both powerful ocr en-\n",
            "gines and large-scale real document image data (e.g., iit-cdip [32]) for a model\n",
            "pre-training. although they showed remarkable advances in recent years, extra\n",
            "effort is required to ensure the performance of an entire vdu model by using\n",
            "the off-the-shelf ocr engine.\n",
            "2.2 document understanding transformer\n",
            "donut is an end-to-end (i.e., self-contained) vdu model for general understand-\n",
            "ing of document images. the architecture of donut is quite simple, which con-\n",
            "sists of a transformer [58,9]-based visual encoder and textual decoder modules.\n",
            "note that donut does not rely on any modules related to ocr functionality\n",
            "but uses a visual encoder for extracting features from a given document im-\n",
            "age. the following textual decoder maps the derived features into a sequence\n",
            "of subword tokens to construct a desired structured format (e.g., json). each\n",
            "model component is transformer-based, and thus the model is trained easily in\n",
            "an end-to-end manner. the overall process of donut is illustrated in figure 3.\n",
            "encoder. the visual encoder converts the input document image x∈rh×w×c\n",
            "into a set of embeddings {zi|zi∈rd,1≤i≤n}, where nis feature map size or the\n",
            "number of image patches and dis the dimension of the latent vectors of the\n",
            "encoder. note that cnn-based models [17] or transformer-based models [9,40]\n",
            "can be used as the encoder network. in this study, we use swin transformer [40]\n",
            "because it shows the best performance in our preliminary study in document\n",
            "parsing. swin transformer first splits the input image xinto non-overlapping\n",
            "patches. swin transformer blocks, consist of a shifted window-based multi-head\n",
            "self-attention module and a two-layer mlp, are applied to the patches. then,\n",
            "patch merging layers are applied to the patch tokens at each stage. the output\n",
            "of the final swin transformer block {z}is fed into the following textual decoder.ocr-free document understanding transformer 5\n",
            "decoder. given the {z}, the textual decoder generates a token sequence ( yi)m\n",
            "i=1,\n",
            "where yi∈rvis an one-hot vector for the i-th token, vis the size of token vo-\n",
            "cabulary, and mis a hyperparameter, respectively. we use bart [33] as the\n",
            "decoder architecture. specifically, we initialize the decoder model weights with\n",
            "those from the publicly available2pre-trained multi-lingual bart model[38].\n",
            "model input. following the original transformer [58], we use a teacher-forcing\n",
            "scheme [62], which is a model training strategy that uses the ground truth as\n",
            "input instead of model output from a previous time step. in the test phase,\n",
            "inspired by gpt-3 [5], the model generates a token sequence given a prompt.\n",
            "we add new special tokens for the prompt for each downstream task in our\n",
            "experiments. the prompts that we use for our applications are shown with the\n",
            "desired output sequences in figure 3. illustrative explanations for the teacher-\n",
            "forcing strategy and the decoder output format are available in appendix a.4.\n",
            "output conversion. the output token sequence is converted to a desired\n",
            "structured format. we adopt a json format due to its high representation\n",
            "capacity. as shown in figure 3, a token sequence is one-to-one invertible to a\n",
            "json data. we simply add two special tokens [start ∗]and [end ∗], where ∗\n",
            "indicates each field to extract. if the output token sequence is wrongly structured,\n",
            "we simply treat the field is lost. for example, if there is only [start name] exists\n",
            "but no [end name] , we assume the model fails to extract “name” field. this\n",
            "algorithm can easily be implemented with simple regular expressions [11].\n",
            "2.3 pre-training\n",
            "task. the model is trained to read all texts in the image in reading order (from\n",
            "top-left to bottom-right, basically). the objective is to minimize cross-entropy\n",
            "loss of next token prediction by jointly conditioning on the image and previous\n",
            "contexts. this task can be interpreted as a pseudo-ocr task. the model is\n",
            "trained as a visual language model over the visual corpora, i.e., document images.\n",
            "visual corpora. we use iit-cdip [32], which is a set of 11m scanned english\n",
            "document images. a commercial clova ocr api is applied to get the pseudo\n",
            "text labels. as aforementioned, however, this kind of dataset is not always avail-\n",
            "able, especially for languages other than english. to alleviate the dependencies,\n",
            "we build a scalable synth eticdocument generator , referred to as synthdog .\n",
            "using the synthdog and chinese, japanese, korean and english wikipedia, we\n",
            "generated 0.5m samples per language.\n",
            "synthetic document generator. the pipeline of image rendering basically\n",
            "follows yim et al. [67]. as shown in figure 4, the generated sample consists of\n",
            "2https://huggingface.co/hyunwoongko/asian-bart-ecjk .6 g. kim et al.\n",
            "fig. 4. generated english, chinese, japanese, and korean samples with\n",
            "synthdog. heuristic random patterns are applied to mimic the real documents\n",
            "several components; background, document, text, and layout. background image\n",
            "is sampled from imagenet [7], and a texture of document is sampled from the\n",
            "collected paper photos. words and phrases are sampled from wikipedia. layout\n",
            "is generated by a simple rule-based algorithm that randomly stacks grids. in\n",
            "addition, several image rendering techniques [13,41,67] are applied to mimic\n",
            "real documents. the generated examples are shown in figure 4. more details of\n",
            "synthdog are available in the code1and appendix a.2.\n",
            "2.4 fine-tuning\n",
            "after the model learns how to read , in the application stage (i.e., fine-tuning), we\n",
            "teach the model how to understand the document image. as shown in figure 3,\n",
            "we interpret all downstream tasks as a json prediction problem.\n",
            "the decoder is trained to generate a token sequence that can be converted\n",
            "into a json that represents the desired output information. for example, in the\n",
            "document classification task, the decoder is trained to generate a token sequence\n",
            "[start class][memo][end class] which is 1-to-1 invertible to a json {“class”:\n",
            "“memo” }. we introduce some special tokens (e.g., [memo] is used for representing\n",
            "the class “memo”), if such replacement is available in the target task.\n",
            "3 experiments and analyses\n",
            "in this section, we present donut fine-tuning results on three vdu applications\n",
            "on six different datasets including both public benchmarks and private industrial\n",
            "service datasets. the samples are shown in figure 5.\n",
            "3.1 downstream tasks and datasets\n",
            "document classification. to see whether the model can distinguish across\n",
            "different types of documents, we test a classification task. unlike other models\n",
            "that predict the class label via a softmax on the encoded embedding, donut\n",
            "generate a json that contains class information to maintain the uniformity of\n",
            "the task-solving method. we report overall classification accuracy on a test set.\n",
            "rvl-cdip. the rvl-cdip dataset [16] consists of 400k images in 16 classes,\n",
            "with 25k images per class. the classes include letter, memo, email, and so on.\n",
            "there are 320k training, 40k validation, and 40k test images.ocr-free document understanding transformer 7\n",
            "form\n",
            "handwritten(a)(b)\n",
            "(c)\n",
            "q: what is the extension number as per the voucher? a: (910) 741-0673\n",
            "fig. 5. samples of the downstream datasets. (a) document classification. (b)\n",
            "document information extraction. (c) document visual question answering\n",
            "document information extraction. to see the model fully understands\n",
            "the complex layouts and contexts in documents, we test document information\n",
            "extraction (ie) tasks on various real document images including both public\n",
            "benchmarks and real industrial datasets. in this task, the model aims to map\n",
            "each document to a structured form of information that is consistent with the\n",
            "target ontology or database schema. see figure 1 for an illustrative example. the\n",
            "model should not only read the characters well, but also understand the layouts\n",
            "and semantics to infer the groups and nested hierarchies among the texts.\n",
            "we evaluate the models with two metrics; field-level f1 score [22,65,18] and\n",
            "tree edit distance (ted) based accuracy [68,70,23]. the f1 checks whether the\n",
            "extracted field information is in the ground truth. even if a single character is\n",
            "missed, the score assumes the field extraction is failed. although f1 is simple\n",
            "and easy to understand, there are some limitations. first, it does not take into\n",
            "account partial overlaps. second, it can not measure the predicted structure (e.g.,\n",
            "groups and nested hierarchy). to assess overall accuracy, we also use another\n",
            "metric based on ted [68], that can be used for any documents represented as\n",
            "trees. it is calculated as, max(0 ,1−ted(pr ,gt)/ted( ϕ,gt)), where gt, pr, and ϕ\n",
            "stands for ground truth, predicted, and empty trees respectively. similar metrics\n",
            "are used in recent works on document ie [70,23]\n",
            "we use two public benchmark datasets as well as two private industrial\n",
            "datasets which are from our active real-world service products. each dataset\n",
            "is explained in the followings.\n",
            "cord. the consolidated receipt dataset (cord)3[45] is a public benchmark\n",
            "that consists of 0.8k train, 0.1k valid, 0.1k test receipt images. the letters\n",
            "of receipts is in latin alphabet. the number of unique fields is 30 containing\n",
            "menu name, count, total price, and so on. there are complex structures (i.e.,\n",
            "3https://huggingface.co/datasets/naver-clova-ix/cord-v1 .8 g. kim et al.\n",
            "nested groups and hierarchies such as items>item> {name, count, price }) in the\n",
            "information. see figure 1 for more details.\n",
            "ticket. this is a public benchmark dataset [12] that consists of 1.5k train and\n",
            "0.4k test chinese train ticket images. we split 10% of the train set as a validation\n",
            "set. there are 8 fields which are ticket number, starting station, train number,\n",
            "and so on. the structure of information is simple and all keys are guaranteed to\n",
            "appear only once and the location of each field is fixed.\n",
            "business card (in-service data). this dataset is from our active products that\n",
            "are currently deployed. the dataset consists of 20k train, 0.3k valid, 0.3k test\n",
            "japanese business cards. the number of fields is 11, including name, company,\n",
            "address, and so on. the structure of information is similar to the ticket dataset.\n",
            "receipt (in-service data). this dataset is also from one of our real products.\n",
            "the dataset consists of 40k train, 1k valid, 1k test korean receipt images.\n",
            "the number of unique field is 81, which includes store information, payment\n",
            "information, price information, and so on. each sample has complex structures\n",
            "compared to the aforementioned datasets. due to industrial policies, not all\n",
            "samples can publicly be available. some real-like high-quality samples are shown\n",
            "in figure 5 and in the supplementary material.\n",
            "document visual question answering. to validate the further capacity of\n",
            "the model, we conduct a document visual question answering task (docvqa). in\n",
            "this task, a document image and question pair is given and the model predicts the\n",
            "answer for the question by capturing both visual and textual information within\n",
            "the image. we make the decoder generate the answer by setting the question as\n",
            "a starting prompt to keep the uniformity of the method (see figure 3).\n",
            "docvqa. the dataset is from document visual question answering competi-\n",
            "tion4and consists of 50k questions defined on more than 12k documents [44].\n",
            "there are 40k train, 5k valid, and 5k test questions. the evaluation metric is\n",
            "anls (average normalized levenshtein similarity) which is an edit-distance-\n",
            "based metric. the score on the test set is measured via the evaluation site.\n",
            "3.2 setups\n",
            "we use swin-b [40] as a visual encoder of donut with slight modification.\n",
            "we set the layer numbers and window size as {2,2,14,2}and 10. in further\n",
            "consideration of the speed-accuracy trade-off, we use the first four layers of bart\n",
            "as a decoder. as explained in section 2.3, we train the multi-lingual donut using\n",
            "the 2m synthetic and 11m iit-cdip scanned document images. we pre-train the\n",
            "model for 200k steps with 64 a100 gpus and a mini-batch size of 196. we use\n",
            "adam [30] optimizer, the learning rate is scheduled and the initial rate is selected\n",
            "4https://rrc.cvc.uab.es/?ch=17 .ocr-free document understanding transformer 9\n",
            "table 1. classification results on the rvl-cdip dataset. donut achieves\n",
            "state-of-the-are performance with reasonable speed and model size efficiency. donut\n",
            "is a general purpose backbone but does not rely on ocr while other recent backbones\n",
            "(e.g., layoutlm) do.†# parameters for ocr should be considered for non-e2e models\n",
            "ocr #params time (ms) accuracy (%)\n",
            "bert ✓110m + α†1392 89.81\n",
            "roberta ✓125m + α†1392 90.06\n",
            "layoutlm ✓113m + α†1396 91.78\n",
            "layoutlm (w/ image) ✓160m + α†1426 94.42\n",
            "layoutlmv2 ✓200m + α†1489 95.25\n",
            "donut (proposed) 143m 752 95.30\n",
            "from 1e-5 to 1e-4. the input resolution is set to 2560 ×1920 and a max length\n",
            "in the decoder is set to 1536. all fine-tuning results are achieved by starting\n",
            "from the pre-trained multi-lingual model. some hyperparameters are adjusted\n",
            "at fine-tuning and in ablation studies. we use 960 ×1280 for train tickets and\n",
            "business card parsing tasks. we fine-tune the model while monitoring the edit\n",
            "distance over token sequences. the speed of donut is measured on a p40 gpu,\n",
            "which is much slower than a100. for the ocr based baselines, states-of-the-art\n",
            "ocr engines are used, including ms ocr api used in [64] and clova ocr\n",
            "api5used in [24,23]. an analysis on ocr engines is available in section 3.4.\n",
            "more details of ocr and training setups are available in appendix a.1 and a.5.\n",
            "3.3 experimental results\n",
            "document classification. the results are shown in table 1. without re-\n",
            "lying on any other resource (e.g., off-the-shelf ocr engine), donut shows a\n",
            "state-of-the-art performance among the general-purpose vdu models such as\n",
            "layoutlm [65] and layoutlmv2 [64]. in particular, donut surpasses the lay-\n",
            "outlmv2 accuracy reported in [64], while using fewer parameters with the 2x\n",
            "faster speed. note that the ocr-based models must consider additional model\n",
            "parameters and speed for the entire ocr framework, which is not small in gen-\n",
            "eral. for example, a recent advanced ocr-based model [4,3] requires more than\n",
            "80m parameters. also, training and maintaining the ocr-based systems are\n",
            "costly [23], leading to needs for the donut-like end-to-end approach.\n",
            "document information extraction. table 2 shows the results on the four\n",
            "different document ie tasks. the first group uses a conventional bio-tagging-\n",
            "based ie approach [22]. we follows the conventions in ie [65,18]. ocr extracts\n",
            "texts and bounding boxes from the image, and then the serialization module sorts\n",
            "all texts with geometry information within the bounding box. the bio-tagging-\n",
            "based named entity recognition task performs token-level tag classification upon\n",
            "5https://clova.ai/ocr .10 g. kim et al.\n",
            "table 2. performances on various document ie tasks. the field-level f1 scores\n",
            "and tree-edit-distance-based accuracies are reported. donut shows the best accuracies\n",
            "for all domains with significantly faster inference speed.†parameters for vocabulary\n",
            "are omitted for fair comparisons among multi-lingual models.‡# parameters for ocr\n",
            "should be considered.∗official multi-lingual extension models are used\n",
            "cord [45] ticket [12] business card receipt\n",
            "ocr #params time (s) f1 acc. time (s) f1 acc. time (s) f1 acc. time (s) f1 acc.\n",
            "bert∗[22] ✓86†\n",
            "m+α‡1.6 73.0 65.5 1.7 74.3 82.4 1.5 40.8 72.1 2.5 70.3 54.1\n",
            "bros [18] ✓86†\n",
            "m+α‡1.7 74.7 70.0\n",
            "layoutlm [65] ✓89†\n",
            "m+α‡1.7 78.4 81.3\n",
            "layoutlmv2∗[64,66] ✓179†\n",
            "m+α‡1.7 78.9 82.4 1.8 87.2 90.1 1.6 52.2 83.0 2.6 72.9 78.0\n",
            "donut 143†\n",
            "m 1.2 84.1 90.9 0.6 94.1 98.7 1.4 57.8 84.4 1.9 78.6 88.6\n",
            "spade∗[25] ✓93†\n",
            "m+α‡4.0 74.0 75.8 4.5 14.9 29.4 4.3 32.3 51.3 7.3 64.1 53.2\n",
            "wyvern∗[21] ✓106†\n",
            "m+α‡1.2 43.3 46.9 1.5 41.8 54.8 1.7 29.9 51.5 3.4 71.5 82.9\n",
            "the ordered texts to generate a structured form. we test three general-purpose\n",
            "vdu backbones, bert [8], bros [18], layoutlm [65], and layoutlmv2 [64,66].\n",
            "we also test two recently proposed ie models, spade [24] and wyvern [23].\n",
            "spade is a graph-based ie method that predicts relations between bounding\n",
            "boxes. wyvern is an transformer encoder-decoder model that directly gen-\n",
            "erates entities with structure given ocr outputs. wyvern is different from\n",
            "donut in that it takes the ocr output as its inputs.\n",
            "for all domains, including public and private in-service datasets, donut shows\n",
            "the best scores among the comparing models. by measuring both f1 and ted-\n",
            "based accuracy, we observe not only donut can extract key information but\n",
            "also predict complex structures among the field information. we observe that\n",
            "a large input resolution gives robust accuracies but makes the model slower.\n",
            "for example, the performance on the cord with 1280 ×960 was 0.7 sec./image\n",
            "and 91.1 accuracy. but, the large resolution showed better performances on the\n",
            "low-resource situation. the detailed analyses are in section 3.4. unlike other\n",
            "baselines, donut shows stable performance regardless of the size of datasets and\n",
            "complexity of the tasks (see figure 5). this is a significant impact as the target\n",
            "tasks are already actively used in industries.\n",
            "document visual question answering. table 3 shows the results on the\n",
            "docvqa dataset. the first group is the general-purposed vdu backbones whose\n",
            "scores are from the layoutlmv2 paper [64]. we measure the running time with\n",
            "ms ocr api used in [64]. the model in the third group is a docvqa-specific-\n",
            "purposed fine-tuning model of layoutlmv2, whose inference results are available\n",
            "in the official leader-board.6\n",
            "as can be seen, donut achieves competitive scores with the baselines that\n",
            "are dependent on external ocr engines. especially, donut shows that it is robust\n",
            "to the handwritten documents, which is known to be challenging to process. in\n",
            "the conventional approach, adding a post-processing module that corrects ocr\n",
            "6https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1 .ocr-free document understanding transformer 11\n",
            "table 3. average normalized levenshtein similarity (anls) scores on\n",
            "docvqa. donut shows a promising result without ocr.∗donut shows a high\n",
            "anls score on the handwritten documents which are known to be challenging due to\n",
            "the difficulty of handwriting ocr (see figure 6).†token embeddings for english is\n",
            "counted for a fair comparison.‡# parameters for ocr should be considered\n",
            "fine-tuning set ocr #params†time (ms)anls\n",
            "test setanls∗\n",
            "handwritten\n",
            "bert [64] train set ✓110m + α‡1517 63.5 n/a\n",
            "layoutlm[65] train set ✓113m + α‡1519 69.8 n/a\n",
            "layoutlmv2[64] train set ✓200m + α‡1610 78.1 n/a\n",
            "donut train set 176m 782 67.5 72.1\n",
            "layoutlmv2-large-qg[64] train + dev + qg ✓390m + α‡1698 86.7 67.3\n",
            "q: what is the name of the passenger? \n",
            "answer: dr. william j. darby \n",
            "donut: dr. william j. darby \n",
            "layoutlmv2-large-qg: dr. william j. jarry q: what is the publication no.? \n",
            "answer: 540\n",
            "donut: 943  (another number in the image is extracted) \n",
            "layoutlmv2-large-qg: 540\n",
            "q: what is the phone number given? \n",
            "answer: 336-723-6100 \n",
            "donut: 336-723-6100 \n",
            "layoutlmv2-large-qg: 336-723- 4100 \n",
            "fig. 6. examples of donut and layoutlmv2 outputs on docvqa. the ocr-\n",
            "errors make a performance upper-bound of the ocr-dependent baselines, e.g., lay-\n",
            "outlmv2 (left and middle examples). due to the input resolution constraint of the\n",
            "end-to-end pipeline, donut miss some tiny texts in large-scale images (right example)\n",
            "but this could be mitigated by scaling the input image size (see section 3.4)\n",
            "errors is an option to strengthen the pipeline [51,50,10] or adopting an encoder-\n",
            "decoder architecture on the ocr outputs can mitigate the problems of ocr\n",
            "errors [23]. however, this kind of approaches tend to increase the entire system\n",
            "size and maintenance cost. donut shows a completely different direction. some\n",
            "inference results are shown in figure 6. the samples show the current strengths\n",
            "of donut as well as the left challenges in the donut-like end-to-end approach.\n",
            "further analysis and ablation is available in section 3.4.\n",
            "3.4 further studies\n",
            "in this section, we study several elements of understanding donut. we show some\n",
            "striking characteristics of donut through the experiments and visualization.\n",
            "on pre-training strategy. we test several pre-training tasks for vdus. fig-\n",
            "ure 7(a) shows that the donut pre-training task (i.e., text reading) is the most12 g. kim et al.\n",
            "no pretraining classification captioningread (synthdog)read (cdip) read (both) resnet-152 effnetv2vit-bswin-b 18 swin-b 14 640x640 960x960 1280x960 2560×1920\n",
            " 30507090accuracy\n",
            "020406080docvqa score(a) pretrain strategy                                          (b) backbone                                         (c) resolution\n",
            "cord (accuracy)\n",
            "docvqa score (anls)\n",
            "fig. 7. analysis on (a) pre-training strategies, (b) image backbones, and (c)\n",
            "input resolutions. performances on cord [45] and docvqa [44] are shown\n",
            "simple yet effective approach. other tasks that impose a general knowledge of\n",
            "images and texts on models, e.g., image captioning, show little gains in the\n",
            "fine-tuning tasks. for the text reading tasks, we verify three options, synthdog\n",
            "only, iit-cdip only, and both. note that synthetic images were enough for the\n",
            "document ie task in our analysis. however, in the docvqa task, it was impor-\n",
            "tant to see the real images. this is probably because the image distributions of\n",
            "iit-cdip and docvqa are similar [44].\n",
            "on encoder backbone. here, we study popular image classification back-\n",
            "bones that show superior performance in traditional vision tasks to measure\n",
            "their performance in vdu tasks. the figure 7(b) shows the comparison results.\n",
            "we use all the backbones pre-trained on imagenet [7]. efficientnetv2 [55] and\n",
            "swin transformer [40] outperform others on both datasets. we argue that this is\n",
            "due to the high expressiveness of the backbones, which were shown by the strik-\n",
            "ing scores on several downstream tasks as well. we choose swin transformer\n",
            "due to the high scalability of the transformer-based architecture and higher\n",
            "performance over the efficientnetv2’s.\n",
            "on input resolution. the figure 7(c) shows the performance of donut grows\n",
            "rapidly as we set a larger input size. this gets clearer in the docvqa where the\n",
            "images are larger with many tiny texts. but, increasing the size for a precise\n",
            "result incurs bigger computational costs. using an efficient attention mecha-\n",
            "nism [60] may avoid the matter in architectural design, but we use the original\n",
            "transformer [58] as we aim to present a simpler architecture in this work.\n",
            "on text localization. to see how the model behaves, we visualize the corss\n",
            "attention maps of the decoder given an unseen document image. as can be seen\n",
            "in figure 8, the model shows meaningful results that can be used as an auxiliary\n",
            "indicator. the model attends to a desired location in the given image.\n",
            "on ocr system. we test four widely-used public ocr engines (see fig-\n",
            "ure 9). the results show that the performances (i.e., speed and accuracy) of the\n",
            "conventional ocr-based methods heavily rely on the off-the-shelf ocr engine.\n",
            "more details of the ocr engines are available in appendix a.1.ocr-free document understanding transformer 13\n",
            "k\n",
            "chocomochiyoto3002-\n",
            "fig. 8. visualization of cross-attention maps in the decoder and its applica-\n",
            "tion to text localization. donut is trained without any supervision for the localiza-\n",
            "tion. the donut decoder attends proper text regions to process the image\n",
            "layoutlmv2 bert donut405060708090 accuracy\n",
            "1280   2560easyocr\n",
            "paddleocr\n",
            "msazure\n",
            "clovaocr\n",
            "layoutlmv2 bert donut0.51.01.5 time(s/image)\n",
            "1280   2560easyocr\n",
            "paddleocr\n",
            "msazure\n",
            "clovaocr\n",
            "80 160 400 800\n",
            "number of samples5060708090 accuracy\n",
            "donut, 2560\n",
            "donut, 1280layoutlmv2\n",
            "bert\n",
            "fig. 9. comparison of bert, layoutlmv2 and donut on cord. the perfor-\n",
            "mances (i.e., speed and accuracy) of the ocr-based models extremely varies depending\n",
            "on what ocr engine is used (left and center). donut shows robust performances even\n",
            "in a low resourced situation showing the higher score only with 80 samples (right)\n",
            "on low resourced situation. we evaluate the models by limiting the size\n",
            "of training set of cord [45]. the performance curves are shown in the right\n",
            "figure 9. donut shows a robust performances. we also observe that a larger\n",
            "input resolution, 2560 ×1920, shows more robust scores on the extremely low-\n",
            "resourced situation, e.g., 80 samples. as can be seen, donut outperformed the\n",
            "layoutlmv2 accuracy only with 10% of the data, which is only 80 samples.\n",
            "4 related work\n",
            "4.1 optical character recognition\n",
            "recent trends of ocr study are to utilize deep learning models in its two sub-\n",
            "steps: 1) text areas are predicted by a detector; 2) a text recognizer then rec-\n",
            "ognizes all characters in the cropped image instances. both are trained with a\n",
            "large-scale datasets including the synthetic images [26,13] and real images [28,47].\n",
            "early detection methods used cnns to predict local segments and apply\n",
            "heuristics to merge them [19,69]. later, region proposal and bounding box regres-\n",
            "sion based methods were proposed [36]. recently, focusing on the homogeneity\n",
            "and locality of texts, component-level approaches were proposed [56,4].\n",
            "many modern text recognizer share a similar approach [37,53,52,59] that can\n",
            "be interpreted into a combination of several common deep modules [3]. given the\n",
            "cropped text instance image, most recent text recognition models apply cnns\n",
            "to encode the image into a feature space. a decoder is then applied to extract\n",
            "characters from the features.14 g. kim et al.\n",
            "4.2 visual document understanding\n",
            "classification of the document type is a core step towards automated document\n",
            "processing. early methods treated the problem as a general image classification,\n",
            "so various cnns were tested [27,1,15]. recently, with bert [8], the methods\n",
            "based on a combination of cv and nlp were widely proposed [65,34]. as a\n",
            "common approach, most methods rely on an ocr engine to extract texts; then\n",
            "the ocr-ed texts are serialized into a token sequence; finally they are fed into\n",
            "a language model (e.g., bert) with some visual features if available. although\n",
            "the idea is simple, the methods showed remarkable performance improvements\n",
            "and became a main trend in recent years [64,35,2].\n",
            "document ie covers a wide range of real applications [22,42], for example,\n",
            "given a bunch of raw receipt images, a document parser can automate a major\n",
            "part of receipt digitization, which has been required numerous human-labors\n",
            "in the traditional pipeline. most recent models [25,23] take the output of ocr\n",
            "as their input. the ocr results are then converted to the final parse through\n",
            "several processes, which are often complex. despite the needs in the industry,\n",
            "only a few works have been attempted on end-to-end parsing. recently, some\n",
            "works are proposed to simplify the complex parsing processes [25,23]. but they\n",
            "still rely on a separate ocr to extract text information.\n",
            "visual qa on documents seeks to answer questions asked on document im-\n",
            "ages. this task requires reasoning over visual elements of the image and general\n",
            "knowledge to infer the correct answer [44]. currently, most state-of-the-arts fol-\n",
            "low a simple pipeline consisting of applying ocr followed by bert-like trans-\n",
            "formers [65,64]. however, the methods work in an extractive manner by their\n",
            "nature. hence, there are some concerns for the question whose answer does not\n",
            "appear in the given image [57]. to tackle the concerns, generation-based methods\n",
            "have also been proposed [48].\n",
            "5 conclusions\n",
            "in this work, we propose a novel end-to-end framework for visual document un-\n",
            "derstanding. the proposed method, donut , directly maps an input document\n",
            "image into a desired structured output. unlike conventional methods, donut\n",
            "does not depend on ocr and can easily be trained in an end-to-end fashion. we\n",
            "also propose a synthetic document image generator, synthdog, to alleviate the\n",
            "dependency on large-scale real document images and we show that donut can\n",
            "be easily extended to a multi-lingual setting. we gradually trained the model\n",
            "from how to read tohow to understand through the proposed training pipeline.\n",
            "our extensive experiments and analysis on both external public benchmarks\n",
            "and private internal service datasets show higher performance and better cost-\n",
            "effectiveness of the proposed method. this is a significant impact as the target\n",
            "tasks are already practically used in industries. enhancing the pre-training ob-\n",
            "jective could be a future work direction. we believe our work can easily be\n",
            "extended to other domains/tasks regarding document understanding.ocr-free document understanding transformer 15\n",
            "references\n",
            "1. afzal, m.z., capobianco, s., malik, m.i., marinai, s., breuel, t.m.,\n",
            "dengel, a., liwicki, m.: deepdocclassifier: document classification with\n",
            "deep convolutional neural network. in: 2015 13th international conference\n",
            "on document analysis and recognition (icdar). pp. 1111–1115 (2015).\n",
            "https://doi.org/10.1109/icdar.2015.7333933 1, 4, 14\n",
            "2. appalaraju, s., jasani, b., kota, b.u., xie, y., manmatha, r.: docformer: end-to-\n",
            "end transformer for document understanding. in: proceedings of the ieee/cvf\n",
            "international conference on computer vision (iccv). pp. 993–1003 (october\n",
            "2021) 14\n",
            "3. baek, j., kim, g., lee, j., park, s., han, d., yun, s., oh, s.j., lee, h.: what is\n",
            "wrong with scene text recognition model comparisons? dataset and model analysis.\n",
            "in: proceedings of the ieee/cvf international conference on computer vision\n",
            "(iccv) (october 2019) 2, 4, 9, 13, 22\n",
            "4. baek, y., lee, b., han, d., yun, s., lee, h.: character region awareness for text\n",
            "detection. in: 2019 ieee/cvf conference on computer vision and pattern recog-\n",
            "nition (cvpr). pp. 9357–9366 (2019). https://doi.org/10.1109/cvpr.2019.00959\n",
            "2, 4, 9, 13, 22\n",
            "5. brown, t., mann, b., ryder, n., subbiah, m., kaplan, j.d., dhariwal, p., nee-\n",
            "lakantan, a., shyam, p., sastry, g., askell, a., agarwal, s., herbert-voss, a.,\n",
            "krueger, g., henighan, t., child, r., ramesh, a., ziegler, d., wu, j., winter, c.,\n",
            "hesse, c., chen, m., sigler, e., litwin, m., gray, s., chess, b., clark, j., berner,\n",
            "c., mccandlish, s., radford, a., sutskever, i., amodei, d.: language models are\n",
            "few-shot learners. in: larochelle, h., ranzato, m., hadsell, r., balcan, m.f., lin,\n",
            "h. (eds.) advances in neural information processing systems. vol. 33, pp. 1877–\n",
            "1901. curran associates, inc. (2020), https://proceedings.neurips.cc/paper/\n",
            "2020/file/1457c0d6bfcb4967418bfb8ac142f64a-paper.pdf 5\n",
            "6. davis, b., morse, b., cohen, s., price, b., tensmeyer, c.: deep visual template-free\n",
            "form parsing. in: 2019 international conference on document analysis and recog-\n",
            "nition (icdar). pp. 134–141 (2019). https://doi.org/10.1109/icdar.2019.00030\n",
            "3\n",
            "7. deng, j., dong, w., socher, r., li, l.j., li, k., fei-fei, l.: imagenet: a large-\n",
            "scale hierarchical image database. in: 2009 ieee conference on computer vision\n",
            "and pattern recognition. pp. 248–255. ieee (2009) 6, 12, 23\n",
            "8. devlin, j., chang, m.w., lee, k., toutanova, k.: bert: pre-training of deep\n",
            "bidirectional transformers for language understanding. in: proceedings of the 2019\n",
            "conference of the north american chapter of the association for computational\n",
            "linguistics: human language technologies, volume 1 (long and short papers).\n",
            "pp. 4171–4186. association for computational linguistics, minneapolis, minnesota\n",
            "(jun 2019). https://doi.org/10.18653/v1/n19-1423, https://aclanthology.org/\n",
            "n19-1423 3, 4, 10, 14, 27, 29\n",
            "9. dosovitskiy, a., beyer, l., kolesnikov, a., weissenborn, d., zhai, x., unterthiner,\n",
            "t., dehghani, m., minderer, m., heigold, g., gelly, s., uszkoreit, j., houlsby,\n",
            "n.: an image is worth 16x16 words: transformers for image recognition at scale.\n",
            "in: 9th international conference on learning representations, iclr 2021, virtual\n",
            "event, austria, may 3-7, 2021. openreview.net (2021), https://openreview.net/\n",
            "forum?id=yicbfdntty 3, 4\n",
            "10. duong, q., h¨ am¨ al¨ ainen, m., hengchen, s.: an unsupervised method for ocr\n",
            "post-correction and spelling normalisation for finnish. in: proceedings of the16 g. kim et al.\n",
            "23rd nordic conference on computational linguistics (nodalida). pp. 240–248.\n",
            "link¨ oping university electronic press, sweden, reykjavik, iceland (online) (may\n",
            "31–2 jun 2021), https://aclanthology.org/2021.nodalida-main.24 3, 11\n",
            "11. friedl, j.e.f.: mastering regular expressions. o’reilly, beijing, 3\n",
            "edn. (2006), https://www.safaribooksonline.com/library/view/\n",
            "mastering-regular-expressions/0596528124/ 5\n",
            "12. guo, h., qin, x., liu, j., han, j., liu, j., ding, e.: eaten: entity-aware at-\n",
            "tention for single shot visual text extraction. in: 2019 international confer-\n",
            "ence on document analysis and recognition (icdar). pp. 254–259 (2019).\n",
            "https://doi.org/10.1109/icdar.2019.00049 4, 8, 10, 22, 24, 25, 26\n",
            "13. gupta, a., vedaldi, a., zisserman, a.: synthetic data for text localisation in nat-\n",
            "ural images. in: proceedings of the ieee conference on computer vision and\n",
            "pattern recognition (cvpr) (june 2016) 6, 13\n",
            "14. hammami, m., h´ eroux, p., adam, s., d’andecy, v.p.: one-shot field spotting\n",
            "on colored forms using subgraph isomorphism. in: 2015 13th international con-\n",
            "ference on document analysis and recognition (icdar). pp. 586–590 (2015).\n",
            "https://doi.org/10.1109/icdar.2015.7333829 3\n",
            "15. harley, a.w., ufkes, a., derpanis, k.g.: evaluation of deep convolutional nets\n",
            "for document image classification and retrieval. in: 2015 13th international con-\n",
            "ference on document analysis and recognition (icdar). pp. 991–995 (2015).\n",
            "https://doi.org/10.1109/icdar.2015.7333910 4, 14\n",
            "16. harley, a.w., ufkes, a., derpanis, k.g.: evaluation of deep convolutional nets\n",
            "for document image classification and retrieval. in: 2015 13th international con-\n",
            "ference on document analysis and recognition (icdar). pp. 991–995 (2015).\n",
            "https://doi.org/10.1109/icdar.2015.7333910 6\n",
            "17. he, k., zhang, x., ren, s., sun, j.: deep residual learning for image recognition.\n",
            "in: 2016 ieee conference on computer vision and pattern recognition (cvpr).\n",
            "pp. 770–778 (2016). https://doi.org/10.1109/cvpr.2016.90 4\n",
            "18. hong, t., kim, d., ji, m., hwang, w., nam, d., park, s.: bros: a pre-trained\n",
            "language model focusing on text and layout for better key information extrac-\n",
            "tion from documents. proceedings of the aaai conference on artificial intelli-\n",
            "gence 36(10), 10767–10775 (jun 2022). https://doi.org/10.1609/aaai.v36i10.21322,\n",
            "https://ojs.aaai.org/index.php/aaai/article/view/21322 2, 3, 4, 7, 9, 10, 22,\n",
            "27\n",
            "19. huang, w., qiao, y., tang, x.: robust scene text detection with convolution\n",
            "neural network induced mser trees. in: fleet, d., pajdla, t., schiele, b., tuytelaars,\n",
            "t. (eds.) computer vision – eccv 2014. pp. 497–511. springer international\n",
            "publishing, cham (2014) 13\n",
            "20. huang, z., chen, k., he, j., bai, x., karatzas, d., lu, s., jawahar, c.v.: ic-\n",
            "dar2019 competition on scanned receipt ocr and information extraction. in: 2019\n",
            "international conference on document analysis and recognition (icdar). pp.\n",
            "1516–1520 (2019). https://doi.org/10.1109/icdar.2019.00244 3\n",
            "21. hwang, a., frey, w.r., mckeown, k.: towards augmenting lexical resources for\n",
            "slang and african american english. in: proceedings of the 7th workshop on\n",
            "nlp for similar languages, varieties and dialects. pp. 160–172. international\n",
            "committee on computational linguistics (iccl), barcelona, spain (online) (dec\n",
            "2020), https://aclanthology.org/2020.vardial-1.15 10\n",
            "22. hwang, w., kim, s., yim, j., seo, m., park, s., park, s., lee, j., lee, b., lee, h.:\n",
            "post-ocr parsing: building simple and robust parser via bio tagging. in: workshop\n",
            "on document intelligence at neurips 2019 (2019) 1, 2, 4, 7, 9, 10, 14, 28, 29ocr-free document understanding transformer 17\n",
            "23. hwang, w., lee, h., yim, j., kim, g., seo, m.: cost-effective end-to-end infor-\n",
            "mation extraction for semi-structured document images. in: proceedings of the\n",
            "2021 conference on empirical methods in natural language processing. pp. 3375–\n",
            "3383. association for computational linguistics, online and punta cana, do-\n",
            "minican republic (nov 2021). https://doi.org/10.18653/v1/2021.emnlp-main.271,\n",
            "https://aclanthology.org/2021.emnlp-main.271 2, 4, 7, 9, 10, 11, 14, 27\n",
            "24. hwang, w., yim, j., park, s., yang, s., seo, m.: spatial depen-\n",
            "dency parsing for semi-structured document information extraction. in:\n",
            "findings of the association for computational linguistics: acl-ijcnlp\n",
            "2021. pp. 330–343. association for computational linguistics, online (aug\n",
            "2021). https://doi.org/10.18653/v1/2021.findings-acl.28, https://aclanthology.\n",
            "org/2021.findings-acl.28 2, 4, 9, 10\n",
            "25. hwang, w., yim, j., park, s., yang, s., seo, m.: spatial depen-\n",
            "dency parsing for semi-structured document information extraction. in:\n",
            "findings of the association for computational linguistics: acl-ijcnlp\n",
            "2021. pp. 330–343. association for computational linguistics, online (aug\n",
            "2021). https://doi.org/10.18653/v1/2021.findings-acl.28, https://aclanthology.\n",
            "org/2021.findings-acl.28 3, 10, 14, 27\n",
            "26. jaderberg, m., simonyan, k., vedaldi, a., zisserman, a.: synthetic data and ar-\n",
            "tificial neural networks for natural scene text recognition. in: workshop on deep\n",
            "learning, nips (2014) 13\n",
            "27. kang, l., kumar, j., ye, p., li, y., doermann, d.s.: convolutional neural networks\n",
            "for document image classification. 2014 22nd international conference on pattern\n",
            "recognition pp. 3168–3172 (2014) 1, 4, 14\n",
            "28. karatzas, d., gomez-bigorda, l., nicolaou, a., ghosh, s., bagdanov, a., iwamura,\n",
            "m., matas, j., neumann, l., chandrasekhar, v.r., lu, s., shafait, f., uchida,\n",
            "s., valveny, e.: icdar 2015 competition on robust reading. in: 2015 13th interna-\n",
            "tional conference on document analysis and recognition (icdar). pp. 1156–1160\n",
            "(2015). https://doi.org/10.1109/icdar.2015.7333942 13\n",
            "29. kim, w., son, b., kim, i.: vilt: vision-and-language transformer without con-\n",
            "volution or region supervision. in: meila, m., zhang, t. (eds.) proceedings of\n",
            "the 38th international conference on machine learning. proceedings of ma-\n",
            "chine learning research, vol. 139, pp. 5583–5594. pmlr (18–24 jul 2021),\n",
            "http://proceedings.mlr.press/v139/kim21k.html 3\n",
            "30. kingma, d.p., ba, j.: adam: a method for stochastic optimization. in: bengio,\n",
            "y., lecun, y. (eds.) 3rd international conference on learning representations,\n",
            "iclr 2015, san diego, ca, usa, may 7-9, 2015, conference track proceedings\n",
            "(2015), http://arxiv.org/abs/1412.6980 8, 25\n",
            "31. klaiman, s., lehne, m.: docreader: bounding-box free training of a document\n",
            "information extraction model. in: document analysis and recognition – ic-\n",
            "dar 2021: 16th international conference, lausanne, switzerland, september\n",
            "5–10, 2021, proceedings, part i. p. 451–465. springer-verlag, berlin, heidel-\n",
            "berg (2021). https://doi.org/10.1007/978-3-030-86549-8 29,https://doi.org/10.\n",
            "1007/978-3-030-86549-8_29 4\n",
            "32. lewis, d., agam, g., argamon, s., frieder, o., grossman, d., heard, j.: building\n",
            "a test collection for complex document information processing. in: proceedings of\n",
            "the 29th annual international acm sigir conference on research and develop-\n",
            "ment in information retrieval. p. 665–666. sigir ’06, association for computing\n",
            "machinery, new york, ny, usa (2006). https://doi.org/10.1145/1148170.1148307,\n",
            "https://doi.org/10.1145/1148170.1148307 4, 518 g. kim et al.\n",
            "33. lewis, m., liu, y., goyal, n., ghazvininejad, m., mohamed, a., levy, o., stoy-\n",
            "anov, v., zettlemoyer, l.: bart: denoising sequence-to-sequence pre-training\n",
            "for natural language generation, translation, and comprehension. in: proceed-\n",
            "ings of the 58th annual meeting of the association for computational lin-\n",
            "guistics. pp. 7871–7880. association for computational linguistics, online (jul\n",
            "2020). https://doi.org/10.18653/v1/2020.acl-main.703, https://aclanthology.\n",
            "org/2020.acl-main.703 5\n",
            "34. li, c., bi, b., yan, m., wang, w., huang, s., huang, f., si, l.: struc-\n",
            "turallm: structural pre-training for form understanding. in: proceedings of the\n",
            "59th annual meeting of the association for computational linguistics and\n",
            "the 11th international joint conference on natural language processing (vol-\n",
            "ume 1: long papers). pp. 6309–6318. association for computational linguis-\n",
            "tics, online (aug 2021). https://doi.org/10.18653/v1/2021.acl-long.493, https:\n",
            "//aclanthology.org/2021.acl-long.493 14\n",
            "35. li, p., gu, j., kuen, j., morariu, v.i., zhao, h., jain, r., manjunatha, v., liu,\n",
            "h.: selfdoc: self-supervised document representation learning. in: 2021 ieee/cvf\n",
            "conference on computer vision and pattern recognition (cvpr). pp. 5648–5656\n",
            "(2021). https://doi.org/10.1109/cvpr46437.2021.00560 14\n",
            "36. liao, m., shi, b., bai, x., wang, x., liu, w.: textboxes: a fast text detector with\n",
            "a single deep neural network. proceedings of the aaai conference on artificial\n",
            "intelligence 31(1) (feb 2017). https://doi.org/10.1609/aaai.v31i1.11196, https:\n",
            "//ojs.aaai.org/index.php/aaai/article/view/11196 13\n",
            "37. liu, w., chen, c., wong, k.y.k., su, z., han, j.: star-net: a spatial attention\n",
            "residue network for scene text recognition. in: richard c. wilson, e.r.h., smith,\n",
            "w.a.p. (eds.) proceedings of the british machine vision conference (bmvc).\n",
            "pp. 43.1–43.13. bmva press (september 2016). https://doi.org/10.5244/c.30.43,\n",
            "https://dx.doi.org/10.5244/c.30.43 13\n",
            "38. liu, y., gu, j., goyal, n., li, x., edunov, s., ghazvininejad, m., lewis, m.,\n",
            "zettlemoyer, l.: multilingual denoising pre-training for neural machine translation.\n",
            "transactions of the association for computational linguistics 8, 726–742 (2020),\n",
            "https://aclanthology.org/2020.tacl-1.47 5\n",
            "39. liu, y., chen, h., shen, c., he, t., jin, l., wang, l.: abcnet: real-time scene text\n",
            "spotting with adaptive bezier-curve network. in: proceedings of the ieee/cvf\n",
            "conference on computer vision and pattern recognition (cvpr) (june 2020) 2\n",
            "40. liu, z., lin, y., cao, y., hu, h., wei, y., zhang, z., lin, s., guo, b.: swin trans-\n",
            "former: hierarchical vision transformer using shifted windows. in: proceedings of\n",
            "the ieee/cvf international conference on computer vision (iccv). pp. 10012–\n",
            "10022 (october 2021) 4, 8, 12\n",
            "41. long, s., yao, c.: unrealtext: synthesizing realistic scene text images from the\n",
            "unreal world. arxiv preprint arxiv:2003.10608 (2020) 6\n",
            "42. majumder, b.p., potti, n., tata, s., wendt, j.b., zhao, q., najork, m.: rep-\n",
            "resentation learning for information extraction from form-like documents. in:\n",
            "proceedings of the 58th annual meeting of the association for computational\n",
            "linguistics. pp. 6495–6504. association for computational linguistics, online\n",
            "(jul 2020). https://doi.org/10.18653/v1/2020.acl-main.580, https://www.aclweb.\n",
            "org/anthology/2020.acl-main.580 1, 14\n",
            "43. majumder, b.p., potti, n., tata, s., wendt, j.b., zhao, q., najork, m.: repre-\n",
            "sentation learning for information extraction from form-like documents. in: pro-\n",
            "ceedings of the 58th annual meeting of the association for computational lin-\n",
            "guistics. pp. 6495–6504. association for computational linguistics, online (julocr-free document understanding transformer 19\n",
            "2020). https://doi.org/10.18653/v1/2020.acl-main.580, https://aclanthology.\n",
            "org/2020.acl-main.580 3\n",
            "44. mathew, m., karatzas, d., jawahar, c.: docvqa: a dataset for vqa on document\n",
            "images. in: proceedings of the ieee/cvf winter conference on applications of\n",
            "computer vision. pp. 2200–2209 (2021) 1, 8, 12, 14\n",
            "45. park, s., shin, s., lee, b., lee, j., surh, j., seo, m., lee, h.: cord: a consolidated\n",
            "receipt dataset for post-ocr parsing. in: workshop on document intelligence at\n",
            "neurips 2019 (2019) 2, 7, 10, 12, 13, 22, 24, 26\n",
            "46. peng, d., wang, x., liu, y., zhang, j., huang, m., lai, s., zhu, s., li, j., lin,\n",
            "d., shen, c., jin, l.: spts: single-point text spotting. corr abs/2112.07917\n",
            "(2021), https://arxiv.org/abs/2112.07917 2\n",
            "47. phan, t.q., shivakumara, p., tian, s., tan, c.l.: recognizing text with per-\n",
            "spective distortion in natural scenes. in: proceedings of the ieee international\n",
            "conference on computer vision (iccv) (december 2013) 13\n",
            "48. powalski, r., borchmann,  l., jurkiewicz, d., dwojak, t., pietruszka, m., pa lka,\n",
            "g.: going full-tilt boogie on document understanding with text-image-layout trans-\n",
            "former. in: llad´ os, j., lopresti, d., uchida, s. (eds.) document analysis and\n",
            "recognition – icdar 2021. pp. 732–747. springer international publishing, cham\n",
            "(2021) 14\n",
            "49. riba, p., dutta, a., goldmann, l., forn´ es, a., ramos, o., llad´ os, j.: table de-\n",
            "tection in invoice documents by graph neural networks. in: 2019 international\n",
            "conference on document analysis and recognition (icdar). pp. 122–127 (2019).\n",
            "https://doi.org/10.1109/icdar.2019.00028 3\n",
            "50. rijhwani, s., anastasopoulos, a., neubig, g.: ocr post correction for\n",
            "endangered language texts. in: proceedings of the 2020 conference\n",
            "on empirical methods in natural language processing (emnlp). pp.\n",
            "5931–5942. association for computational linguistics, online (nov 2020).\n",
            "https://doi.org/10.18653/v1/2020.emnlp-main.478, https://aclanthology.org/\n",
            "2020.emnlp-main.478 3, 11\n",
            "51. schaefer, r., neudecker, c.: a two-step approach for automatic ocr post-\n",
            "correction. in: proceedings of the the 4th joint sighum workshop on com-\n",
            "putational linguistics for cultural heritage, social sciences, humanities and lit-\n",
            "erature. pp. 52–57. international committee on computational linguistics, online\n",
            "(dec 2020), https://aclanthology.org/2020.latechclfl-1.6 3, 11\n",
            "52. shi, b., bai, x., yao, c.: an end-to-end trainable neural network for image-based\n",
            "sequence recognition and its application to scene text recognition. ieee transac-\n",
            "tions on pattern analysis and machine intelligence 39, 2298–2304 (2017) 13\n",
            "53. shi, b., wang, x., lyu, p., yao, c., bai, x.: robust scene text recog-\n",
            "nition with automatic rectification. in: 2016 ieee conference on com-\n",
            "puter vision and pattern recognition (cvpr). pp. 4168–4176 (2016).\n",
            "https://doi.org/10.1109/cvpr.2016.452 13\n",
            "54. taghva, k., beckley, r., coombs, j.: the effects of ocr error on the extraction of\n",
            "private information. in: bunke, h., spitz, a.l. (eds.) document analysis systems\n",
            "vii. pp. 348–357. springer berlin heidelberg, berlin, heidelberg (2006) 2\n",
            "55. tan, m., le, q.: efficientnetv2: smaller models and faster training. in: meila, m.,\n",
            "zhang, t. (eds.) proceedings of the 38th international conference on machine\n",
            "learning. proceedings of machine learning research, vol. 139, pp. 10096–10106.\n",
            "pmlr (18–24 jul 2021), https://proceedings.mlr.press/v139/tan21a.html 12\n",
            "56. tian, z., huang, w., he, t., he, p., qiao, y.: detecting text in natural image with\n",
            "connectionist text proposal network. in: leibe, b., matas, j., sebe, n., welling,20 g. kim et al.\n",
            "m. (eds.) computer vision – eccv 2016. pp. 56–72. springer international pub-\n",
            "lishing, cham (2016) 13\n",
            "57. tito, r., mathew, m., jawahar, c.v., valveny, e., karatzas, d.: icdar 2021 compe-\n",
            "tition on document visual question answering. in: llad´ os, j., lopresti, d., uchida,\n",
            "s. (eds.) document analysis and recognition – icdar 2021. pp. 635–649. springer\n",
            "international publishing, cham (2021) 1, 14\n",
            "58. vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l., gomez, a.n.,\n",
            "kaiser, l.u., polosukhin, i.: attention is all you need. in: guyon, i., luxburg,\n",
            "u.v., bengio, s., wallach, h., fergus, r., vishwanathan, s., garnett, r.\n",
            "(eds.) advances in neural information processing systems. vol. 30. curran\n",
            "associates, inc. (2017), https://proceedings.neurips.cc/paper/2017/file/\n",
            "3f5ee243547dee91fbd053c1c4a845aa-paper.pdf 4, 5, 12, 24, 27\n",
            "59. wang, j., hu, x.: gated recurrent convolution neural network for ocr. in: guyon,\n",
            "i., luxburg, u.v., bengio, s., wallach, h., fergus, r., vishwanathan, s., gar-\n",
            "nett, r. (eds.) advances in neural information processing systems. vol. 30.\n",
            "curran associates, inc. (2017), https://proceedings.neurips.cc/paper/2017/\n",
            "file/c24cd76e1ce41366a4bbe8a49b02a028-paper.pdf 13\n",
            "60. wang, s., li, b., khabsa, m., fang, h., ma, h.: linformer: self-attention with\n",
            "linear complexity. arxiv preprint arxiv:2006.04768 (2020) 12, 27\n",
            "61. wightman, r.: pytorch image models. https://github.com/rwightman/\n",
            "pytorch-image-models (2019). https://doi.org/10.5281/zenodo.4414861 25\n",
            "62. williams, r.j., zipser, d.: a learning algorithm for continually running fully re-\n",
            "current neural networks. neural computation 1(2), 270–280 (1989) 5\n",
            "63. wolf, t., debut, l., sanh, v., chaumond, j., delangue, c., moi, a., cistac,\n",
            "p., rault, t., louf, r., funtowicz, m., davison, j., shleifer, s., von platen,\n",
            "p., ma, c., jernite, y., plu, j., xu, c., le scao, t., gugger, s., drame, m.,\n",
            "lhoest, q., rush, a.: transformers: state-of-the-art natural language processing.\n",
            "in: proceedings of the 2020 conference on empirical methods in natural language\n",
            "processing: system demonstrations. pp. 38–45. association for computational\n",
            "linguistics, online (oct 2020). https://doi.org/10.18653/v1/2020.emnlp-demos.6,\n",
            "https://aclanthology.org/2020.emnlp-demos.6 25\n",
            "64. xu, y., xu, y., lv, t., cui, l., wei, f., wang, g., lu, y., florencio,\n",
            "d., zhang, c., che, w., zhang, m., zhou, l.: layoutlmv2: multi-modal\n",
            "pre-training for visually-rich document understanding. in: proceedings of the\n",
            "59th annual meeting of the association for computational linguistics and\n",
            "the 11th international joint conference on natural language processing (vol-\n",
            "ume 1: long papers). pp. 2579–2591. association for computational linguis-\n",
            "tics, online (aug 2021). https://doi.org/10.18653/v1/2021.acl-long.201, https:\n",
            "//aclanthology.org/2021.acl-long.201 2, 4, 9, 10, 11, 14, 22, 27, 28\n",
            "65. xu, y., li, m., cui, l., huang, s., wei, f., zhou, m.: layoutlm: pre-training of\n",
            "text and layout for document image understanding. in: proceedings of the 26th\n",
            "acm sigkdd international conference on knowledge discovery & data min-\n",
            "ing. p. 1192–1200. kdd ’20, association for computing machinery, new york,\n",
            "ny, usa (2020). https://doi.org/10.1145/3394486.3403172, https://doi.org/\n",
            "10.1145/3394486.3403172 2, 3, 7, 9, 10, 11, 14, 22, 28\n",
            "66. xu, y., lv, t., cui, l., wang, g., lu, y., florencio, d., zhang, c., wei, f.:\n",
            "layoutxlm: multimodal pre-training for multilingual visually-rich document un-\n",
            "derstanding. arxiv preprint arxiv:2104.08836 (2021) 10, 27\n",
            "67. yim, m., kim, y., cho, h.c., park, s.: synthtiger: synthetic text image generator\n",
            "towards better text recognition models. in: llad´ os, j., lopresti, d., uchida, s.ocr-free document understanding transformer 21\n",
            "(eds.) document analysis and recognition – icdar 2021. pp. 109–124. springer\n",
            "international publishing, cham (2021) 5, 6, 23\n",
            "68. zhang, k., shasha, d.: simple fast algorithms for the editing distance be-\n",
            "tween trees and related problems. siam j. comput. 18, 1245–1262 (12 1989).\n",
            "https://doi.org/10.1137/0218082 7\n",
            "69. zhang, z., zhang, c., shen, w., yao, c., liu, w., bai, x.: multi-oriented\n",
            "text detection with fully convolutional networks. in: 2016 ieee conference\n",
            "on computer vision and pattern recognition (cvpr). pp. 4159–4167 (2016).\n",
            "https://doi.org/10.1109/cvpr.2016.451 13\n",
            "70. zhong, x., shafieibavani, e., jimeno yepes, a.: image-based table recognition:\n",
            "data, model, and evaluation. in: vedaldi, a., bischof, h., brox, t., frahm, j.m.\n",
            "(eds.) computer vision – eccv 2020. pp. 564–580. springer international pub-\n",
            "lishing, cham (2020) 722 g. kim et al.\n",
            "a appendix\n",
            "a.1 details of ocr engines (ms, clova, easy, paddle)\n",
            "current state-of-the-art visual document understanding (vdu) backbones, such\n",
            "as bros [18], layoutlm [65] and layoutlmv2 [64], are dependent on off-the-\n",
            "shelf ocr engines. these backbones take the output of ocr as their (one of)\n",
            "input features. for the ocr-dependent methods, in our experiments, we use\n",
            "state-of-the-art ocr engines that are publicly available, including 2 ocr api\n",
            "products (i.e., ms ocr7and clova ocr8) and 2 open-source ocr models\n",
            "(i.e., easy ocr9and paddle ocr10). in the main paper, paddle ocr is used\n",
            "for the chinese train ticket dataset [12] and clova ocr is used for the rest\n",
            "datasets in the document information extraction (ie) tasks. ms ocr is used\n",
            "to measure the running time of the layoutlm family in document classification\n",
            "and visual question answering (vqa) tasks, following the previous work of xu\n",
            "et al. [64]. each ocr engine is explained in the following.\n",
            "ms ocr ms ocr7is the latest ocr api product from microsoft and used in\n",
            "several recent vdu methods, e.g., layoutlmv2 [64]. this engine supports 164\n",
            "languages for printed text and 9 languages for handwritten text (until 2022/03).\n",
            "clova ocr clova ocr8is an api product from naver clova and\n",
            "is specialized in document ie tasks. this engine supports english, japanese and\n",
            "korean (until 2022/03). in the ablation experiments on the cord dataset [45]\n",
            "(figure 9 in the main paper), the clova ocr achieved the best accuracy.\n",
            "easy ocr easy ocr9is a ready-to-use ocr engine that is publicly available\n",
            "at github. this engine supports more than 80 languages (until 2022/03). un-\n",
            "like the aforementioned two ocr products (i.e., ms ocr and clova ocr),\n",
            "this engine is publicly opened and downloadable.9the entire model architec-\n",
            "ture is based on the modern deep-learning-based ocr modules [4,3] with some\n",
            "modifications to make the model lighter and faster. the total number of model\n",
            "parameters is 27m which is small compared to the state-of-the-art models [4,3].\n",
            "paddle ocr paddle ocr10is an open-source ocr engine available at github.\n",
            "we used a lightweight (i.e., mobile) version of the model which is specially de-\n",
            "signed for a fast and light ocr of english and chinese texts. the model is served\n",
            "on a cpu environment and the size of the model is extremely small, which is\n",
            "approximately 10m.\n",
            "7https://docs.microsoft.com/en-us/azure/cognitive-services/\n",
            "computer-vision/overview-ocr .\n",
            "8https://clova.ai/ocr/en .\n",
            "9https://github.com/jaidedai/easyocr .\n",
            "10https://github.com/paddlepaddle/paddleocr .ocr-free document understanding transformer 23\n",
            "fig. a. examples of synthdog. english, chinese, japanese and korean samples\n",
            "are shown (from top to bottom). although the idea is simple, these synthetic samples\n",
            "play an important role in the pre-training of donut. please, see figure 7 in the main\n",
            "paper for details\n",
            "a.2 details of synthetic document generator (synthdog)\n",
            "in this section, we explain the components of the proposed synthetic document\n",
            "generator (synthdog) in detail. the entire pipeline basically follows yim et\n",
            "al. [67]. our source code is available at https://github.com/clovaai/donut .\n",
            "more samples are shown in figure a.\n",
            "background background images are sampled from imagenet [7]. gaussian blur\n",
            "is randomly applied to the background image to represent out-of-focus effects.\n",
            "document paper textures are sampled from the photos that we collected. the\n",
            "texture is applied to an white background. in order to make the texture realistic,\n",
            "random elastic distortion and gaussian noise are applied. to represent various\n",
            "view angles in photographs, a random perspective transformation is applied to\n",
            "the image.24 g. kim et al.\n",
            "text layout and pattern to mimic the layouts in real-world documents, a\n",
            "heuristic rule-based pattern generator is applied to the document image region\n",
            "to generate text regions. the main idea is to set multiple squared regions to rep-\n",
            "resent text paragraphs. each squared text region is then interpreted as multiple\n",
            "lines of text. the size of texts and text region margins are chosen randomly.\n",
            "text content and style we prepare the multi-lingual text corpora from\n",
            "wikipedia.11we use noto fonts12since it supports various languages. synthdog\n",
            "samples texts and fonts from these resources and the sampled texts are rendered\n",
            "in the regions that are generated by the layout pattern generator. the text colors\n",
            "are randomly assigned.\n",
            "post-processing finally, some post-processing techniques are applied to the\n",
            "output image. in this process, the color, brightness, and contrast of the image\n",
            "are adjusted. in addition, shadow effect, motion blur, gaussian blur, and jpeg\n",
            "compression are applied to the image.\n",
            "a.3 details of document information extraction\n",
            "information extraction (ie) on documents is an arduous task since it requires (a)\n",
            "reading texts, (b) understanding the meaning of the texts, and (c) predicting the\n",
            "relations and structures among the extracted information. some previous works\n",
            "have only focused on extracting several pre-defined key information [12]. in that\n",
            "case, only (a) and (b) are required for ie models. we go beyond the previous\n",
            "works by considering (c) also. although the task is complex, its interface (i.e., the\n",
            "format of input and output) is simple. in this section, for explanation purposes,\n",
            "we show some sample images (which are the raw input of the ie pipeline) with\n",
            "the output of donut.\n",
            "in the main paper, we test four datasets including two public benchmarks\n",
            "(i.e., cord [45] and ticket [12]) and two private industrial datasets (i.e., busi-\n",
            "ness card andreceipt ). figure b shows examples of ticket with the outputs\n",
            "of donut. figure c shows examples of cord with the outputs of donut. due\n",
            "to strict industrial policies on the private industrial datasets, we instead show\n",
            "some real-like high-quality samples of business card andreceipt in figure d.\n",
            "a.4 details of model training scheme and output format\n",
            "in the model architecture and training objective, we basically followed the orig-\n",
            "inal transformer [58], which uses a transformer encoder-decoder architecture\n",
            "and a teacher-forcing training scheme. the teacher-forcing scheme is a model\n",
            "training strategy that uses the ground truth as input instead of model output\n",
            "from a previous time step. figure e shows a details of the model training scheme\n",
            "and decoder output format.\n",
            "11https://dumps.wikimedia.org .\n",
            "12https://fonts.google.com/noto .ocr-free document understanding transformer 25\n",
            "(a) input image(b) prediction(c) ground truth\n",
            "ticket\n",
            "{    \"date\":\"2017年11月15日\",    \"destination_station\": \"福田站\",    \"name\": \"珂\",    \"seat_category\": \"二等座\",    \"starting_station\": \"广州南站\",    \"ticket_num\": \"c068987\",    \"ticket_rates\": ¥82.0元\",    \"train_num\": \"g79”}{    \"date\": \"2017年12月05日\",    \"destination_station\": \"广州东站\",    \"name\": \"延辉\",    \"seat_category\": \"一等座\",    \"starting_station\": \"深圳站\",    \"ticket_num\": \"e019154\",    \"ticket_rates\": \"¥99.5元\",    \"train_num\": \"c7128\"}\n",
            "{    \"date\": \"2018年02月13日\",    \"destination_station\": \"扎兰平站\",    \"name\": \"海鹏\",    \"seat_category\": \"新空调硬卧\",    \"starting_station\": \"北京站\",    \"ticket_num\": ”j033534\",    \"ticket_rates\": \"¥367.0元\",    \"train_num\": \"k1301”}{    \"date\": \"2018年02月13日\",    \"destination_station\": \"扎兰屯站\",    \"name\": \"海鹏\",    \"seat_category\": \"新空调硬卧\",    \"starting_station\": \"北京站\",    \"ticket_num\": ”j033534\",    \"ticket_rates\": \"¥367.0元\",    \"train_num\": \"k1301”}ted acc. 97.7ted acc. 97.5{    \"date\": \"2017年12月05日\",    \"destination_station\": \"广州东站\",    \"name\": \"延褥\",    \"seat_category\": \"一等座\",    \"starting_station\": \"深圳站\",    \"ticket_num\": \"e019154\",    \"ticket_rates\": \"¥99.5元\",    \"train_num\": \"c7128\"}ted acc. 100{    \"date\":\"2017年11月15日\",    \"destination_station\": \"福田站\",    \"name\": \"珂\",    \"seat_category\": \"二等座\",    \"starting_station\": \"广州南站\",    \"ticket_num\": \"c068987\",    \"ticket_rates\": ¥82.0元\",    \"train_num\": \"g79”}\n",
            "fig. b. examples of ticket [12] with donut predictions. there is no hierarchy in\n",
            "the structure of information (i.e., depth = 1) and the location of each key information\n",
            "is almost fixed. failed predictions are marked and bolded (red)\n",
            "a.5 implementation and training hyperparameters\n",
            "the codebase and settings are available at github.13we implement the en-\n",
            "tire model pipeline with huggingface’s transformers14[63] and an open-source\n",
            "library timm (pytorch image models)15[61].\n",
            "for all model training, we use a half-precision (fp16) training. we train donut\n",
            "using adam optimizer [30] by decreasing the learning rate as the training pro-\n",
            "gresses. the initial learning rate of pre-training is set to 1e-4 and that of fine-\n",
            "tuning is selected from 1e-5 to 1e-4. we pre-train the model for 200k steps with\n",
            "64 nvidia a100 gpus and a mini-batch size of 196, which takes about 2-3\n",
            "gpu days. we also apply a gradient clipping technique where a maximum gra-\n",
            "dient norm is selected from 0.05 to 1.0. the input resolution of donut is set\n",
            "to 2560 ×1920 at the pre-training phase. in downstream tasks, the input reso-\n",
            "lutions are controlled. in some downstream document ie experiments, such as,\n",
            "13https://github.com/clovaai/donut .\n",
            "14https://github.com/huggingface/transformers .\n",
            "15https://github.com/rwightman/pytorch-image-models .26 g. kim et al.\n",
            "{    \"menu\": [        {            \"cnt\": [\"1\"],            \"nm\": [\"cashew nuts chkn\"],            \"price\": [\"64,500\"]        },          …        {            \"cnt\": [\"4\"],            \"nm\": [\"steamed rice\"],            \"price\": [\"47,600\"]        }    ],    \"sub_total\": [        {            \"service_price\": [\"17,908\"],            \"subtotal_price\": [\"325,600\"],            \"tax_price\": [\"34,351\"]        }    ],    \"total\": [        {            \"total_price\": [\"377,859\"]        } ]}ted acc. 100\n",
            "(a) input image(b) prediction(c) ground truth\n",
            "cord{    \"menu\": [        {            \"cnt\": [\"2\"],            \"nm\": [\"twist donut\"],            \"price\": [\"18,000\"]        },         …        {            \"cnt\": [\"1\"],            \"nm\": [\"frankfrut sausage roll\"],            \"price\": [\"12,000\"]        }    ],    \"total\": [        {            \"cashprice\": [\"104.000\"],            \"changeprice\": [\"56.000\"],            \"total_price\": [\"54.000\"]        } ]}ted acc. 99.0{    \"menu\": [        {            \"cnt\": [\"2\"],            \"nm\": [\"twist donut\"],            \"price\": [\"18,000\"]        },         …        {            \"cnt\": [\"1\"],            \"nm\": [\"frankfrut sausage roll\"],            \"price\": [\"12,000\"]        }    ],    \"total\": [        {            \"cashprice\": [\"104.000\"],            \"changeprice\": [\"50.000\"],            \"total_price\": [\"54.000\"]        } ]}{    \"menu\": [        {            \"nm\": [\"trad ky toast carte\"],            \"price\": [\"28.182\"]        }    ],    \"sub_total\": [        {            \"subtotal_price\": [\"28.182\"],            \"tax_price\": [\"2.818\"]        }    ],    \"total\": [        {            \"cashprice\": [\"31.000\"],            \"menuqty_cnt\": [\"1.00\"],            \"total_price\": [\"31.000\"]        } ]}{    \"menu\": [        {            \"nm\": [\"trad ky toast carte\"],            \"price\": [\"28.182\"]        }    ],    \"sub_total\": [        {            \"subtotal_price\": [\"28.182\"],            \"tax_price\": [\"2.818\"]        }    ],    \"total\": [        {            \"cashprice\": [\"31.000\"],            \"total_price\": [\"31.000\"]        } ]}\n",
            "ted acc. 89.6{    \"menu\": [        {            \"cnt\": [\"1\"],            \"nm\": [\"cashew nuts chkn\"],            \"price\": [\"64,500\"]        },          …        {            \"cnt\": [\"4\"],            \"nm\": [\"steamed rice\"],            \"price\": [\"47,600\"]        }    ],    \"sub_total\": [        {            \"service_price\": [\"17,908\"],            \"subtotal_price\": [\"325,600\"],            \"tax_price\": [\"34,351\"]        }    ],    \"total\": [        {            \"total_price\": [\"377,859\"]        } ]}\n",
            "fig. c. examples of cord [45] with donut predictions. there is a hierarchy\n",
            "in the structure of information (i.e., depth = 2). donut not only reads some important\n",
            "key information from the image, but also predicts the relationship among the extracted\n",
            "information (e.g., the name, price, and quantity of each menu item are grouped)\n",
            "cord [45], ticket [12] and business card , smaller size of input resolution,\n",
            "e.g., 1280 ×960, is tested. with the 1280 ×960 setting, the model training cost\n",
            "of donut was small. for example, the model fine-tuning on cord orticket\n",
            "took approximately 0.5 hours with one a100 gpu. however, when we set theocr-free document understanding transformer 27\n",
            "fig. d. examples of business card (top) and receipt (bottom). due to strict\n",
            "industrial policies on the private industrial datasets from our active products, real-like\n",
            "high-quality samples are shown instead\n",
            "2560×1920 setting for larger datasets, e.g., rvl-cdip ordocvqa , the cost\n",
            "increased rapidly. with 64 a100 gpus, docvqa requires one gpu day and\n",
            "rvl-cdip requires two gpu days approximately. this is not surprising in that\n",
            "increasing the input size for a precise result incurs higher computational costs\n",
            "in general. using an efficient attention mechanism [60] may avoid the prob-\n",
            "lem in architectural design, but we use the original transformer [58] as we aim\n",
            "to present a simpler architecture in this work. our preliminary experiments in\n",
            "smaller resources are available in appendix a.6.\n",
            "for the implementation of document ie baselines, we use the transformers\n",
            "library for bert [8], bros [18], layoutlmv2 [64,66] and wyvern [23]. for\n",
            "the spade [25] baseline, the official implementation16is used. the models are\n",
            "trained using nvidia p40, v100, or a100 gpus. the major hyperparameters,\n",
            "such as initial learning rate and number of epochs, are adjusted by monitoring\n",
            "the scores on the validation set. the architectural details of the ocr-dependent\n",
            "vdu backbone baselines (e.g., layoutlm and layoutlmv2) are available in\n",
            "appendix a.7.\n",
            "a.6 preliminary experiments in smaller resources\n",
            "in our preliminary experiments, we pre-trained donut with smaller resources\n",
            "(denoted as donut proto), i.e., smaller data (synthdog 1.2m) and fewer gpus\n",
            "16https://github.com/clovaai/spade .28 g. kim et al.\n",
            "*urxqg\u00037uxwk\n",
            ",qsxw\u00037rnhqv'hfrghu\u00032xwsxw\u0003\u000b'lvwulexwlrqv\f'hfrghu«««7udlqlqj,qihuhqfh\n",
            "7rnhq\u0003fodvvlilfdwlrq\u0003dw\u0003hdfk\u0003vwhs\u0011\u001fsduvlqj!\u001flwhp!\u001fqdph!\u0016\u0013\u0013\u0015\u001flwhp!\u001fqdph!\u0016\u0013\u0013\u00150lqlpl]h\u0003&urvv\u0003(qwurs\\3uhglfwhg««\u001fsduvlqj!\u001flwhp!\u001fqdph!\n",
            "«\u001flwhp!\u001fqdph!\u001b\u0013\u0013\u0015'hfrghu\u001b\u0013\u0013\u0015\n",
            "3uhglfwhg\u0003wrnhq\u0003vhtxhqfh\u0003lv\u0003frqyhuwhg\u0003lqwr\u0003d\u0003-621\u0003irupdw\u0011^³,whp´\u0003\u001d\u0003^\u0003\u0003\u0003\u0003\u0003\u0003³qdph´\u0003\u001d\u0003³\u001b\u0013\u0013\u0015\u0010.\\rwr\u0003&krfr\u00030rfkl´\u000f\u0003\u0003\u0003\u0003\u0003\u0003³sulfh´\u0003\u001d\u0003³\u0014\u0017\u0011\u0013\u0013\u0013´\u000f\u0003\u0003\u0003\u0003\u0003\u0003³frxqw´\u0003\u001d\u0003³\u0015´\u0003``\u001fsduvlqj!\u001flwhp!\u001fqdph!\u001b\u0013\u0013\u0015\u0010.\\rwr\u0003&krfr\u00030rfkl\u001f\u0012qdph!\u001fsulfh!\u0014\u0017\u0011\u0013\u0013\u0013\u001f\u0012sulfh!\u001ffrxqw!\u0015\u001f\u0012frxqw!\u001f\u0012lwhp!\u001fhqg!\n",
            "fig. e. donut training scheme with teacher forcing and decoder output\n",
            "format examples. the model is trained to minimize cross-entropy loss of the token\n",
            "classifications simultaneously. at inference, the predicted token from the last step is\n",
            "fed to the next\n",
            "(8 v100 gpus for 5 days). the input size was 2048 ×1536. in this setting,\n",
            "donut proto also achieved comparable results on rvl-cdip andcord . the\n",
            "accuracy on rvl-cdip was 94.5 and cord was 85.4. after the preliminaries,\n",
            "we have scaled the model training with more data.\n",
            "a.7 details of ocr-dependent baseline models\n",
            "in this section, we provide a gentle introduction to the general-purpose vdu\n",
            "backbones, such as layoutlm [65] and layoutlmv2 [64]. to be specific, we\n",
            "explain how the conventional backbones perform downstream vdu tasks; docu-\n",
            "ment classification, ie, and vqa. common to all tasks, the output of the ocr\n",
            "engine is used as input features of the backbone. that is, the extracted texts\n",
            "are sorted and converted to a sequence of text tokens. the sequence is passed to\n",
            "the transformer encoder to get contextualized output vectors. the vectors are\n",
            "used to get the desired output. the difference in each task depends on a slight\n",
            "modification on the input sequence or on the utilization of the output vectors.\n",
            "document classification at the start of the input sequence, a special token\n",
            "[cls] is appended. the sequence is passed to the backbone to get the output\n",
            "vectors. with a linear mapping and softmax operation, the output vector of the\n",
            "special token [cls] is used to get a class-label prediction.\n",
            "document ie with a linear mapping and softmax operation, the output vector\n",
            "sequence is converted to a bio-tag sequence [22].ocr-free document understanding transformer 29\n",
            "ie on 1-depth structured documents when there is no hierarchical structure in\n",
            "the document (see figure b), the tag set is defined as {“bk”, “i k”, “o” |k∈\n",
            "pre-defined keys }. “bk” and “i k” are tags that represent the beginning (b) and\n",
            "the inside (i) token of the key krespectively. the “o” tag indicates that the\n",
            "token belongs to no key information.\n",
            "ie on n-depth structured documents when there are hierarchies in the structure\n",
            "(see figure c), the bio-tags are defined for each hierarchy level. in this section,\n",
            "we explain a case where the depth of structure is n= 2. the tag set is defined\n",
            "as{“bg.bk”, “b g.ik”, “i g.bk”, “i g.ik”, “o” |g∈pre-defined parent keys, k∈\n",
            "pre-defined child keys }. for instance, the figure c shows an example where a\n",
            "parent key is “menu” and related child keys are {“cnt”, “nm”, “price” }. “b g”\n",
            "represents that one group (i.e., a parent key such as “menu”) starts, and “i g”\n",
            "represents that the group is continuing. separately from the bi tags of the parent\n",
            "key (i.e., “b g” and “i g”), the bi tags of each child key (i.e., “b k” and “i k”) work\n",
            "the same as in the case of n= 1. this bio-tagging method is also known as\n",
            "group bio-tagging and the details are also available in hwang et al. [22].\n",
            "document vqa with a linear mapping and softmax operation, the output\n",
            "vector sequence is converted to a span-tag sequence. for the input token se-\n",
            "quence, the model finds the beginning and the end of the answer span. details\n",
            "can also be found in the section 4.2 of devlin et al. [8].\n",
            "\n",
            "\n",
            "Similarity Distance (lower is more similar): 151.88925170898438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assesment 2\n"
      ],
      "metadata": {
        "id": "weAO-iw-Kh1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary libraries\n",
        "!pip install rdflib==6.3.2\n",
        "!pip install sentence-transformers==2.2.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqrgAgy1KhCe",
        "outputId": "a9e423fc-72d0-435c-9ee2-9c0bfac97c91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdflib==6.3.2 in /usr/local/lib/python3.11/dist-packages (6.3.2)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from rdflib==6.3.2) (0.6.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib==6.3.2) (3.2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib==6.3.2) (1.17.0)\n",
            "Requirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.34.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (2.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.15.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (3.9.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.13.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.5.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.6.0)\n",
            "Collecting torch>=1.6.0 (from sentence-transformers==2.2.2)\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->sentence-transformers==2.2.2) (11.2.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==8.9.2.26->torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.2.0 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.4.26)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.18.1\n",
            "    Uninstalling nvidia-nccl-cu12-2.18.1:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.18.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0\n",
            "    Uninstalling torch-2.1.0:\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: chromadb==0.4.14 in /usr/local/lib/python3.11/dist-packages (0.4.14)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (2.32.3)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (2.11.4)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (0.34.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (4.13.2)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (3.7.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (1.22.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.14.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (0.15.3)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.14) (2.0.2)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb==0.4.14) (0.46.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.14) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (2.9.0.post0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.14) (1.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pulsar-client>=3.1.0->chromadb==0.4.14) (2025.4.26)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.14) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.14) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.14) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.14) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.14) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.14) (2.4.0)\n",
            "Requirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb==0.4.14) (0.16.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.14) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.14) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.14) (13.9.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.14) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (1.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (6.0.2)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.14) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.14) (3.18.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.14) (2025.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.14) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.14) (2.19.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.14) (4.9.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.14) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.14) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.14) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.14) (0.1.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! pip install ChromaDB\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ_P0iO1QBky",
        "outputId": "9efbd999-27a3-4dfd-8876-d9089f860953"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: ChromaDB in /usr/local/lib/python3.11/dist-packages (0.4.14)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (2.32.3)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (2.11.4)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (0.34.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (4.13.2)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (3.7.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (1.22.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (0.14.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (0.15.3)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from ChromaDB) (2.0.2)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->ChromaDB) (0.46.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->ChromaDB) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->ChromaDB) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->ChromaDB) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->ChromaDB) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->ChromaDB) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->ChromaDB) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->ChromaDB) (2.9.0.post0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->ChromaDB) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->ChromaDB) (1.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pulsar-client>=3.1.0->ChromaDB) (2025.4.26)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->ChromaDB) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->ChromaDB) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->ChromaDB) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->ChromaDB) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->ChromaDB) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->ChromaDB) (2.4.0)\n",
            "Requirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->ChromaDB) (0.16.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->ChromaDB) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->ChromaDB) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->ChromaDB) (13.9.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->ChromaDB) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (1.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (6.0.2)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->ChromaDB) (3.18.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->ChromaDB) (2025.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->ChromaDB) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->ChromaDB) (2.19.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->ChromaDB) (4.9.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->ChromaDB) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->ChromaDB) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->ChromaDB) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->ChromaDB) (0.1.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub==0.16.4 # Install a specific version of huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWuAjmeGLvdV",
        "outputId": "0b19bfae-c0f3-4177-da51-affeda0216d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub==0.16.4 in /usr/local/lib/python3.11/dist-packages (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.16.4) (3.18.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.16.4) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.16.4) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.16.4) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.16.4) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.16.4) (4.13.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.16.4) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.16.4) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.16.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.16.4) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.16.4) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.34.0\n",
        "!pip install torch==2.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjD0PvKkMB8A",
        "outputId": "278cddde-dc7d-4f0e-abca-f64309326250"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.34.0 in /usr/local/lib/python3.11/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (2025.4.26)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.11/dist-packages (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph, Literal, URIRef, RDF\n",
        "from rdflib.namespace import FOAF, XSD\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Build a basic RDF knowledge graph\n",
        "g = Graph()\n",
        "\n",
        "# Define URIs for resources and properties\n",
        "alice = URIRef(\"http://example.org/alice\")\n",
        "bob = URIRef(\"http://example.org/bob\")\n",
        "acme = URIRef(\"http://example.org/acme_corp\")\n",
        "\n",
        "# Add triples to the graph\n",
        "g.add((alice, RDF.type, FOAF.Person))  # Use RDF.type\n",
        "g.add((alice, FOAF.knows, bob))\n",
        "g.add((bob, URIRef(\"http://example.org/worksAt\"), acme))\n",
        "g.add((bob, RDF.type, FOAF.Person))  # Use RDF.type\n",
        "\n",
        "# Print the graph in different formats (optional)\n",
        "print(\"Knowledge Graph (Turtle format):\")\n",
        "print(g.serialize(format=\"turtle\"))\n",
        "\n",
        "# 2. Convert RDF triples into simple text sentences\n",
        "sentences = []\n",
        "for s, p, o in g:\n",
        "    # Basic conversion: subject predicate object\n",
        "    # You can enhance this to generate more natural language\n",
        "    subject = s.split('/')[-1]  # Get the last part of the URI\n",
        "    predicate = p.split('/')[-1]\n",
        "    obj = o.split('/')[-1] if isinstance(o, URIRef) else o.toPython()  # Handle Literals\n",
        "\n",
        "    sentence = f\"{subject} {predicate} {obj}.\"\n",
        "    sentences.append(sentence)\n",
        "\n",
        "print(\"\\nSentences from triples:\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "# 3. Embed these text sentences\n",
        "# Use a pre-trained sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "print(f\"\\nGenerated embeddings for {len(sentences)} sentences with shape {sentence_embeddings.shape}\")\n",
        "\n",
        "# 4. Store the embeddings in ChromaDB\n",
        "client = chromadb.Client()\n",
        "\n",
        "# Create or get a collection\n",
        "collection_name = \"knowledge_graph_sentences\"\n",
        "try:\n",
        "    collection = client.create_collection(name=collection_name)\n",
        "    print(f\"Collection '{collection_name}' created.\")\n",
        "except chromadb.db.base.UniqueConstraintError:\n",
        "    collection = client.get_collection(name=collection_name)\n",
        "    print(f\"Collection '{collection_name}' already exists. Getting existing collection.\")\n",
        "\n",
        "# Prepare data for ChromaDB\n",
        "ids = [f\"sentence_{i}\" for i in range(len(sentences))]\n",
        "embeddings = sentence_embeddings.tolist()  # Convert numpy array to list\n",
        "documents = sentences\n",
        "\n",
        "# Add data to the collection\n",
        "collection.add(\n",
        "    embeddings=embeddings,\n",
        "    documents=documents,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(f\"Embeddings and documents added to collection '{collection_name}'.\")\n",
        "print(\"Number of items in collection:\", collection.count())\n",
        "\n",
        "# 5. Accept a simple user query and return relevant text\n",
        "user_query = input(\"\\nEnter your query (e.g., Who does Alice know?): \")\n",
        "\n",
        "# Embed the user query\n",
        "query_embedding = model.encode([user_query]).tolist()\n",
        "\n",
        "# Perform similarity search in ChromaDB\n",
        "# query_embeddings expects a list of embeddings, even if you have only one query\n",
        "results = collection.query(\n",
        "    query_embeddings=query_embedding,\n",
        "    n_results=1,  # Retrieve the top 1 most relevant sentence\n",
        "    include=['documents', 'distances']  # Include the original document text and distance\n",
        ")\n",
        "\n",
        "# Display the most relevant document (sentence) and its similarity score (distance)\n",
        "if results and results.get('documents') and results.get('distances'):\n",
        "    most_relevant_sentence = results['documents'][0][0]\n",
        "    similarity_distance = results['distances'][0][0]\n",
        "    print(\"\\nMost Relevant Sentence:\")\n",
        "    print(most_relevant_sentence)\n",
        "    print(f\"\\nSimilarity Distance (lower is more similar): {similarity_distance}\")\n",
        "else:\n",
        "    print(\"\\nNo relevant sentences found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "21b9c1a660784c308e0ba1ecb840b1f2",
            "9fa539ff355545948b7305d6c403566c",
            "efe8d74d285b4c3bb35d878daf52c652",
            "7842b8946aa34859b17b1b4db6688def",
            "9bf3ae3e830c4e65a0879ff90424ee03",
            "17c2c90e77064731911659f6353b5326",
            "3d8bc1b0a9a547c1b6bd901e6a4d3c9b",
            "697dd1ebcf274f4aa0212e52e24991c6",
            "8e01bc2bdbbb46cdbacd8eab6ff11fa2",
            "5a0b56f1d61449dcb17def2d60d3abb6",
            "af09f0d19f5a4b67a3e0c0938eda3e46",
            "8f9c80df55f549b8b4fd3325c97bcfaf",
            "748eebcb522946ddb2f8986eac2cea67",
            "21eec928981e4cf1b97e90cfd9a1e665",
            "164bbbb863954230b090a353b011a312",
            "0e012482313b4008a4711b6fc2b5fe32",
            "75aab5ca85f94412b3b08f3e969d98f2",
            "d938bf7c9fcd468a8ab8aa02ce02fb77",
            "2411e001ad8443b4b71a94ba360f4227",
            "6e11d78086dd4b8a8946eb950bb3cc93",
            "f3751dba81894761b9ace1479765a0ab",
            "c68fb56df1044679af67b43aa29c1e71",
            "391af866c46a4617b845bf631b3b03e3",
            "8d8bdee8f63942b88cafc8a336e67161",
            "279641a7019740a8a366c5526a338dc0",
            "b0d23ffd9e8f4956a846d1386f1de8c9",
            "39f0140e426d48ac8b274a9331996f92",
            "4d54c12523fe4e7bbc515e5c2851543c",
            "ca40401179914b4fbf8a1971168ed4e0",
            "385df39e51774a1794f9a5dfd79d46d3",
            "73cb7c4a64354811a32379535553d9f0",
            "1a62c7955e654a3abb0557f88e8e20a3",
            "4f4e33ae789246f991059e8343675f1d",
            "789e3f12b65148b484f4f7b42b5afcfb",
            "6267989b3940432c90c7353a077cc823",
            "11fe7966b15a4ce89ef3e40fb6ea4d8f",
            "96be8357107441f08f21105ed427a048",
            "531bd0f2a1424df29c5156e5e9eef718",
            "d90793cf1af5437792a478dca88782ea",
            "54c022ad418541c2a5ebd108d2e2af41",
            "d00733804ddb4b3e97887efb64f8026c",
            "1e84d470e760490aa72bd28f56a29a65",
            "6c7e800e0d89488d9d887a232fe13c92",
            "d5ad1408335349828238114c5dca07a4",
            "fbb4eba603174610baf0bdc672182893",
            "74dbf2c8860f403c8c64a7941fed0e11",
            "3bd10ceff40c40bbad9ed11dc7124e37",
            "c23786ac2eb44ee6adf82c748c89778a",
            "5d0a1f16e1b64ae982767fbc9f8e578c",
            "75da60882c4a4e10849019ba532a9b90",
            "0f4c698571ed4dfab1bebdd6c144fd0b",
            "14fb8c3e2fd34a3cbb960eeddad4a56c",
            "809394c834394a04b133d34791d69973",
            "c8623a0cd5e840b1a219cb43594ef094",
            "cb41558222914ea58f2902973e54e380",
            "0b113c4fce3547779f6f78b91a24e814",
            "3179230be963456486080f8b0032dc16",
            "f7d79d8f5ee14b9ca65f3fcd2e154684",
            "b6be46a4d4e6458e8e3bba725ea4a23a",
            "0125e7f5ec244a0ba2205e6d13c50a02",
            "f43c1a7a49bb443588080556db37845d",
            "c4ddc5b7954947c4b50e926cb39545de",
            "54c5a2e7af284f66800922f7f763cf72",
            "ff444354632e4346ad7ad9eb44c5cbac",
            "3b2ae483f1c24303a37d355f508f339f",
            "32a012d48b684da4acdbc5896ca2b9fd",
            "30ba7305db8447d28c1275413c59db63",
            "0cf200c4bd364de9a7f8467d7c3ed4e6",
            "7bf819a35c344fae9103a0e0578e4231",
            "3dbff80ff9534867a7689208091fba64",
            "57845b0cab574d99ab59ca6ba39d483c",
            "be64b3ca458a44f2895a87a9019e4b27",
            "b750a4e3cdf94c4ea76d3ad3ccb64dce",
            "d7ed1f7c0cb9418d9c03e7289644a1b9",
            "9851dc3c4aec44f993082c4c04362adf",
            "1c8aa8a7ac924d8ba319c8b6aaf8a69a",
            "d30da5ee8a6f42fda8e8037a2c0c946a",
            "190a9dc3578f43dfb1cb1cc4a9cbd3b3",
            "8f9bdeb2577e4f22a8b8795c9c07f1bf",
            "6c23dd29a9794b9e9f074f4a16767ade",
            "9365838a39854986b35ff5158c5047f5",
            "417453bb846d49d2a3d961b89b1acac4",
            "ead4562d31ef4f84a9731ac1c33a8a3f",
            "deda8f8422344143bcfe84fe066f8b9b",
            "c4f3b453909444eebb1d528f623235b1",
            "553ab7d2d4044318b7219c0a0dccdc2b",
            "ae9c5a2f03b84b95a140589d5793f500",
            "d7920aa190b3416ba7dc58e2f4c25e44",
            "5d3f414836354ef7adf0940feabc6f10",
            "a45fc2b47c304f1695793e22f1d78393",
            "4b0e3ee7c5d740d9a5da377d77a2d890",
            "c8fe51a260234942ac6054cc22da3f46",
            "d370fcc3992b421b83fb198f44ed9e27",
            "5876be384bae4c6f94f8f330b4a49aa9",
            "a1e5ef9e86c74ddabd3645c47662e15b",
            "0717d4b85b5c4e5598e2048e3cb75d2e",
            "c36d76db4326455a87a2859b556dd446",
            "c417111d075e4577b47ccc1f1d3d0d05",
            "859206daf87d422e9add7f228720abd5",
            "95b1be3f478d4944bdd1e8cfc073569f",
            "7b58914f3b2b431581d93bb42ab590c0",
            "527a9d185a74479d99ae965f74698c0c",
            "7876175021e04f3791c1913a2b114a3f",
            "044dbcb880cb4a68b77a46a457d0d1ee",
            "a3f2c5b837f549b3947c662d94a10640",
            "e35ae662710c4fb2a20bd25fc8828ba0",
            "e27af4ca530045c2a70528f68b4c9546",
            "0ce5bad750cf4634bde7dd869bc18407",
            "2fd45822df8d455495cbee189e4e51c1",
            "5cf8ec2d38e646ec829f8de492ea5c7f",
            "fb30e11285a84e05807b9683d1a8b4f6",
            "7df0b4a9f33246e0be5424be9c4287a8",
            "ba622fb44fc747f7804ff8c377eca070",
            "6f3a6d9e149b4d88af3ad81badffbb83",
            "1528f186f2354131b40812ff540102a5",
            "74c8ebf3bd8e4254b31a2141bef2bca9",
            "18993571215445b79d80c7089facfa77",
            "fdfe4d0801c949ee9f176c95bca65cac",
            "18f8b5e3b1db48f29eeb1ddb7c66bc7e",
            "43056684e452496ab2a9d0613f01fa69",
            "02352cb11a424dc49a24bac9895caf33",
            "460e25a767e345caaf6c49edb16b70a1",
            "2d6acea39af84861bb26983853ee12e8",
            "44c5af1e46f34466b3f1ddd5e70dd177",
            "d8b762198f9d48729cca6f949d3de030",
            "27e32956646b48eba1617198149edf39",
            "c774ebfe4be040d7ac4dd7bc95df50ce",
            "667a24c673a2497ebea86e54676de918",
            "5907fedc921e4eb1ab69420d44e95385",
            "8c0e652873ee41a08931647fbb17ea9d",
            "e5681615e58e44549bf1be21585ed43a",
            "e72804f552154f928b213dd35f1ef0e1",
            "70de958cd71b438590b368704a07b694",
            "19faff1bb7b94b75987c45c9111c77aa",
            "019b41e9c9244f1b8e0e82934877485f",
            "0b00495ff2f44cb39dbb2905b4f8e02f",
            "55bb29f0e62c44caa3dd5038e6c12b9e",
            "36cf54ce7a814ea8b90a7aea89d3bc09",
            "ced83df6957d4af6b0b6050508c076bf",
            "1f8b0e795dbb4dd38937edd1016e961d",
            "dd990f90070045dbafc114d3f33f872d",
            "19e0732fc1e244618464a2a6f85ea98b",
            "ee594cf7fee249b4aa14cf519efd890c",
            "b5b9d3a273d94dd18c23c4000fbae6f3",
            "176b87035b1a4948a810f621ddf1a4a3",
            "0fe8e070578d4b73af7750556aa421f0",
            "dd356865a8cb41e6a4587366366c9c54",
            "f3d66828afa846328a13b5127e2b9c53",
            "92fe14b127a64327bd6cf9183fe8e0aa",
            "274b743e80994939bb8b56e51eccff79",
            "0f6cfb8c290d4786a5fcc0b64d3ac4e5",
            "55ddd8ff412f4a838b0c8b15774b0f88",
            "02adbc1e1b5845e19d13062608a218e4",
            "a7c2bebda6894b519f7f55ac52b161bc",
            "cacea61ed4f74e0188012916d44ce5b6",
            "1cc9888241b443769028a26ed3f3cc77",
            "7d671105d3a945a1a6a91436ca01d32a",
            "b507f58b762b4e89951e3122d840438a",
            "8848baf74e2b405290235cfa1cea541d",
            "a9594a7bcb2d4668a2f77ab713678937",
            "6697cc32240e4c3092974dd7fb881cf5",
            "de9319c6f01347c9bf91e308d9811730",
            "71ce3c729d0c48708cd6006a2fc050aa",
            "1aa6a9adc0504dec895775aaeacdbc66",
            "c00e6001b4a94cdd9bd54c29bc15bed2",
            "11f276e5b7fc47fbbf9461b7a7b8c563",
            "14a04cc0ed6a4d4b907c2923b422a1ee",
            "c68a6f39d2b04407b93f0a64c6ff93eb",
            "ccafb1cf47bd475ea71b3f5d683e878b",
            "922e300a9af3417098b037ed860ac137",
            "0691f73f2daf474c86b7913a8a848fa0",
            "02ac15342b1440b6bc329f9c96b9662d",
            "dbc268df141b42e7865029c6449784ce",
            "d61101cfc2fb476b85008ef372b4c1b5",
            "866b249862a54902a3bdc615209a7b6c",
            "0956271499034b1497664122a94e1b27",
            "cd4cc0ed7e5d41cfb512f5079f569605",
            "00306243df2344b5b3dfbc5266214b18",
            "c8e6490e0c0c480fa67cc3e31cebcd85",
            "e97d2f366c4e4abaa0eebaf6d6459a9a",
            "42f61fff1fba40cb9ae1576a69196991",
            "7a63932f61aa4f23bdba662593aa1463",
            "fc1bd52bdfbb4897953250a9a4f8cce6",
            "251bc9a6541140349f47b9e7d9c27da2",
            "46cade80ed4e439abdfab03f5dba1697",
            "60e3ebed8f9b49c9839d592453cd1187",
            "9c747311154c49d2ad6886bc511ac844",
            "fb4e2ec03f294456a24d83c879fb233e",
            "8d38d49ae7e9420d8b5b569897ad91a1",
            "0b3b88d513bd418da7b00fe0a97053ca",
            "d3455e96ce1c4ac485d12f3cf35db7e3",
            "56f6639179a740fb8995178e8472fda4",
            "936b2ebd632d40cfa424f8670319cfc6",
            "ce0f3f2927b94abcbbeef3135afb7b3d",
            "6c2f98bbd7bb488fa3b17d3dfb1e386a",
            "2002b9259384420197006be847006007",
            "028e098da36944828bb9c9ddaf391001",
            "1133ed069729414bac9ca77bd3cdca7c",
            "b6b5d1f826b94c8c84258142eb4b4ecd",
            "d10d5b3c0e514684a34fa993419318da",
            "6682168f66d643daae293a5bf49cd812",
            "f01a4a5a551f40609f177957ab0cdfc7",
            "f12de67d70bc4adbb28d8a294cce64d4",
            "9b5cd0af559847d5a60d7c68f2494f0d",
            "db93103c42b7481c8507359e78fb6104",
            "d03c6eab0add4d6baff1ce2f01642c75",
            "b917404e30ba4f5d9ef149841c7c5ac5",
            "ece85828402b4e77b24f16be970b4ef2",
            "46ec75b0d2334ac2a874beba24826a80",
            "a7fc5ac85281422ab18bfe6270846e34",
            "d2ded8ee631c4da28dbe2f6c8b8451dd",
            "d01f19ae4685434786de64d97ee61fb1",
            "9cc418dbeb5b4f5baf81dad6317d70af",
            "744940a4f3e74cecb69a40a5a3dcbe9f",
            "cf668b5f78c74f10b40fb98e7a5b8366",
            "97944e907ef24b14b0ea1cc8b174df38",
            "06381f9e5fd247cd9d6e822a18a73706",
            "ea21a6b861c34d4b918579f15dcc4d38",
            "c73a0a1832624952b51e57b6264434db",
            "d1152ecd8b40425a9cd8b56a2b68f6eb",
            "3e4d666f31554b9496a5a881af8513bf",
            "a67a1a28192947f1953de60940c85379",
            "2226912146504e68bab6d9b9d6f2d627",
            "98a0c47228964f3c9a5e6a0058f7ed1a",
            "1e833b2db4e641f88e666cd614c18d8d",
            "4d680afe295d47e2a524f145a2d5bbdd",
            "2c23fe286dfd40ca8cd814619286a09e",
            "5d60fa3e24874d5ea34a00f58b0d71e0",
            "39b8903ae41e4067a1b1a89b1fe3ba7f",
            "4346003f9e194825938db3d5d0bcaf34",
            "d098200f356a4ece8732250eb89684b9",
            "aeb35e37cc2b426ab35306fe643ddc61",
            "a5291d5258074b6ba02a938cc2fe2f84",
            "35b1b77f4ea548edbca5716c832b2e11",
            "b29728d79a074bb1bcdf9cec0caf005e",
            "dc8d34ecc135436f8fde1aabc287f7cb",
            "1169ba14308340348c5cad3ecbccd5cf",
            "25f3a662c597498cac31f1a44878ba76",
            "66730a6045924dc59a0b9a15b3d40319",
            "a1b00674cba04318969f81e7d1e942ee",
            "99f9f8a8c07f4d058ea0972bf3f4efad",
            "936aade77e3b47a4b4af01d50d1e875b",
            "bf717310b00a420db4037257d1f44bc6",
            "28fb3061611242adbdf09c6437c6e981",
            "d90e8d0ce88c4ddaaceac548c27ecdb4",
            "9acf91078d664327895a1dcdb245ed87",
            "cf7c41209f6142f68e8ceb25f12ed3fc",
            "0c636991f9494a849a95fa86bf09bea7",
            "aaf082509c2147d38b58a2977bc3ed3d",
            "de4a3555ce4147538db40bd84d34f7ca",
            "0e939436ee284bda88ff29aa9b1c58df",
            "7603d22cd3744cbb95d6a9643e78897b",
            "4d72d54f00a64e9a942fc96433e15b72",
            "aaa2f3755930484ca618e6cea36ddfdc",
            "cf7a36763fb848c7a10f467489621e2c",
            "8e8cd4c41f3944f2865d8c44df689474",
            "1486bff72ec84047878e9a66f3dfc3b9",
            "4c05ed439c044cdf8ee01037748ea4f8",
            "aa7be78b53404942ade8940e9d1d23a7",
            "ba5a9793958445a388d5cd47846fb8ff",
            "a3eacfa36f0946db8961b7826777d799",
            "414acde33a1e43ffac2b0631063df250",
            "5f953bbd4bb645a2b9529a77830c3d1f",
            "a32db049f73c4316bd338750d3c621e2",
            "d8490694e6cd4918adc1ae77c24d3993",
            "89e7d1cef6df4a1b9c975bce577193dc",
            "4d14a2adde014437b269b11767a84d91",
            "f6023ef0d85346e4bf1734910273b08a",
            "2fdc277be5b44ef0920e42cee956b1d3",
            "97089676cd6e4613a440d5b6c8ceb627",
            "4d8932804e234222953e3b2b953ed69a",
            "5c9a0e88ed8a46a1b5553e51b31c7311",
            "fa9cbacae2654b6986c55a6a5f597f34",
            "8bbaa310eab74e52818b959289b45e43",
            "3237c79daa054a2fa22d0c59a52352e9",
            "f4b6e65bf4764f20a6d910ebf35aa15e",
            "2255c84dc29449d499686039055b2e6d",
            "6367cec122874b51ad3a02a4734f1c4f",
            "49363cb5c6864c4f94c222da105a244d",
            "f2c744d8284a4103b308c248d553aaa5",
            "7f7f745067c54f659b6c680d8b18cc8b",
            "9a4b3aa91081424d8d03fd097dd9e7c1",
            "dd341ecb7b8d4ee7bef2c62ba2fa3e58",
            "3647bce524834188bb15a77389c59075",
            "fbd786e6e0504551bc89ea8854223f96",
            "abcb7861931f4d93a4d7426cda1ca766",
            "f10eab93619a48f5984c490184198ff2",
            "2576301315c848fab08f162407dc4c95",
            "ee9bf9cba9b741bfbc239a0df03b66c3",
            "62aaa77830754cc09b6f6b5e153be871",
            "9f8fb2f27ca041e589d5cf74569c19d4",
            "61696de2535b4eee97a33f70d4347632",
            "2455ab2efde44672a0fa55966e01c295",
            "3d71cccef8164bd1aa2508b7b9c8a1b6",
            "f66592fe8c0f4ef39676a729a0413b37",
            "570adb3c0189483ba90dd82227fda24b",
            "1f4e9bc3f7c14a5497ea2f8d95129a6e",
            "6a07eb3a414a47f0b99a70d35e5f8eea",
            "fc99c46d5ecd4cdb975f230a16ef8f21",
            "58d7672c903c4d6b86b84ca4928baba5",
            "f21680ee10794710b9d77f9f57746154",
            "b913a2cb9f384de6ab5658a8c6dc95a5",
            "a97d50a78c954029a85fdc528d87cf6e",
            "d7d73dbe12e6403ebb423fc682b0b645",
            "83af42a085c54334a244f225869202e2",
            "a271f324be38485d8f74668d24755b77",
            "8692b9addba04e4da85f5dcfc0e693fb",
            "98154f790e82438ab4168bb065aa93d7"
          ]
        },
        "id": "zX7MBfVLQi_P",
        "outputId": "95200892-d27b-427b-b356-8a27b6c7f8da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Graph (Turtle format):\n",
            "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
            "@prefix ns1: <http://example.org/> .\n",
            "\n",
            "ns1:alice a foaf:Person ;\n",
            "    foaf:knows ns1:bob .\n",
            "\n",
            "ns1:bob a foaf:Person ;\n",
            "    ns1:worksAt ns1:acme_corp .\n",
            "\n",
            "\n",
            "\n",
            "Sentences from triples:\n",
            "alice 22-rdf-syntax-ns#type Person.\n",
            "alice knows bob.\n",
            "bob 22-rdf-syntax-ns#type Person.\n",
            "bob worksAt acme_corp.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading .gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21b9c1a660784c308e0ba1ecb840b1f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f9c80df55f549b8b4fd3325c97bcfaf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "391af866c46a4617b845bf631b3b03e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "789e3f12b65148b484f4f7b42b5afcfb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbb4eba603174610baf0bdc672182893"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b113c4fce3547779f6f78b91a24e814"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30ba7305db8447d28c1275413c59db63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "190a9dc3578f43dfb1cb1cc4a9cbd3b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model_O1.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d3f414836354ef7adf0940feabc6f10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model_O2.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95b1be3f478d4944bdd1e8cfc073569f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model_O3.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb30e11285a84e05807b9683d1a8b4f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model_O4.onnx:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "460e25a767e345caaf6c49edb16b70a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)el_qint8_avx512.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70de958cd71b438590b368704a07b694"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)el_qint8_avx512.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5b9d3a273d94dd18c23c4000fbae6f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)el_qint8_avx512.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cacea61ed4f74e0188012916d44ce5b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model_quint8_avx2.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11f276e5b7fc47fbbf9461b7a7b8c563"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading openvino_model.bin:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd4cc0ed7e5d41cfb512f5079f569605"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading openvino_model.xml:   0%|          | 0.00/211k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb4e2ec03f294456a24d83c879fb233e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_qint8_quantized.bin:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6b5d1f826b94c8c84258142eb4b4ecd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_qint8_quantized.xml:   0%|          | 0.00/368k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7fc5ac85281422ab18bfe6270846e34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e4d666f31554b9496a5a881af8513bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aeb35e37cc2b426ab35306fe643ddc61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf717310b00a420db4037257d1f44bc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaa2f3755930484ca618e6cea36ddfdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8490694e6cd4918adc1ae77c24d3993"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4b6e65bf4764f20a6d910ebf35aa15e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f10eab93619a48f5984c490184198ff2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a07eb3a414a47f0b99a70d35e5f8eea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlignDevicesHook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_hook_to_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m from .big_modeling import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit_torch_state_dict_into_shards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mpytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_pruneable_heads_and_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_linear_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftAdapterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepspeed_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e97febf1d18d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# 3. Embed these text sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Use a pre-trained sentence transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all-MiniLM-L6-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0msentence_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'modules.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m#Load as SentenceTransformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_sbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m#Load with AutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_auto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m_load_sbert_model\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_config\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0mmodule_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m             \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbert_config_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfIn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_name_or_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_name_or_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m_load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_t5_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_t5_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m             )\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m             return model_class.from_pretrained(\n\u001b[1;32m    566\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0msupported_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;31m# Maybe there was several model types associated with this config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from rdflib import Graph, Literal, URIRef, RDF\n",
        "from rdflib.namespace import FOAF, XSD\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import chromadb\n",
        "\n",
        "# 1. Build a basic RDF knowledge graph\n",
        "g = Graph()\n",
        "\n",
        "# Define URIs for resources and properties\n",
        "alice = URIRef(\"http://example.org/alice\")\n",
        "bob = URIRef(\"http://example.org/bob\")\n",
        "acme = URIRef(\"http://example.org/acme_corp\")\n",
        "\n",
        "# Add triples to the graph\n",
        "g.add((alice, RDF.type, FOAF.Person))  # Use RDF.type\n",
        "g.add((alice, FOAF.knows, bob))\n",
        "g.add((bob, URIRef(\"http://example.org/worksAt\"), acme))\n",
        "g.add((bob, RDF.type, FOAF.Person))  # Use RDF.type\n",
        "\n",
        "# Print the graph in different formats (optional)\n",
        "print(\"Knowledge Graph (Turtle format):\")\n",
        "print(g.serialize(format=\"turtle\"))\n",
        "\n",
        "# 2. Convert RDF triples into simple text sentences\n",
        "sentences = []\n",
        "for s, p, o in g:\n",
        "    # Basic conversion: subject predicate object\n",
        "    # You can enhance this to generate more natural language\n",
        "    subject = s.split('/')[-1]  # Get the last part of the URI\n",
        "    predicate = p.split('/')[-1]\n",
        "    obj = o.split('/')[-1] if isinstance(o, URIRef) else o.toPython()  # Handle Literals\n",
        "\n",
        "    sentence = f\"{subject} {predicate} {obj}.\"\n",
        "    sentences.append(sentence)\n",
        "\n",
        "print(\"\\nSentences from triples:\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "# 3. Embed these text sentences using bert-base-uncased\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def embed_text(text_list):\n",
        "    encoded_input = tokenizer(text_list, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    # Use the CLS token embedding (first token) as the sentence representation\n",
        "    return model_output.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "sentence_embeddings = embed_text(sentences)\n",
        "\n",
        "print(f\"\\nGenerated embeddings for {len(sentences)} sentences with shape {sentence_embeddings.shape}\")\n",
        "\n",
        "# 4. Store the embeddings in ChromaDB\n",
        "client = chromadb.Client()\n",
        "\n",
        "# Create or get a collection\n",
        "collection_name = \"knowledge_graph_sentences_bert\" # Using a distinct name\n",
        "try:\n",
        "    collection = client.create_collection(name=collection_name)\n",
        "    print(f\"Collection '{collection_name}' created.\")\n",
        "except chromadb.db.base.UniqueConstraintError:\n",
        "    collection = client.get_collection(name=collection_name)\n",
        "    print(f\"Collection '{collection_name}' already exists. Getting existing collection.\")\n",
        "\n",
        "# Prepare data for ChromaDB\n",
        "ids = [f\"sentence_{i}\" for i in range(len(sentences))]\n",
        "embeddings = sentence_embeddings.tolist()  # Convert numpy array to list\n",
        "documents = sentences\n",
        "\n",
        "# Add data to the collection\n",
        "collection.add(\n",
        "    embeddings=embeddings,\n",
        "    documents=documents,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(f\"Embeddings and documents added to collection '{collection_name}'.\")\n",
        "print(\"Number of items in collection:\", collection.count())\n",
        "\n",
        "# 5. Accept a simple user query and return relevant text\n",
        "user_query = input(\"\\nEnter your query (e.g., Who does Alice know?): \")\n",
        "\n",
        "# Embed the user query using the same BERT model\n",
        "query_embedding = embed_text([user_query]).tolist()\n",
        "\n",
        "# Perform similarity search in ChromaDB\n",
        "# query_embeddings expects a list of embeddings, even if you have only one query\n",
        "results = collection.query(\n",
        "    query_embeddings=query_embedding,\n",
        "    n_results=1,  # Retrieve the top 1 most relevant sentence\n",
        "    include=['documents', 'distances']  # Include the original document text and distance\n",
        ")\n",
        "\n",
        "# Display the most relevant document (sentence) and its similarity score (distance)\n",
        "if results and results.get('documents') and results.get('distances'):\n",
        "    most_relevant_sentence = results['documents'][0][0]\n",
        "    similarity_distance = results['distances'][0][0]\n",
        "    print(\"\\nMost Relevant Sentence:\")\n",
        "    print(most_relevant_sentence)\n",
        "    print(f\"\\nSimilarity Distance (lower is more similar): {similarity_distance}\")\n",
        "else:\n",
        "    print(\"\\nNo relevant sentences found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "SnH9tPrVRDjz",
        "outputId": "3f686cdb-a505-4f7e-ee81-26454567a8a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge Graph (Turtle format):\n",
            "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
            "@prefix ns1: <http://example.org/> .\n",
            "\n",
            "ns1:alice a foaf:Person ;\n",
            "    foaf:knows ns1:bob .\n",
            "\n",
            "ns1:bob a foaf:Person ;\n",
            "    ns1:worksAt ns1:acme_corp .\n",
            "\n",
            "\n",
            "\n",
            "Sentences from triples:\n",
            "alice 22-rdf-syntax-ns#type Person.\n",
            "alice knows bob.\n",
            "bob 22-rdf-syntax-ns#type Person.\n",
            "bob worksAt acme_corp.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlignDevicesHook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_hook_to_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m from .big_modeling import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit_torch_state_dict_into_shards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mpytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_pruneable_heads_and_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_linear_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftAdapterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepspeed_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f16536026215>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0membed_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m             )\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m             return model_class.from_pretrained(\n\u001b[1;32m    566\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0msupported_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;31m# Maybe there was several model types associated with this config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Had some trouble with library versions here.\n",
        "# Kept hitting import errors, seemed like sentence-transformers, transformers, and huggingface_hub\n",
        "# were clashing with each other. Took a bit to figure out installing specific versions and\n",
        "# upgrading huggingface_hub sorted it out. If you're seeing weird import errors,\n",
        "#  i have also used particular versions and double-check it, especially for the huggingface stuff.but then also issues is not been resolved and due to tim,e constraint i need to submit the notebook as it is but i am pretty sure the code is in working state for second assestment and for first i have got the output\n"
      ],
      "metadata": {
        "id": "hUyZB1q1SDHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install aiofiles==24.1.0\n",
        "! pip install aiohttp==3.11.18\n",
        "! pip install python-dotenv==1.1.0\n",
        "! pip install chromadb==1.0.8\n",
        "! pip install autogen-core==0.5.6\n",
        "! pip install autogen-ext==0.5.6\n",
        "! pip install autogen-agentchat==0.5.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ooHcB174TOVZ",
        "outputId": "2bb196b8-154d-48d6-bc9a-50ac73e110d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting aiofiles==24.1.0\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: aiofiles\n",
            "Successfully installed aiofiles-24.1.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting aiohttp==3.11.18\n",
            "  Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.11.18) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.11.18) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.11.18) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.11.18) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.11.18) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.11.18) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp==3.11.18) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp==3.11.18) (3.10)\n",
            "Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: aiohttp\n",
            "  Attempting uninstall: aiohttp\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: aiohttp 3.11.15\n",
            "    Uninstalling aiohttp-3.11.15:\n",
            "      Successfully uninstalled aiohttp-3.11.15\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.11.18\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: python-dotenv==1.1.0 in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting chromadb==1.0.8\n",
            "  Downloading chromadb-1.0.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (2.11.4)\n",
            "Collecting fastapi==0.115.9 (from chromadb==1.0.8)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==1.0.8) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (2.0.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (4.13.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (1.22.0)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb==1.0.8)\n",
            "  Downloading opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==1.0.8)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb==1.0.8)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.54b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb==1.0.8)\n",
            "  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (0.14.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (0.15.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb==1.0.8)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb==1.0.8)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==1.0.8) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb==1.0.8)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb==1.0.8) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb==1.0.8) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb==1.0.8) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb==1.0.8) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb==1.0.8) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb==1.0.8) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb==1.0.8) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb==1.0.8) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb==1.0.8) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb==1.0.8) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb==1.0.8) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==1.0.8) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb==1.0.8)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==1.0.8) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==1.0.8) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==1.0.8) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==1.0.8) (1.13.1)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb==1.0.8)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb==1.0.8)\n",
            "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==1.0.8) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==1.0.8)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==1.0.8)\n",
            "  Downloading opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.54b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8)\n",
            "  Downloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8)\n",
            "  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8)\n",
            "  Downloading opentelemetry_util_http-0.54b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.54b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.54b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.8)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==1.0.8) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==1.0.8) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==1.0.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==1.0.8) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==1.0.8) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb==1.0.8) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb==1.0.8) (2.19.1)\n",
            "Requirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb==1.0.8) (0.16.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==1.0.8) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==1.0.8) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==1.0.8) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==1.0.8) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==1.0.8) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==1.0.8) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==1.0.8) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==1.0.8) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==1.0.8) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==1.0.8) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==1.0.8) (3.18.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==1.0.8) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==1.0.8) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb==1.0.8) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb==1.0.8) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb==1.0.8) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==1.0.8) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==1.0.8) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==1.0.8) (0.6.1)\n",
            "Downloading chromadb-1.0.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.33.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.54b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.54b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.54b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: durationpy, opentelemetry-util-http, opentelemetry-proto, mmh3, importlib-metadata, deprecated, asgiref, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.46.2\n",
            "    Uninstalling starlette-0.46.2:\n",
            "      Successfully uninstalled starlette-0.46.2\n",
            "  Attempting uninstall: fastapi\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: fastapi 0.115.12\n",
            "    Uninstalling fastapi-0.115.12:\n",
            "      Successfully uninstalled fastapi-0.115.12\n",
            "  Attempting uninstall: chromadb\n",
            "    Found existing installation: chromadb 0.4.14\n",
            "    Uninstalling chromadb-0.4.14:\n",
            "      Successfully uninstalled chromadb-0.4.14\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.33.1 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.16.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 chromadb-1.0.8 deprecated-1.2.18 durationpy-0.10 fastapi-0.115.9 importlib-metadata-8.6.1 kubernetes-32.0.1 mmh3-5.1.0 opentelemetry-api-1.33.1 opentelemetry-exporter-otlp-proto-common-1.33.1 opentelemetry-exporter-otlp-proto-grpc-1.33.1 opentelemetry-instrumentation-0.54b1 opentelemetry-instrumentation-asgi-0.54b1 opentelemetry-instrumentation-fastapi-0.54b1 opentelemetry-proto-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 opentelemetry-util-http-0.54b1 starlette-0.45.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "e918e7417271433f817e41cf159aeaf3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting autogen-core==0.5.6\n",
            "  Downloading autogen_core-0.5.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting jsonref~=1.1.0 (from autogen-core==0.5.6)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6) (1.33.1)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6) (11.2.1)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6) (4.13.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.6) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.6) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6) (0.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.5.6) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.5.6) (3.21.0)\n",
            "Downloading autogen_core-0.5.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: jsonref, autogen-core\n",
            "Successfully installed autogen-core-0.5.6 jsonref-1.1.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting autogen-ext==0.5.6\n",
            "  Downloading autogen_ext-0.5.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: autogen-core==0.5.6 in /usr/local/lib/python3.11/dist-packages (from autogen-ext==0.5.6) (0.5.6)\n",
            "Requirement already satisfied: jsonref~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-ext==0.5.6) (1.1.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-ext==0.5.6) (1.33.1)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-ext==0.5.6) (11.2.1)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-ext==0.5.6) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-ext==0.5.6) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-ext==0.5.6) (4.13.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-ext==0.5.6) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-ext==0.5.6) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6->autogen-ext==0.5.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6->autogen-ext==0.5.6) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6->autogen-ext==0.5.6) (0.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-ext==0.5.6) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-ext==0.5.6) (3.21.0)\n",
            "Downloading autogen_ext-0.5.6-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.8/296.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: autogen-ext\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed autogen-ext-0.5.6\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting autogen-agentchat==0.5.6\n",
            "  Downloading autogen_agentchat-0.5.6-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: autogen-core==0.5.6 in /usr/local/lib/python3.11/dist-packages (from autogen-agentchat==0.5.6) (0.5.6)\n",
            "Requirement already satisfied: jsonref~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-agentchat==0.5.6) (1.1.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-agentchat==0.5.6) (1.33.1)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-agentchat==0.5.6) (11.2.1)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-agentchat==0.5.6) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-agentchat==0.5.6) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.6->autogen-agentchat==0.5.6) (4.13.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-agentchat==0.5.6) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-agentchat==0.5.6) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6->autogen-agentchat==0.5.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6->autogen-agentchat==0.5.6) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.6->autogen-agentchat==0.5.6) (0.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-agentchat==0.5.6) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.5.6->autogen-agentchat==0.5.6) (3.21.0)\n",
            "Downloading autogen_agentchat-0.5.6-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: autogen-agentchat\n",
            "Successfully installed autogen-agentchat-0.5.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG with autogen please have a look on below code with existing autogen framework\n",
        "\n",
        "# rag_app.py\n",
        "import os\n",
        "import asyncio\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "import aiofiles\n",
        "import aiohttp\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from autogen_ext.memory.chromadb import (\n",
        "    ChromaDBVectorMemory,\n",
        "    PersistentChromaDBVectorMemoryConfig,\n",
        ")\n",
        "from autogen_core.memory import MemoryContent, MemoryMimeType\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "from autogen_core.models import ModelInfo\n",
        "\n",
        "# Configure logging for debug output\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class SimpleDocumentIndexer:\n",
        "    \"\"\"\n",
        "    Loads and splits documents into fixed-size chunks, then stores them in a ChromaDBVectorMemory.\n",
        "    \"\"\"\n",
        "    def __init__(self, memory: ChromaDBVectorMemory, chunk_size: int = 300):\n",
        "        self.memory = memory\n",
        "        self.chunk_size = chunk_size  # Number of characters per chunk\n",
        "\n",
        "    async def _fetch_content(self, source: str) -> str:\n",
        "        \"\"\"\n",
        "        Fetch raw text from a local file or an HTTP(S) URL.\n",
        "\n",
        "        Args:\n",
        "            source (str): Path or URL to fetch.\n",
        "        Returns:\n",
        "            str: Raw text content.\n",
        "        \"\"\"\n",
        "        if source.startswith((\"http://\", \"https://\")):\n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                async with session.get(source) as resp:\n",
        "                    resp.raise_for_status()\n",
        "                    return await resp.text()\n",
        "        else:\n",
        "            # Read local file asynchronously\n",
        "            async with aiofiles.open(source, \"r\", encoding=\"utf-8\") as f:\n",
        "                return await f.read()\n",
        "\n",
        "    def _strip_html(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove HTML tags and collapse multiple whitespace characters.\n",
        "        \"\"\"\n",
        "        text = re.sub(r\"<[^>]*>\", \" \", text)\n",
        "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    def _split_text(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Break text into chunks of up to chunk_size characters.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            text[i : i + self.chunk_size].strip()\n",
        "            for i in range(0, len(text), self.chunk_size)\n",
        "        ]\n",
        "\n",
        "    async def index_documents(self, sources: List[str]) -> int:\n",
        "        \"\"\"\n",
        "        Index each document source into the vector store.\n",
        "\n",
        "        Args:\n",
        "            sources (List[str]): List of file paths or URLs.\n",
        "        Returns:\n",
        "            int: Total number of chunks indexed.\n",
        "        \"\"\"\n",
        "        total_chunks = 0\n",
        "        for source in sources:\n",
        "            try:\n",
        "                raw = await self._fetch_content(source)\n",
        "                # Detect HTML and remove tags\n",
        "                if \"<\" in raw and \">\" in raw:\n",
        "                    raw = self._strip_html(raw)\n",
        "                chunks = self._split_text(raw)\n",
        "                for idx, chunk in enumerate(chunks):\n",
        "                    await self.memory.add(\n",
        "                        MemoryContent(\n",
        "                            content=chunk,\n",
        "                            mime_type=MemoryMimeType.TEXT,\n",
        "                            metadata={\"source\": source, \"chunk_index\": idx},\n",
        "                        )\n",
        "                    )\n",
        "                total_chunks += len(chunks)\n",
        "                logger.info(f\"Indexed {len(chunks)} chunks from {source}\")\n",
        "            except Exception as e:\n",
        "                logger.exception(f\"Failed to index {source}: {e}\")\n",
        "        return total_chunks\n",
        "\n",
        "\n",
        "class RAGAssistant:\n",
        "    \"\"\"\n",
        "    Retrieval-Augmented Generation (RAG) assistant:\n",
        "      1. Index KB sources once on startup.\n",
        "      2. For each user query, retrieve relevant chunks and answer.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        openai_model: str,\n",
        "        openai_api_key: str,\n",
        "        kb_sources: List[str],\n",
        "        persistence_dir: Optional[str] = None,\n",
        "        k: int = 3,\n",
        "        score_threshold: float = 0.4,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the RAG assistant.\n",
        "\n",
        "        Args:\n",
        "            openai_model (str): LLM model name (e.g., \"gemini-2.0-flash\").\n",
        "            openai_api_key (str): API key for OpenAI.\n",
        "            kb_sources (List[str]): Paths or URLs to index as knowledge base.\n",
        "            persistence_dir (Optional[str]): Directory to persist vector store.\n",
        "            k (int): Number of nearest neighbors to retrieve per query.\n",
        "            score_threshold (float): Minimum similarity score to include.\n",
        "        \"\"\"\n",
        "        # Validate required parameters\n",
        "        if not persistence_dir:\n",
        "            raise ValueError(\"persistence_dir must be provided\")\n",
        "        if not kb_sources:\n",
        "            raise ValueError(\"kb_sources must be provided\")\n",
        "\n",
        "        # Set up the LLM client\n",
        "        self.client = OpenAIChatCompletionClient(\n",
        "            model=openai_model,\n",
        "            api_key=openai_api_key,\n",
        "            model_info=ModelInfo(\n",
        "                vision=True,\n",
        "                function_calling=True,\n",
        "                json_output=True,\n",
        "                structured_output=True,\n",
        "                family=openai_model,\n",
        "                multiple_system_messages=True,\n",
        "\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Configure vector DB memory\n",
        "        config = PersistentChromaDBVectorMemoryConfig(\n",
        "            collection_name=\"rag_collection\",\n",
        "            persistence_path=persistence_dir,\n",
        "            k=k,\n",
        "            score_threshold=score_threshold,\n",
        "        )\n",
        "        self.memory = ChromaDBVectorMemory(config=config)\n",
        "        self.indexer = SimpleDocumentIndexer(\n",
        "            memory=self.memory,\n",
        "            chunk_size=1500,\n",
        "        )\n",
        "        self.kb_sources = kb_sources\n",
        "\n",
        "    async def initialize(self) -> int:\n",
        "        \"\"\"\n",
        "        Clear existing vectors and index KB sources once.\n",
        "\n",
        "        Returns:\n",
        "            int: Total chunks indexed.\n",
        "        \"\"\"\n",
        "        logger.info(\"Clearing existing memory…\")\n",
        "        await self.memory.clear()\n",
        "        logger.info(\"Indexing knowledge base sources…\")\n",
        "        total_chunks = await self.indexer.index_documents(self.kb_sources)\n",
        "        logger.info(f\"Indexing complete: {total_chunks} chunks in total.\")\n",
        "        return total_chunks\n",
        "\n",
        "    async def answer(self, query: str):\n",
        "        \"\"\"\n",
        "        Retrieve relevant document chunks and answer the user query.\n",
        "\n",
        "        Args:\n",
        "            query (str): User's question.\n",
        "        \"\"\"\n",
        "        agent = AssistantAgent(\n",
        "            name=\"rag_agent\",\n",
        "            model_client=self.client,\n",
        "            memory=[self.memory],\n",
        "            system_message=(\n",
        "                \"You are an AI assistant. Use these document excerpts to answer questions.if you dont have answer to that question just say dont have answer to that question.\"\n",
        "\n",
        "            ),\n",
        "        )\n",
        "        stream = agent.run_stream(task=query)\n",
        "        await Console(stream)\n",
        "\n",
        "    async def shutdown(self):\n",
        "        \"\"\"\n",
        "        Cleanly close any persistent connections (e.g., vector DB).\n",
        "        \"\"\"\n",
        "        await self.memory.close()\n",
        "\n",
        "    async def run_loop(self):\n",
        "        \"\"\"\n",
        "        Entry point:\n",
        "          1) initialize (index once)\n",
        "          2) interactive REPL for questions\n",
        "        \"\"\"\n",
        "        await self.initialize()\n",
        "\n",
        "        print(\"\\n>> Enter your question (empty to exit):\")\n",
        "        while True:\n",
        "            user_q = input(\"Q: \").strip()\n",
        "            if not user_q:\n",
        "                break\n",
        "            await self.answer(user_q)\n",
        "            print(\"\\n>> Next question, or press Enter to quit.\")\n",
        "\n",
        "        await self.shutdown()"
      ],
      "metadata": {
        "id": "hiFY9klJS2r9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}